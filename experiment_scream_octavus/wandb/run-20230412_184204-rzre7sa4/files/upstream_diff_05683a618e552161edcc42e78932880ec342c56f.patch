diff --git a/experiment_scream_octavus/run_large_debug_128seq.sh b/experiment_scream_octavus/run_large_debug_128seq.sh
index 8a9c4f4..25b59f1 100644
--- a/experiment_scream_octavus/run_large_debug_128seq.sh
+++ b/experiment_scream_octavus/run_large_debug_128seq.sh
@@ -23,7 +23,7 @@ python ../run_flax_speech_recognition_seq2seq_streaming_debug.py \
     --eval_steps 10000 \
     --learning_rate 5e-6 \
     --preprocessing_num_workers 32 \
-    --per_device_train_batch_size 5 \
+    --per_device_train_batch_size 3 \
     --per_device_eval_batch_size 5 \
     --predict_with_generate \
     --log_max_eval_predictions 100 \
diff --git a/experiment_scream_octavus/wandb/latest-run b/experiment_scream_octavus/wandb/latest-run
new file mode 120000
index 0000000..ea49409
--- /dev/null
+++ b/experiment_scream_octavus/wandb/latest-run
@@ -0,0 +1 @@
+run-20230412_184204-rzre7sa4
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/code/run_flax_speech_recognition_seq2seq_streaming.py b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/code/run_flax_speech_recognition_seq2seq_streaming.py
new file mode 100644
index 0000000..fd32b95
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/code/run_flax_speech_recognition_seq2seq_streaming.py
@@ -0,0 +1,1309 @@
+#!/usr/bin/env python
+# coding=utf-8
+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR COND    ITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""
+Fine-tuning the Flax library models for sequence to sequence speech recognition.
+"""
+# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.
+
+import itertools
+import json
+import logging
+import os
+import shutil
+import socket
+import sys
+import tempfile
+import time
+from dataclasses import field
+from datetime import datetime
+from functools import partial
+from importlib import import_module
+from pathlib import Path
+from typing import Any, Callable, Dict, Generator, List, Optional, Union
+
+import flax
+import jax
+import jax.numpy as jnp 
+import numpy as np
+import optax
+import pandas as pd
+import torch
+# from jax.experimental.compilation_cache import compilation_cache; compilation_cache.initialize_cache(tempfile.gettempdir())
+from flax import jax_utils, traverse_util
+from flax.jax_utils import pad_shard_unpad, unreplicate
+from flax.training import train_state
+from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key
+from torch.utils.data import IterableDataset
+from tqdm import tqdm
+
+import datasets
+import evaluate
+import transformers
+from datasets import Dataset, DatasetDict, IterableDatasetDict, interleave_datasets, load_dataset
+from datasets.distributed import split_dataset_by_node
+from huggingface_hub import Repository, create_repo
+from transformers import (
+    AutoConfig,
+    AutoFeatureExtractor,
+    AutoProcessor,
+    AutoTokenizer,
+    FlaxAutoModelForSpeechSeq2Seq,
+    HfArgumentParser,
+    Seq2SeqTrainingArguments,
+    is_tensorboard_available,
+)
+from transformers.modelcard import TrainingSummary
+from transformers.models.whisper.english_normalizer import BasicTextNormalizer
+from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE
+from transformers.file_utils import get_full_repo_name
+from transformers.utils import check_min_version, send_example_telemetry
+from transformers.utils.versions import require_version
+
+from flax.training import checkpoints
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+check_min_version("4.27.0.dev0")
+
+require_version("datasets>=1.18.2",
+                "To fix: pip install -r examples/flax/speech-recogintion/requirements.txt")
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+logger = logging.getLogger(__name__)
+
+
+@flax.struct.dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={
+            "help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    feature_extractor_name: Optional[str] = field(
+        default=None, metadata={"help": "feature extractor name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Where to store the pretrained models downloaded from huggingface.co"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={
+            "help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+    dtype: Optional[str] = field(
+        default="float32",
+        metadata={
+            "help": (
+                "Floating-point format in which the model weights should be initialized and trained. Choose one of"
+                " `[float32, float16, bfloat16]`."
+            )
+        },
+    )
+    num_beams: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Number of beams to use for evaluation. This argument will be passed to `model.generate`, "
+                "which is used during evaluation."
+            )
+        },
+    )
+
+
+@flax.struct.dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    dataset_name: str = field(
+        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
+    )
+    dataset_config_name: Optional[str] = field(
+        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
+    )
+    text_column: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "The name of the column in the datasets containing the full texts (for summarization)."},
+    )
+    dataset_cache_dir: Optional[str] = field(
+        default=None, metadata={"help": "Path to cache directory for saving and loading datasets"}
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
+    )
+    preprocessing_num_workers: Optional[int] = field(
+        default=50,
+        metadata={"help": "The number of processes to use for the preprocessing."},
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    audio_column_name: str = field(
+        default="audio",
+        metadata={
+            "help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
+    )
+    text_column_name: str = field(
+        default="text",
+        metadata={
+            "help": "The name of the dataset column containing the text data. Defaults to 'text'"},
+    )
+    max_duration_in_seconds: float = field(
+        default=30.0,
+        metadata={
+            "help": "Filter audio files that are longer than `max_duration_in_seconds` seconds"},
+    )
+    min_duration_in_seconds: float = field(
+        default=0.0,
+        metadata={
+            "help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"},
+    )
+    max_label_length: float = field(
+        default=128,
+        metadata={
+            "help": "Truncate transcriptions that are longer `max_eval_length` tokens."},
+    )
+    pad_input_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the input sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the inputs to max length."
+        },
+    )
+    pad_target_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the target sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the targets to max length."
+        },
+    )
+    train_split_name: str = field(
+        default="train",
+        metadata={
+            "help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
+        },
+    )
+    eval_split_name: str = field(
+        default="validation",
+        metadata={
+            "help": "The name of the evaluation data set split to use (via the datasets library). Defaults to 'validation'"
+        },
+    )
+    do_lower_case: bool = field(
+        default=False,
+        metadata={"help": "Whether the target text should be lower cased."},
+    )
+    do_remove_punctuation: bool = field(
+        default=False,
+        metadata={
+            "help": "Whether the target text should be striped of punctuation."},
+    )
+    do_normalize_eval: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to normalise the references and predictions in the eval WER calculation."},
+    )
+    language: str = field(
+        default=None,
+        metadata={
+            "help": (
+                "Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning "
+                "only. For English speech recognition, it should be set to `None`."
+            )
+        },
+    )
+    task: str = field(
+        default="transcribe",
+        metadata={
+            "help": "Task, either `transcribe` for speech recognition or `translate` for speech translation."},
+    )
+    num_train_steps: int = field(default=50000, metadata={
+                                 "help": "The number of training steps."})
+    shuffle_buffer_size: Optional[int] = field(
+        default=500,
+        metadata={
+            "help": (
+                "The number of streamed examples to download before shuffling them. The large the buffer, "
+                "the closer it is to real offline shuffling."
+            )
+        },
+    )
+    streaming: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use streaming mode to load and pre-process the data."},
+    )
+    log_max_eval_predictions: Optional[int] = field(
+        default=0,
+        metadata={
+            "help": (
+                "Number of label and prediction pairs to write to the summary at each evaluation step."
+            )
+        },
+    )
+    log_eval_predictions_fn: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Python path to function for logging evaluation predictions. It can be an external function like fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions, labels)."
+            )
+        },
+    )
+    run_description: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "A longer description of the run/experiment."
+            )
+        },
+    )
+    wandb_entity: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases username or entity (organization name)."
+            )
+        },
+    )
+    wandb_project: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases project to log metrics to."
+            )
+        },
+    )
+
+
+def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:
+    """
+    Shift label ids one token to the right.
+    """
+    shifted_label_ids = np.zeros_like(label_ids)
+    shifted_label_ids[:, 1:] = label_ids[:, :-1]
+    shifted_label_ids[:, 0] = decoder_start_token_id
+
+    return shifted_label_ids
+
+
+@flax.struct.dataclass
+class FlaxDataCollatorSpeechSeq2SeqWithPadding:
+    """
+    Data collator that will dynamically pad the inputs received.
+    Args:
+        processor ([`Wav2Vec2Processor`])
+            The processor used for proccessing the data.
+        decoder_start_token_id (:obj: `int`)
+            The begin-of-sentence of the decoder.
+        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)
+            among:
+            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
+              sequence if provided).
+            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
+              maximum acceptable input length for the model if that argument is not provided.
+            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
+              different lengths).
+        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).
+            See above for details.
+        max_input_length (:obj:`float`, `optional`):
+            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
+        max_target_length (:obj:`int`, `optional`):
+            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).
+        pad_input_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the input sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+        pad_target_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the target sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+    """
+
+    processor: Any
+    decoder_start_token_id: int
+    input_padding: Union[bool, str] = "longest"
+    target_padding: Union[bool, str] = "max_length"
+    max_input_length: Optional[float] = None
+    max_target_length: Optional[int] = None
+    pad_input_to_multiple_of: Optional[int] = None
+    pad_target_to_multiple_of: Optional[int] = None
+
+    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:
+        model_input_name = self.processor.model_input_names[0]
+        input_features = {model_input_name: features[model_input_name]}
+        label_features = {"input_ids": features["labels"]}
+
+        # reformat list to dict and set to pytorch format
+        batch = self.processor.feature_extractor.pad(
+            input_features,
+            max_length=self.max_input_length,
+            padding=self.input_padding,
+            pad_to_multiple_of=self.pad_input_to_multiple_of,
+            return_tensors="np",
+        )
+
+        labels_batch = self.processor.tokenizer.pad(
+            label_features,
+            max_length=self.max_target_length,
+            padding=self.target_padding,
+            pad_to_multiple_of=self.pad_target_to_multiple_of,
+            return_tensors="np",
+        )
+
+        # if bos token is appended in previous tokenization step,
+        # cut bos token here as it's append later anyways
+        labels = labels_batch["input_ids"]
+        if (labels[:, 0] == self.decoder_start_token_id).all().item():
+            labels = labels[:, 1:]
+            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]
+
+        decoder_input_ids = shift_tokens_right(
+            labels, self.decoder_start_token_id)
+
+        # replace padding with -100 to ignore correctly when computing the loss
+        labels = np.ma.array(labels, mask=np.not_equal(
+            labels_batch.attention_mask, 1))
+        labels = labels.filled(fill_value=-100)
+
+        batch["labels"] = labels
+        batch["decoder_input_ids"] = decoder_input_ids
+        batch["attention_mask"] = labels_batch.attention_mask  # Add attention_mask to the batch
+
+        return batch
+
+
+def load_maybe_streaming_dataset(dataset_name, dataset_config_name, split="train", streaming=True, **kwargs):
+    """
+    Utility function to load a dataset in streaming mode. For datasets with multiple splits,
+    each split is loaded individually and then splits combined by taking alternating examples from
+    each (interleaving).
+    """
+    if "+" in split:
+        # load multiple splits separated by the `+` symbol with streaming mode
+        dataset_splits = [
+            load_dataset(dataset_name, dataset_config_name,
+                         split=split_name, streaming=streaming, **kwargs)
+            for split_name in split.split("+")
+        ]
+        # interleave multiple splits to form one dataset
+        interleaved_dataset = interleave_datasets(dataset_splits)
+        return interleaved_dataset
+    else:
+        # load a single split *with* streaming mode
+        dataset = load_dataset(
+            dataset_name, dataset_config_name, split=split, streaming=streaming, **kwargs)
+        return dataset
+
+
+def collate_batch(samples):
+    return {key: [feature[key] for feature in samples] for key in samples[0]}
+
+def data_loader(
+    dataset: Dataset,
+    batch_size: int,
+    drop_last: bool=True,
+    num_workers: int=0,
+) -> Generator:
+    """
+    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,
+    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.
+    """
+    data_loader_iterator = iter(torch.utils.data.DataLoader(
+        batch_size=batch_size,
+        dataset=dataset.with_format("torch"),
+        num_workers=num_workers,
+        collate_fn=collate_batch,
+        drop_last=drop_last,
+    ))
+    return data_loader_iterator
+
+
+class TrainState(train_state.TrainState):
+    dropout_rng: jnp.ndarray
+
+    def replicate(self):
+        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))
+
+
+def create_learning_rate_fn(
+    num_train_steps: int, num_warmup_steps: int, learning_rate: float, start_step: int=0, warmup_init_value: float=0.0, decay_end_value: float=0.0,
+) -> Callable[[int], jnp.array]:
+    """Returns a linear warmup, linear_decay learning rate function."""
+    warmup_fn = optax.linear_schedule(
+        init_value=warmup_init_value, end_value=learning_rate, transition_steps=num_warmup_steps)
+    decay_fn = optax.linear_schedule(
+        init_value=learning_rate, end_value=decay_end_value, transition_steps=num_train_steps - num_warmup_steps
+    )
+    schedule_fn = optax.join_schedules(
+        schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])
+    
+    def learning_rate_fn(step: int) -> jnp.array:
+        return schedule_fn(step + start_step)
+    
+    return learning_rate_fn
+
+
+def main():
+    # Parse input arguments
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+    parser = HfArgumentParser(
+        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
+
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args = parser.parse_json_file(
+            json_file=os.path.abspath(sys.argv[1]))
+    else:
+        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
+    # information sent is the one passed as arguments along with your JAX/Flax versions.
+    send_example_telemetry("run_speech_recognition_seq2seq",
+                           model_args, data_args, framework="flax")
+
+    # Setup logging
+    # Make one log on every process with the configuration for debugging.
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+    # Set the verbosity to info of the Transformers logger.
+    # We only want one process per machine to log things on the screen.
+
+    # logger.setLevel(logging.INFO if jax.local_devices()[0].id%jax.local_device_count() == 0 else logging.ERROR)
+
+    # logger.setLevel(logging.INFO if jax.process_index()
+    #                == 0 else logging.ERROR)
+    
+    # Number of hosts
+    num_of_hosts = jax.process_count()
+    current_host_idx = jax.process_index()
+
+    if current_host_idx == 0:
+        datasets.utils.logging.set_verbosity_warning()
+        transformers.utils.logging.set_verbosity_info()
+    else:
+        datasets.utils.logging.set_verbosity_error()
+        transformers.utils.logging.set_verbosity_error()
+    
+    logger.setLevel(logging.INFO)
+    logger.info("Training/evaluation parameters %s", training_args)
+
+    if num_of_hosts and not training_args.push_to_hub:
+        logger.warning(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --push_to_hub to be able to save checkpoints to the hub."
+        )
+    if num_of_hosts and not training_args.overwrite_output_dir and training_args.resume_from_checkpoint:
+        logger.error(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --overwrite_output_dir to be able to resume from a pushed checkpoint."
+        )
+        sys.exit(1)
+
+    # Check the output dir is valid
+    if os.path.exists(training_args.output_dir):
+        if (
+            os.listdir(training_args.output_dir)
+            and training_args.do_train
+            and not training_args.overwrite_output_dir
+        ):
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use `--overwrite_output_dir` to overcome."
+            )
+        elif training_args.overwrite_output_dir:
+            logger.warning(f"Removing path {training_args.output_dir}")
+            shutil.rmtree(training_args.output_dir)
+      
+    # Handle the repository creation
+    output_dir = Path(training_args.output_dir)
+    if training_args.push_to_hub:
+        if training_args.hub_model_id is None:
+            repo_name = get_full_repo_name(
+                output_dir.absolute().name,
+                token=training_args.hub_token,
+                organization=training_args.push_to_hub_organization,
+            )
+        else:
+            repo_name = training_args.hub_model_id
+         
+        repo_url = None  
+        while not repo_url:
+            # Workaround for an internal HuggingFace error if the repo is being created by another worker
+            try:
+                repo_url = create_repo(
+                    repo_name, exist_ok=True, token=training_args.hub_token, private=training_args.hub_private_repo
+                )
+            except:
+                time.sleep(1)
+
+        repo = Repository(training_args.output_dir,
+                          clone_from=repo_name, token=training_args.hub_token)
+
+    # Set the model_name_or_path
+    model_name_or_path = model_args.model_name_or_path
+
+    # Try to detect last checkpoint and continue if possible
+    training_state = {"step": 0, "eval_lines": []}
+    if training_args.resume_from_checkpoint:
+        if (output_dir / "flax_model.msgpack").exists() and (output_dir / "training_state.bin").exists():
+            training_state = json.loads((output_dir / "training_state.bin").read_text())
+            model_name_or_path = os.path.join(training_args.output_dir)
+            logger.info(
+                f"Checkpoint detected, resuming training from {training_args.output_dir} at step {training_state['step']}."
+            )
+        else:
+            logger.info(
+                f"No valid checkpoint found in {training_args.output_dir}. Starting from {model_name_or_path}."
+            )
+    
+    
+    # Load dataset
+    raw_datasets = IterableDatasetDict() if data_args.streaming else DatasetDict()
+
+    if training_args.do_train:
+        raw_datasets["train"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.train_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if training_args.do_eval:
+        raw_datasets["eval"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.eval_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if not training_args.do_train and not training_args.do_eval:
+        raise ValueError(
+            "Cannot not train and not do evaluation. At least one of training or evaluation has to be performed."
+        )
+
+    raw_datasets_features = list(
+        next(iter(raw_datasets.values())).features.keys())
+
+    if data_args.audio_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--audio_column_name` to the correct audio column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    if data_args.text_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--text_column_name` to the correct text column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    # Load pretrained model, tokenizer, and feature extractor
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    feature_extractor = AutoFeatureExtractor.from_pretrained(
+        model_args.feature_extractor_name if model_args.feature_extractor_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(
+        model_name_or_path,
+        config=config,
+        dtype=getattr(jnp, model_args.dtype),
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    logger.info(
+        f"Successfully loaded the model '{model_name_or_path}'."
+    )
+    
+    if model.config.decoder_start_token_id is None:
+        raise ValueError(
+            "Make sure that `config.decoder_start_token_id` is correctly defined")
+
+    # Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,
+    # so we just need to set the correct target sampling rate.
+    dataset_sampling_rate = next(
+        iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
+
+    if dataset_sampling_rate != feature_extractor.sampling_rate:
+        raw_datasets = raw_datasets.cast_column(
+            data_args.audio_column_name, datasets.features.Audio(
+                sampling_rate=feature_extractor.sampling_rate)
+        )
+
+    # Preprocessing the datasets.
+    # We need to read the audio files as arrays and tokenize the targets.
+    max_input_length = int(
+        data_args.max_duration_in_seconds * feature_extractor.sampling_rate)
+    min_input_length = int(
+        data_args.min_duration_in_seconds * feature_extractor.sampling_rate)
+    max_label_length = (
+        data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length
+    )
+    pad_input_to_multiple_of = data_args.pad_input_to_multiple_of
+    pad_target_to_multiple_of = data_args.pad_target_to_multiple_of
+    audio_column_name = data_args.audio_column_name
+    num_workers = data_args.preprocessing_num_workers
+    text_column_name = data_args.text_column_name
+    model_input_name = feature_extractor.model_input_names[0]
+    do_lower_case = data_args.do_lower_case
+    do_remove_punctuation = data_args.do_remove_punctuation
+    normalizer = BasicTextNormalizer()  # 'official' text normalizer from OpenAI
+
+    if data_args.language is not None:
+        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)
+        tokenizer.set_prefix_tokens(
+            language=data_args.language, task=data_args.task)
+
+    def prepare_dataset(batch):
+        # Process audio
+        sample = batch[audio_column_name]
+        inputs = feature_extractor(
+            sample["array"], sampling_rate=sample["sampling_rate"])
+        # Process audio length
+        batch[model_input_name] = inputs.get(model_input_name)[0]
+        batch["input_length"] = len(sample["array"])
+
+        # Process targets
+        input_str = batch[text_column_name].lower(
+        ) if do_lower_case else batch[text_column_name]
+        if do_remove_punctuation:
+            input_str = normalizer(input_str).strip()
+        batch["labels"] = tokenizer(input_str).input_ids
+        return batch
+
+    with training_args.main_process_first(desc="dataset map pre-processing"):
+        vectorized_datasets = raw_datasets.map(
+            prepare_dataset,
+            remove_columns=raw_datasets_features,
+        )
+
+    # Filter training data with inputs longer than max_input_length
+    def is_audio_in_length_range(length):
+        return min_input_length < length < max_input_length
+
+    if training_args.do_train:
+        vectorized_datasets["train"] = vectorized_datasets["train"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    if training_args.do_eval:
+        vectorized_datasets["eval"] = vectorized_datasets["eval"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    # Load metrics and write stats
+    metric_wer = evaluate.load("wer")
+    metric_cer = evaluate.load("cer")
+    do_normalize_eval = data_args.do_normalize_eval
+
+    def compute_metrics(pred_ids, label_ids, return_preds_labels=False):
+        # Replace padded labels by the padding token
+        for idx in range(len(label_ids)):
+            label_ids[idx][label_ids[idx] == -100] = tokenizer.pad_token_id
+
+        predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
+        # We do not want to group tokens when computing the metrics
+        labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
+
+        if do_normalize_eval:
+            pred_str = [normalizer(pred) for pred in predictions]
+            label_str = [normalizer(label) for label in labels]
+            # Filtering step to only evaluate the samples that correspond to non-zero references:
+            pred_str = [pred_str[i]
+                        for i in range(len(pred_str)) if len(label_str[i]) > 0]
+            label_str = [label_str[i]
+                         for i in range(len(label_str)) if len(label_str[i]) > 0]
+        else:
+            pred_str = predictions
+            label_str = labels
+
+        wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)
+        cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)
+
+        if return_preds_labels:
+            return {"wer": wer, "cer": cer}, predictions, labels
+        else:
+            return {"wer": wer, "cer": cer}
+
+    def update_training_state(training_state, train_metrics, eval_metrics, step):
+        safe_value = lambda x: float(x.tolist() if isinstance(x, jnp.ndarray) else x)
+        state = {"step": step}
+        eval_lines = training_state["eval_lines"]
+       
+        train_metrics = get_metrics(train_metrics)
+        train_metrics_dict = {}
+        for metric_name, values in train_metrics.items():
+            tag = f"train_{metric_name}"
+            for i, value in enumerate(values):
+                train_metrics_dict[step - len(values) + i + 1] = {tag: safe_value(value)}
+
+        eval_metrics_dict = {}
+        for metric_name, value in eval_metrics.items():
+            tag = f"eval_{metric_name}"
+            eval_metrics_dict.update({
+                "step": step,
+                tag: safe_value(value),
+            })
+            if step in train_metrics_dict:
+                eval_metrics_dict.update(train_metrics_dict[step])
+        eval_lines.append(eval_metrics_dict)
+        return {**state, "eval_lines": eval_lines}
+
+    def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=None, labels=None):
+        summary_writer.scalar("train_time", train_time, step)
+
+        train_metrics = get_metrics(train_metrics)
+        for key, vals in train_metrics.items():
+            tag = f"train_{key}"
+            for i, val in enumerate(vals):
+                summary_writer.scalar(tag, val, step - len(vals) + i + 1)
+
+        for metric_name, value in eval_metrics.items():
+            summary_writer.scalar(f"eval_{metric_name}", value, step)
+        
+        # Log evaluation predictions
+        if predictions and labels:
+            df = pd.DataFrame({
+                "references": labels,
+                "predictions": predictions,
+            })
+            df["wer"] = df.apply(lambda row: metric_wer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            df["cer"] = df.apply(lambda row: metric_cer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            markdown_table = df.to_markdown(index=False)
+            eval_metrics_table = pd.DataFrame.from_dict([{"step": step, **eval_metrics}]).to_markdown(index=False)
+            summary_writer.text("eval_predictions", eval_metrics_table + "\n\n" + markdown_table, step)
+            # External logging function
+            if data_args.log_eval_predictions_fn:
+                module, fname = data_args.log_eval_predictions_fn.rsplit('.', 1)
+                fn = getattr(import_module(module), fname)
+                fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=predictions, labels=labels, training_args=training_args)
+
+    # Save feature extractor, tokenizer and config
+    feature_extractor.save_pretrained(training_args.output_dir)
+    tokenizer.save_pretrained(training_args.output_dir)
+    config.save_pretrained(training_args.output_dir)
+
+    processor = AutoProcessor.from_pretrained(training_args.output_dir)
+
+    data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(
+        processor=processor,
+        decoder_start_token_id=model.config.decoder_start_token_id,
+        input_padding="longest",
+        target_padding="longest",
+        max_target_length=max_label_length,
+        pad_input_to_multiple_of=pad_input_to_multiple_of,
+        pad_target_to_multiple_of=pad_target_to_multiple_of if pad_target_to_multiple_of else max_label_length,
+    )
+
+    # Enable tensorboard only on the master node
+    has_tensorboard = is_tensorboard_available()
+    if has_tensorboard and current_host_idx == 0:
+        try:
+            # TODO: Decouple wandb from tensorboard
+            import wandb
+
+            has_wandb = True
+        except ImportError:
+            has_wandb = False
+            if data_args.wandb_entity is not None or data_args.wandb_project is not None:
+                logger.warning(
+                    f"Unable to display metrics through Weights & Biases because some packages are not installed: {ie}"
+                )
+        try:
+            if has_wandb:
+                wandb.tensorboard.patch(root_logdir=output_dir / "runs")
+                wandb.init(
+                    entity=data_args.wandb_entity,
+                    project=data_args.wandb_project,
+                    name=training_args.run_name,
+                    notes=data_args.run_description,
+                    save_code=True,
+                    sync_tensorboard=True,
+                )
+                wandb.config.update(training_args)
+                wandb.config.update(model_args)
+                wandb.config.update(data_args)
+            from flax.metrics.tensorboard import SummaryWriter
+
+            summary_writer = SummaryWriter(
+                log_dir=output_dir / "runs" / f"{datetime.now():%b%d_%H-%M-%S}_{socket.gethostname()}")
+        except ImportError as ie:
+            has_tensorboard = False
+            logger.warning(
+                f"Unable to display metrics through TensorBoard because some packages are not installed: {ie}"
+            )
+    else:
+        logger.warning(
+            "Unable to display metrics through TensorBoard because the package is not installed: "
+            "Please run pip install tensorboard to enable."
+        )
+
+    # Initialize our training
+    rng = jax.random.PRNGKey(training_args.seed)
+    rng, dropout_rng = jax.random.split(rng)
+
+    # Store some constant
+    train_batch_size = int(
+        training_args.per_device_train_batch_size) * jax.device_count()
+    eval_batch_size = int(
+        training_args.per_device_eval_batch_size) * jax.device_count()
+
+    # Create learning rate schedule
+    lr_scheduler_types = {"linear", "constant", "constant_with_warmup"}
+    if training_args.lr_scheduler_type not in lr_scheduler_types:
+        raise ValueError(
+            f"lr_scheduler_type of type {training_args.lr_scheduler_type} not supported, choose from {lr_scheduler_types}."
+        )
+    elif training_args.lr_scheduler_type == "constant":
+        warmup_init_value = training_args.learning_rate
+        decay_end_value = training_args.learning_rate
+    elif training_args.lr_scheduler_type == "constant_with_warmup":
+        warmup_init_value = 0.0
+        decay_end_value = training_args.learning_rate
+    else:
+        warmup_init_value = 0.0
+        decay_end_value = 0.0
+        
+    linear_decay_lr_schedule_fn = create_learning_rate_fn(
+        data_args.num_train_steps,
+        training_args.warmup_steps,
+        training_args.learning_rate,
+        start_step=training_state["step"],
+        warmup_init_value=warmup_init_value,
+        decay_end_value=decay_end_value
+    )
+    
+    # We use Optax's "masking" functionality to not apply weight decay
+    # to bias and LayerNorm scale parameters. decay_mask_fn returns a
+    # mask boolean with the same structure as the parameters.
+    # The mask is True for parameters that should be decayed.
+    def decay_mask_fn(params):
+        flat_params = traverse_util.flatten_dict(params)
+        # Find out all LayerNorm parameters
+        layer_norm_candidates = ["layernorm", "layer_norm", "ln"]
+        layer_norm_named_params = set(
+            [
+                layer[-2:]
+                for layer_norm_name in layer_norm_candidates
+                for layer in flat_params.keys()
+                if layer_norm_name in "".join(layer).lower()
+            ]
+        )
+        flat_mask = {path: (path[-1] != "bias" and path[-2:]
+                            not in layer_norm_named_params) for path in flat_params}
+        return traverse_util.unflatten_dict(flat_mask)
+    
+    # Create adam optimizer
+    adamw = optax.adamw(
+        learning_rate=linear_decay_lr_schedule_fn,
+        b1=training_args.adam_beta1,
+        b2=training_args.adam_beta2,
+        eps=training_args.adam_epsilon,
+        weight_decay=training_args.weight_decay,
+        mask=decay_mask_fn,
+    )
+
+    # Setup train state
+    state = TrainState.create(
+        apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)
+
+    # Label smoothed cross entropy
+    def loss_fn(logits, labels, label_smoothing_factor=0.0):
+        """
+        The label smoothing implementation is adapted from Flax's official example:
+        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104
+        """
+        vocab_size = logits.shape[-1]
+        confidence = 1.0 - label_smoothing_factor
+        low_confidence = (1.0 - confidence) / (vocab_size - 1)
+        normalizing_constant = -(
+            confidence * jnp.log(confidence) + (vocab_size - 1) *
+            low_confidence * jnp.log(low_confidence + 1e-20)
+        )
+        soft_labels = onehot(labels, vocab_size,
+                             on_value=confidence, off_value=low_confidence)
+
+        loss = optax.softmax_cross_entropy(logits, soft_labels)
+        loss = loss - normalizing_constant
+
+        # Ignore padded tokens from loss, i.e. where labels are not set to -100
+        padding_mask = labels >= 0
+        loss = loss * padding_mask
+        loss = loss.sum()
+        num_labels = padding_mask.sum()
+        return loss, num_labels
+
+    # Define gradient update step fn
+    def train_step(state, batch, label_smoothing_factor=0.0):
+        
+        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)
+
+        def compute_loss(params):
+            labels = batch.pop("labels")
+            logits = state.apply_fn(
+                **batch, params=params, dropout_rng=dropout_rng, train=True)[0]
+            loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+            return loss, num_labels
+
+        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)
+        (loss, num_labels), grad = grad_fn(state.params)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        # True grad = total grad / total samples
+        grad = jax.lax.psum(grad, "batch")
+        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)
+        new_state = state.apply_gradients(
+            grads=grad, dropout_rng=new_dropout_rng)
+
+        metrics = {"loss": loss,
+                   "learning_rate": linear_decay_lr_schedule_fn(state.step)}
+
+        return new_state, metrics
+
+    # Define eval fn
+    def eval_step(params, batch, label_smoothing_factor=0.0):
+        labels = batch.pop("labels")
+        logits = model(**batch, params=params, train=False)[0]
+
+        loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        metrics = {"loss": loss}
+        return metrics
+
+    # Define generation function
+    num_beams = model_args.num_beams if model_args.num_beams is not None else model.config.num_beams
+    gen_kwargs = {"max_length": max_label_length, "num_beams": num_beams}
+
+    def generate_step(params, batch):
+        model.params = params
+        output_ids = model.generate(batch[model_input_name], attention_mask=batch.get(
+            "attention_mask"), **gen_kwargs)
+        return output_ids.sequences
+
+    # Create parallel version of the train and eval step
+    p_train_step = jax.pmap(
+        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch", donate_argnums=(0, )
+    )
+    p_eval_step = jax.pmap(partial(
+        eval_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch")
+    p_generate_step = jax.pmap(generate_step, "batch")
+
+    # Replicate the train state on each device
+    state = state.replicate()
+    
+    # Logging
+    logger.info("***** Running training *****")
+    logger.info(
+        f"  Dataset name = {data_args.dataset_name}")
+    logger.info(
+        f"  Dataset config name = {data_args.dataset_config_name}")
+    logger.info(
+        f"  Learning rate = {training_args.learning_rate}")
+    logger.info(
+        f"  Scheduler = {training_args.lr_scheduler_type}")
+    logger.info(
+        f"  Num examples = {data_args.num_train_steps * train_batch_size}")
+    if num_of_hosts > 1:
+        logger.info(
+            f"  Number of hosts = {num_of_hosts}")
+        logger.info(
+            f"  Current host idx = {current_host_idx}")
+    logger.info(
+        f"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}")
+    logger.info(
+        f"  Total train batch size per node (w. parallel & distributed) = {train_batch_size // num_of_hosts}")
+    logger.info(
+        f"  Total train batch size (w. parallel & distributed) = {train_batch_size}")
+    logger.info(f"  Total optimization steps = {data_args.num_train_steps - training_state['step']}")
+    if training_state['step'] > 0:
+        logger.info(f"  ↪ Starting at {str(training_state['step'])} and finishing at {str(data_args.num_train_steps)}")
+
+    train_time = 0
+
+    # Training summary
+    language_code = None  # Maybe 'multilingual'?
+    if data_args.language is not None:
+        language = data_args.language.lower()
+        if language in TO_LANGUAGE_CODE:
+            language_code = TO_LANGUAGE_CODE[language]
+        elif len(language) == 2:
+            language_code = language
+    training_summary = {
+        "model_name": repo_name.split("/")[-1],
+        "language": language_code,
+        "tags": ["audio", "asr", "automatic-speech-recognition", "hf-asr-leaderboard"],
+        "license": "apache-2.0",
+        "finetuned_from": model_args.model_name_or_path,
+        "tasks": ["asr"],
+        "dataset": data_args.dataset_name,
+        "dataset_args": {"name": data_args.dataset_config_name},
+        "source": "flax",
+        "eval_lines": [],
+        "eval_results": None,
+        "hyperparameters": {
+            "learning_rate": training_args.learning_rate,
+            "lr_scheduler_type": training_args.lr_scheduler_type,
+            "per_device_train_batch_size": training_args.per_device_train_batch_size,
+            "total_train_batch_size_per_node": train_batch_size // num_of_hosts,
+            "total_train_batch_size": train_batch_size,
+            "total_optimization_steps": data_args.num_train_steps - training_state['step'],
+            "starting_optimization_step": training_state['step'] if training_state['step'] > 0 else None,
+            "finishing_optimization_step": data_args.num_train_steps,
+            "num_train_dataset_workers": f"{num_workers}",
+            "total_num_training_examples": data_args.num_train_steps * train_batch_size,
+        },
+        # TODO: Adapt https://github.com/huggingface/transformers/blob/main/src/transformers/modelcard.py#L855
+        # "hyperparameters": training_args.to_sanitized_dict()
+    }
+    
+    # Create README if it does not exist
+    readme = output_dir / "README.md"
+    if not readme.exists():
+        readme.write_text(TrainingSummary(**training_summary).to_model_card())
+    
+    # ======================== Training ================================
+    train_start = time.time()
+
+    train_metrics = []
+    epoch = 0
+    train_dataset = vectorized_datasets["train"].shuffle(seed=training_args.seed, buffer_size=data_args.shuffle_buffer_size)
+    
+    # Split by node
+    train_dataset = split_dataset_by_node(train_dataset, rank=current_host_idx, world_size=num_of_hosts)   
+    
+    if train_dataset.n_shards < data_args.preprocessing_num_workers:
+        num_workers = train_dataset.n_shards
+
+    logger.info(f"  Number of train dataset workers = {num_workers} {'(Capped by the number of dataset shards)' if train_dataset.n_shards < data_args.preprocessing_num_workers else ''} {'(ADVICE: In most cases you will speed up training considerably if you increase the value of --preprocessing_num_workers!)' if num_workers < 10 else ''}")
+ 
+    eval_dataset = vectorized_datasets["eval"]
+    train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+    
+    if not training_args.ignore_data_skip and training_state["step"] > 0:
+        logger.info(
+            f"  Will skip the first {training_state['step']} steps. If this takes a lot of time,"
+            " you can add the `--ignore_data_skip` flag to your launch command, but you will resume the"
+            " training on data already seen by your model."
+        )
+        for step in tqdm(range(training_state["step"]), desc=f"Skipping data for {training_state['step']} steps...", position=1, leave=False):
+            try:
+                samples = next(train_loader)
+            except StopIteration:
+                epoch += 1
+                train_dataset.set_epoch(epoch)
+                train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+                samples = next(train_loader)
+            batch = data_collator(samples)
+            # batch = shard(batch.data)
+
+    for step in tqdm(range(data_args.num_train_steps), desc="Training...", position=1, leave=False):
+        # Skip initial steps if these are specified. 
+        if step < training_state["step"]:
+            continue
+        
+        # =========================== Training ===========================
+        try:
+            samples = next(train_loader)
+        except StopIteration:
+            epoch += 1
+            train_dataset.set_epoch(epoch)
+            train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+            samples = next(train_loader)
+            logger.info(
+                f"Completed epoch ({epoch} | Loss: {train_metric['loss']}, Learning Rate:"
+                f" {train_metric['learning_rate']})"
+            )
+
+        batch = data_collator(samples)
+        batch = shard(batch.data)
+        
+        state, train_metric = p_train_step(state, batch)
+        
+        train_metrics.append(train_metric)
+        
+        train_time += time.time() - train_start
+        train_metric = unreplicate(train_metric)
+
+        # ========================== Evaluating ==========================
+        # Evaluate at each eval_steps, and at the end of training at num_train_steps
+        if step % training_args.eval_steps == 0 or step == data_args.num_train_steps - 1:
+            logger.info(
+                f"Starting evaluation at step {step} of num_training_step {data_args.num_train_steps} steps. Planned evaluation every {training_args.eval_steps} steps." 
+            )
+            eval_metrics = []
+            eval_preds = []
+            eval_labels = []
+            eval_loader = data_loader(eval_dataset, eval_batch_size, drop_last=False)
+            if data_args.max_eval_samples:
+                max_eval_steps_iter = range(1 + data_args.max_eval_samples // eval_batch_size)
+            else:
+                max_eval_steps_iter = itertools.repeat(None)
+            for _ in tqdm(max_eval_steps_iter, desc="Evaluating...", position=2, leave=False):
+                # Model forward
+                try:
+                    samples = next(eval_loader)
+                except StopIteration:
+                    break
+                batch = data_collator(samples)
+                
+                labels = batch["labels"]
+
+                metrics = pad_shard_unpad(p_eval_step, static_return=True)(
+                    state.params, batch.data, min_device_batch=training_args.per_device_eval_batch_size
+                )
+                eval_metrics.append(metrics)
+
+                # Generation
+                if training_args.predict_with_generate:
+                    generated_ids = pad_shard_unpad(
+                        p_generate_step)(state.params, batch.data)
+                    eval_preds.extend(jax.device_get(
+                        generated_ids.reshape(-1, gen_kwargs["max_length"])))
+                    eval_labels.extend(labels)
+
+            # Normalize eval metrics
+            eval_metrics = get_metrics(eval_metrics)
+            eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)
+
+            # Compute metrics
+            metric_desc = ""
+            if training_args.predict_with_generate:
+                metric_values, pred_str, label_str = compute_metrics(
+                    eval_preds, eval_labels, return_preds_labels=True
+                )
+                eval_metrics.update(metric_values)
+                metric_desc = " | ".join(
+                    [f"Eval {key}: {value}" for key, value in metric_values.items()])
+
+            # Print metrics
+            desc = f"Step: {step} | Epoch: {epoch} (Eval Loss: {eval_metrics['loss']} | {metric_desc})"
+            logger.info(desc)
+
+            # Update training state
+            training_state = update_training_state(
+                training_state,
+                train_metrics,
+                eval_metrics,
+                step,
+            )
+
+            # Save metrics
+            if has_tensorboard and current_host_idx == 0:
+                log_max_predictions = data_args.log_max_eval_predictions if data_args.log_max_eval_predictions else 0
+                write_metric(
+                    summary_writer,
+                    train_metrics,
+                    eval_metrics,
+                    train_time,
+                    step,
+                    predictions=pred_str[:log_max_predictions],
+                    labels=label_str[:log_max_predictions]
+                )
+
+            # Save checkpoint at each eval_steps and push checkpoint to the hub
+            if current_host_idx  == 0:
+                params = jax.device_get(
+                    jax.tree_util.tree_map(lambda x: x[0], state.params))
+                model.save_pretrained(training_args.output_dir, params=params)
+                tokenizer.save_pretrained(training_args.output_dir)
+                # Report eval results if training is done
+                if step == data_args.num_train_steps - 1:
+                    training_summary["eval_results"] = training_state["eval_lines"][-1]
+                else:
+                    training_summary.update({"eval_lines": training_state["eval_lines"]})
+                (output_dir / "training_state.bin").write_text(json.dumps(training_state))
+                # Write model card
+                readme.write_text(TrainingSummary(**training_summary).to_model_card())
+                if training_args.push_to_hub:
+                    repo.push_to_hub(
+                        commit_message=f"Saving weights and logs of step {step} - epoch {epoch}", blocking=False)
+
+if __name__ == "__main__":
+    main()
diff --git a/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/config.yaml b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/config.yaml
new file mode 100644
index 0000000..67a23df
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/config.yaml
@@ -0,0 +1,512 @@
+wandb_version: 1
+
+__cached__setup_devices:
+  desc: null
+  value: cpu
+_n_gpu:
+  desc: null
+  value: 0
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.14.0
+    code_path: code/run_flax_speech_recognition_seq2seq_streaming.py
+    framework: huggingface
+    huggingface_version: 4.28.0.dev0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.10
+    start_time: 1681312570.007933
+    t:
+      1:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      2:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      3:
+      - 13
+      - 23
+      - 34
+      4: 3.8.10
+      5: 0.14.0
+      6: 4.28.0.dev0
+      8:
+      - 5
+adafactor:
+  desc: null
+  value: false
+adam_beta1:
+  desc: null
+  value: 0.9
+adam_beta2:
+  desc: null
+  value: 0.999
+adam_epsilon:
+  desc: null
+  value: 1.0e-08
+audio_column_name:
+  desc: null
+  value: audio
+auto_find_batch_size:
+  desc: null
+  value: false
+bf16:
+  desc: null
+  value: false
+bf16_full_eval:
+  desc: null
+  value: false
+cache_dir:
+  desc: null
+  value: null
+config_name:
+  desc: null
+  value: null
+data_seed:
+  desc: null
+  value: null
+dataloader_drop_last:
+  desc: null
+  value: false
+dataloader_num_workers:
+  desc: null
+  value: 0
+dataloader_pin_memory:
+  desc: null
+  value: true
+dataset_cache_dir:
+  desc: null
+  value: null
+dataset_config_name:
+  desc: null
+  value: null
+dataset_name:
+  desc: null
+  value: NbAiLab/NCC_speech_all_v5
+ddp_bucket_cap_mb:
+  desc: null
+  value: null
+ddp_find_unused_parameters:
+  desc: null
+  value: null
+ddp_timeout:
+  desc: null
+  value: 1800
+debug:
+  desc: null
+  value: []
+deepspeed:
+  desc: null
+  value: null
+disable_tqdm:
+  desc: null
+  value: false
+do_eval:
+  desc: null
+  value: true
+do_lower_case:
+  desc: null
+  value: false
+do_normalize_eval:
+  desc: null
+  value: true
+do_predict:
+  desc: null
+  value: false
+do_remove_punctuation:
+  desc: null
+  value: false
+do_train:
+  desc: null
+  value: true
+dtype:
+  desc: null
+  value: bfloat16
+eval_accumulation_steps:
+  desc: null
+  value: null
+eval_delay:
+  desc: null
+  value: 0
+eval_split_name:
+  desc: null
+  value: validation
+eval_steps:
+  desc: null
+  value: 10000
+evaluation_strategy:
+  desc: null
+  value: IntervalStrategy.NO
+feature_extractor_name:
+  desc: null
+  value: null
+fp16:
+  desc: null
+  value: false
+fp16_backend:
+  desc: null
+  value: auto
+fp16_full_eval:
+  desc: null
+  value: false
+fp16_opt_level:
+  desc: null
+  value: O1
+fsdp:
+  desc: null
+  value: []
+fsdp_config:
+  desc: null
+  value:
+    fsdp_min_num_params: 0
+    xla: false
+    xla_fsdp_grad_ckpt: false
+fsdp_min_num_params:
+  desc: null
+  value: 0
+fsdp_transformer_layer_cls_to_wrap:
+  desc: null
+  value: null
+full_determinism:
+  desc: null
+  value: false
+generation_config:
+  desc: null
+  value: null
+generation_max_length:
+  desc: null
+  value: null
+generation_num_beams:
+  desc: null
+  value: null
+gradient_accumulation_steps:
+  desc: null
+  value: 1
+gradient_checkpointing:
+  desc: null
+  value: false
+greater_is_better:
+  desc: null
+  value: null
+group_by_length:
+  desc: null
+  value: false
+half_precision_backend:
+  desc: null
+  value: auto
+hub_model_id:
+  desc: null
+  value: NbAiLab/scream_large_oct_debug
+hub_private_repo:
+  desc: null
+  value: true
+hub_strategy:
+  desc: null
+  value: HubStrategy.EVERY_SAVE
+hub_token:
+  desc: null
+  value: null
+ignore_data_skip:
+  desc: null
+  value: true
+include_inputs_for_metrics:
+  desc: null
+  value: false
+jit_mode_eval:
+  desc: null
+  value: false
+label_names:
+  desc: null
+  value: null
+label_smoothing_factor:
+  desc: null
+  value: 0.0
+language:
+  desc: null
+  value: Norwegian
+learning_rate:
+  desc: null
+  value: 5.0e-06
+length_column_name:
+  desc: null
+  value: length
+load_best_model_at_end:
+  desc: null
+  value: false
+local_rank:
+  desc: null
+  value: -1
+log_eval_predictions_fn:
+  desc: null
+  value: log_predictions.write_predictions
+log_level:
+  desc: null
+  value: passive
+log_level_replica:
+  desc: null
+  value: warning
+log_max_eval_predictions:
+  desc: null
+  value: 100
+log_on_each_node:
+  desc: null
+  value: true
+logging_dir:
+  desc: null
+  value: ../../scream_large_oct_debug/runs/Apr12_15-14-58_t1v-n-0a06f6ef-w-0
+logging_first_step:
+  desc: null
+  value: false
+logging_nan_inf_filter:
+  desc: null
+  value: true
+logging_steps:
+  desc: null
+  value: 500
+logging_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+lr_scheduler_type:
+  desc: null
+  value: SchedulerType.LINEAR
+max_duration_in_seconds:
+  desc: null
+  value: 30.0
+max_eval_samples:
+  desc: null
+  value: null
+max_grad_norm:
+  desc: null
+  value: 1.0
+max_label_length:
+  desc: null
+  value: 128
+max_steps:
+  desc: null
+  value: -1
+max_train_samples:
+  desc: null
+  value: null
+metric_for_best_model:
+  desc: null
+  value: null
+min_duration_in_seconds:
+  desc: null
+  value: 0.0
+model_name_or_path:
+  desc: null
+  value: openai/whisper-large-v2
+model_revision:
+  desc: null
+  value: main
+mp_parameters:
+  desc: null
+  value: ''
+no_cuda:
+  desc: null
+  value: false
+num_beams:
+  desc: null
+  value: 5
+num_train_epochs:
+  desc: null
+  value: 3.0
+num_train_steps:
+  desc: null
+  value: 50000
+optim:
+  desc: null
+  value: OptimizerNames.ADAMW_HF
+optim_args:
+  desc: null
+  value: null
+output_dir:
+  desc: null
+  value: ../../scream_large_oct_debug
+overwrite_cache:
+  desc: null
+  value: false
+overwrite_output_dir:
+  desc: null
+  value: true
+pad_input_to_multiple_of:
+  desc: null
+  value: null
+pad_target_to_multiple_of:
+  desc: null
+  value: null
+past_index:
+  desc: null
+  value: -1
+per_device_eval_batch_size:
+  desc: null
+  value: 5
+per_device_train_batch_size:
+  desc: null
+  value: 5
+per_gpu_eval_batch_size:
+  desc: null
+  value: null
+per_gpu_train_batch_size:
+  desc: null
+  value: null
+predict_with_generate:
+  desc: null
+  value: true
+prediction_loss_only:
+  desc: null
+  value: false
+preprocessing_num_workers:
+  desc: null
+  value: 32
+push_to_hub:
+  desc: null
+  value: true
+push_to_hub_model_id:
+  desc: null
+  value: null
+push_to_hub_organization:
+  desc: null
+  value: null
+push_to_hub_token:
+  desc: null
+  value: null
+ray_scope:
+  desc: null
+  value: last
+remove_unused_columns:
+  desc: null
+  value: true
+report_to:
+  desc: null
+  value:
+  - tensorboard
+  - wandb
+resume_from_checkpoint:
+  desc: null
+  value: 'True'
+run_description:
+  desc: null
+  value: A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear
+    decay on the all_v5-corpus.
+run_name:
+  desc: null
+  value: ScreamLarge - debug_beam5_long
+save_on_each_node:
+  desc: null
+  value: false
+save_steps:
+  desc: null
+  value: 500
+save_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+save_total_limit:
+  desc: null
+  value: null
+seed:
+  desc: null
+  value: 42
+sharded_ddp:
+  desc: null
+  value: []
+shuffle_buffer_size:
+  desc: null
+  value: 500
+skip_memory_metrics:
+  desc: null
+  value: true
+sortish_sampler:
+  desc: null
+  value: false
+streaming:
+  desc: null
+  value: true
+task:
+  desc: null
+  value: transcribe
+text_column:
+  desc: null
+  value: null
+text_column_name:
+  desc: null
+  value: text
+tf32:
+  desc: null
+  value: null
+tokenizer_name:
+  desc: null
+  value: null
+torch_compile:
+  desc: null
+  value: false
+torch_compile_backend:
+  desc: null
+  value: null
+torch_compile_mode:
+  desc: null
+  value: null
+torchdynamo:
+  desc: null
+  value: null
+tpu_metrics_debug:
+  desc: null
+  value: false
+tpu_num_cores:
+  desc: null
+  value: null
+train_split_name:
+  desc: null
+  value: train
+use_auth_token:
+  desc: null
+  value: true
+use_fast_tokenizer:
+  desc: null
+  value: true
+use_ipex:
+  desc: null
+  value: false
+use_legacy_prediction_loop:
+  desc: null
+  value: false
+use_mps_device:
+  desc: null
+  value: false
+wandb_entity:
+  desc: null
+  value: nbailab
+wandb_project:
+  desc: null
+  value: Scream - septimus
+warmup_ratio:
+  desc: null
+  value: 0.0
+warmup_steps:
+  desc: null
+  value: 10000
+weight_decay:
+  desc: null
+  value: 0.0
+xpu_backend:
+  desc: null
+  value: null
diff --git a/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/events.out.tfevents.1681312571.t1v-n-0a06f6ef-w-0.1558125.0.v2 b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/events.out.tfevents.1681312571.t1v-n-0a06f6ef-w-0.1558125.0.v2
new file mode 120000
index 0000000..e694788
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/events.out.tfevents.1681312571.t1v-n-0a06f6ef-w-0.1558125.0.v2
@@ -0,0 +1 @@
+/home/perk/models/scream_large_oct_debug/runs/Apr12_15-16-11_t1v-n-0a06f6ef-w-0/events.out.tfevents.1681312571.t1v-n-0a06f6ef-w-0.1558125.0.v2
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/requirements.txt b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/requirements.txt
new file mode 100644
index 0000000..34d1446
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/requirements.txt
@@ -0,0 +1,140 @@
+absl-py==1.4.0
+aiohttp==3.8.4
+aiosignal==1.3.1
+appdirs==1.4.4
+astunparse==1.6.3
+async-timeout==4.0.2
+attrs==22.2.0
+audioread==3.0.0
+cached-property==1.5.2
+cachetools==5.3.0
+certifi==2022.12.7
+cffi==1.15.1
+charset-normalizer==3.1.0
+chex==0.1.7
+click==8.1.3
+cmake==3.26.1
+datasets==2.11.0
+decorator==5.1.1
+dill==0.3.6
+dm-tree==0.1.8
+docker-pycreds==0.4.0
+etils==1.1.1
+evaluate==0.4.0
+filelock==3.10.7
+flatbuffers==23.3.3
+flax==0.6.8
+frozenlist==1.3.3
+fsspec==2023.3.0
+gast==0.4.0
+gitdb==4.0.10
+gitpython==3.1.31
+google-auth-oauthlib==1.0.0
+google-auth==2.17.1
+google-pasta==0.2.0
+grpcio==1.53.0
+h5py==3.8.0
+huggingface-hub==0.13.3
+idna==3.4
+importlib-metadata==6.1.0
+importlib-resources==5.12.0
+jax==0.4.8
+jaxlib==0.4.7
+jinja2==3.1.2
+jiwer==3.0.1
+joblib==1.2.0
+keras==2.12.0
+lazy-loader==0.2
+libclang==16.0.0
+librosa==0.10.0.post2
+libtpu-nightly==0.1.dev20230327
+lit==16.0.0
+llvmlite==0.39.1
+markdown-it-py==2.2.0
+markdown==3.4.3
+markupsafe==2.1.2
+mdurl==0.1.2
+ml-dtypes==0.0.4
+mpmath==1.3.0
+msgpack==1.0.5
+multidict==6.0.4
+multiprocess==0.70.14
+nest-asyncio==1.5.6
+networkx==3.0
+numba==0.56.4
+numpy==1.23.5
+nvidia-cublas-cu11==11.10.3.66
+nvidia-cuda-cupti-cu11==11.7.101
+nvidia-cuda-nvrtc-cu11==11.7.99
+nvidia-cuda-runtime-cu11==11.7.99
+nvidia-cudnn-cu11==8.5.0.96
+nvidia-cufft-cu11==10.9.0.58
+nvidia-curand-cu11==10.2.10.91
+nvidia-cusolver-cu11==11.4.0.1
+nvidia-cusparse-cu11==11.7.4.91
+nvidia-nccl-cu11==2.14.3
+nvidia-nvtx-cu11==11.7.91
+oauthlib==3.2.2
+opt-einsum==3.3.0
+optax==0.1.4
+orbax==0.1.7
+packaging==23.0
+pandas==1.5.3
+pathtools==0.1.2
+pip==23.0.1
+pkg-resources==0.0.0
+pooch==1.6.0
+protobuf==4.22.1
+psutil==5.9.4
+pyarrow==11.0.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pycparser==2.21
+pydub==0.25.1
+pygments==2.14.0
+python-dateutil==2.8.2
+pytz==2023.3
+pyyaml==6.0
+rapidfuzz==2.13.7
+regex==2023.3.23
+requests-oauthlib==1.3.1
+requests==2.28.2
+responses==0.18.0
+rich==13.3.3
+rsa==4.9
+scikit-learn==1.2.2
+scipy==1.10.1
+sentry-sdk==1.18.0
+setproctitle==1.3.2
+setuptools==44.0.0
+six==1.16.0
+smmap==5.0.0
+soundfile==0.12.1
+soxr==0.3.4
+sympy==1.11.1
+tabulate==0.9.0
+tensorboard-data-server==0.7.0
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.12.1
+tensorflow-estimator==2.12.0
+tensorflow-io-gcs-filesystem==0.32.0
+tensorflow==2.12.0
+tensorstore==0.1.35
+termcolor==2.2.0
+threadpoolctl==3.1.0
+tokenizers==0.13.2
+toolz==0.12.0
+torch==2.0.0
+torchaudio==2.0.1
+tqdm==4.65.0
+transformers==4.28.0.dev0
+triton==2.0.0
+typing-extensions==4.5.0
+urllib3==1.26.15
+wandb==0.14.0
+werkzeug==2.2.3
+wheel==0.40.0
+wrapt==1.14.1
+xxhash==3.2.0
+yarl==1.8.2
+zipp==3.15.0
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/wandb-metadata.json b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/wandb-metadata.json
new file mode 100644
index 0000000..b32353c
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/wandb-metadata.json
@@ -0,0 +1,1300 @@
+{
+    "os": "Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2023-04-12T15:16:11.157494",
+    "startedAt": "2023-04-12T15:16:09.998662",
+    "docker": null,
+    "cuda": null,
+    "args": [
+        "--model_name_or_path",
+        "openai/whisper-large-v2",
+        "--run_name",
+        "ScreamLarge - debug_beam5_long",
+        "--run_description",
+        "A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear decay on the all_v5-corpus.",
+        "--wandb_entity",
+        "nbailab",
+        "--wandb_project",
+        "Scream - septimus",
+        "--dataset_name",
+        "NbAiLab/NCC_speech_all_v5",
+        "--language",
+        "Norwegian",
+        "--text_column_name",
+        "text",
+        "--train_split_name",
+        "train",
+        "--eval_split_name",
+        "validation",
+        "--output_dir",
+        "../../scream_large_oct_debug",
+        "--overwrite_output_dir",
+        "--warmup_steps",
+        "10000",
+        "--do_train",
+        "--do_eval",
+        "--num_train_steps",
+        "50000",
+        "--lr_scheduler_type",
+        "linear",
+        "--eval_steps",
+        "10000",
+        "--learning_rate",
+        "5e-6",
+        "--preprocessing_num_workers",
+        "32",
+        "--per_device_train_batch_size",
+        "5",
+        "--per_device_eval_batch_size",
+        "5",
+        "--predict_with_generate",
+        "--log_max_eval_predictions",
+        "100",
+        "--log_eval_predictions_fn",
+        "log_predictions.write_predictions",
+        "--streaming",
+        "True",
+        "--use_auth_token",
+        "True",
+        "--dtype",
+        "bfloat16",
+        "--hub_private_repo",
+        "True",
+        "--hub_model_id",
+        "NbAiLab/scream_large_oct_debug",
+        "--resume_from_checkpoint",
+        "True",
+        "--num_beams",
+        "5",
+        "--ignore_data_skip",
+        "--push_to_hub"
+    ],
+    "state": "running",
+    "program": "../run_flax_speech_recognition_seq2seq_streaming.py",
+    "codePath": "run_flax_speech_recognition_seq2seq_streaming.py",
+    "git": {
+        "remote": "https://github.com/NbAiLab/nb-whisper.git",
+        "commit": "79a2e1e0a805716cf8788a6da7027a6f074da5a0"
+    },
+    "email": "per@capia.no",
+    "root": "/home/perk/models/nb-whisper",
+    "host": "t1v-n-0a06f6ef-w-0",
+    "username": "perk",
+    "executable": "/home/perk/.whisper/bin/python",
+    "cpu_count": 120,
+    "cpu_count_logical": 240,
+    "cpu_freq": {
+        "current": 2249.9980000000096,
+        "min": 0.0,
+        "max": 0.0
+    },
+    "cpu_freq_per_core": [
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        }
+    ],
+    "disk": {
+        "total": 96.74600601196289,
+        "used": 33.32321548461914
+    },
+    "memory": {
+        "total": 400.47254943847656
+    }
+}
diff --git a/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/wandb-summary.json b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/wandb-summary.json
new file mode 100644
index 0000000..12354d8
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 820}}
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/run-p9nst4gu.wandb b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/run-p9nst4gu.wandb
new file mode 100644
index 0000000..b796282
Binary files /dev/null and b/experiment_scream_octavus/wandb/run-20230412_151609-p9nst4gu/run-p9nst4gu.wandb differ
diff --git a/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/code/run_flax_speech_recognition_seq2seq_streaming.py b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/code/run_flax_speech_recognition_seq2seq_streaming.py
new file mode 100644
index 0000000..fd32b95
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/code/run_flax_speech_recognition_seq2seq_streaming.py
@@ -0,0 +1,1309 @@
+#!/usr/bin/env python
+# coding=utf-8
+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR COND    ITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""
+Fine-tuning the Flax library models for sequence to sequence speech recognition.
+"""
+# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.
+
+import itertools
+import json
+import logging
+import os
+import shutil
+import socket
+import sys
+import tempfile
+import time
+from dataclasses import field
+from datetime import datetime
+from functools import partial
+from importlib import import_module
+from pathlib import Path
+from typing import Any, Callable, Dict, Generator, List, Optional, Union
+
+import flax
+import jax
+import jax.numpy as jnp 
+import numpy as np
+import optax
+import pandas as pd
+import torch
+# from jax.experimental.compilation_cache import compilation_cache; compilation_cache.initialize_cache(tempfile.gettempdir())
+from flax import jax_utils, traverse_util
+from flax.jax_utils import pad_shard_unpad, unreplicate
+from flax.training import train_state
+from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key
+from torch.utils.data import IterableDataset
+from tqdm import tqdm
+
+import datasets
+import evaluate
+import transformers
+from datasets import Dataset, DatasetDict, IterableDatasetDict, interleave_datasets, load_dataset
+from datasets.distributed import split_dataset_by_node
+from huggingface_hub import Repository, create_repo
+from transformers import (
+    AutoConfig,
+    AutoFeatureExtractor,
+    AutoProcessor,
+    AutoTokenizer,
+    FlaxAutoModelForSpeechSeq2Seq,
+    HfArgumentParser,
+    Seq2SeqTrainingArguments,
+    is_tensorboard_available,
+)
+from transformers.modelcard import TrainingSummary
+from transformers.models.whisper.english_normalizer import BasicTextNormalizer
+from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE
+from transformers.file_utils import get_full_repo_name
+from transformers.utils import check_min_version, send_example_telemetry
+from transformers.utils.versions import require_version
+
+from flax.training import checkpoints
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+check_min_version("4.27.0.dev0")
+
+require_version("datasets>=1.18.2",
+                "To fix: pip install -r examples/flax/speech-recogintion/requirements.txt")
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+logger = logging.getLogger(__name__)
+
+
+@flax.struct.dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={
+            "help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    feature_extractor_name: Optional[str] = field(
+        default=None, metadata={"help": "feature extractor name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Where to store the pretrained models downloaded from huggingface.co"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={
+            "help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+    dtype: Optional[str] = field(
+        default="float32",
+        metadata={
+            "help": (
+                "Floating-point format in which the model weights should be initialized and trained. Choose one of"
+                " `[float32, float16, bfloat16]`."
+            )
+        },
+    )
+    num_beams: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Number of beams to use for evaluation. This argument will be passed to `model.generate`, "
+                "which is used during evaluation."
+            )
+        },
+    )
+
+
+@flax.struct.dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    dataset_name: str = field(
+        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
+    )
+    dataset_config_name: Optional[str] = field(
+        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
+    )
+    text_column: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "The name of the column in the datasets containing the full texts (for summarization)."},
+    )
+    dataset_cache_dir: Optional[str] = field(
+        default=None, metadata={"help": "Path to cache directory for saving and loading datasets"}
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
+    )
+    preprocessing_num_workers: Optional[int] = field(
+        default=50,
+        metadata={"help": "The number of processes to use for the preprocessing."},
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    audio_column_name: str = field(
+        default="audio",
+        metadata={
+            "help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
+    )
+    text_column_name: str = field(
+        default="text",
+        metadata={
+            "help": "The name of the dataset column containing the text data. Defaults to 'text'"},
+    )
+    max_duration_in_seconds: float = field(
+        default=30.0,
+        metadata={
+            "help": "Filter audio files that are longer than `max_duration_in_seconds` seconds"},
+    )
+    min_duration_in_seconds: float = field(
+        default=0.0,
+        metadata={
+            "help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"},
+    )
+    max_label_length: float = field(
+        default=128,
+        metadata={
+            "help": "Truncate transcriptions that are longer `max_eval_length` tokens."},
+    )
+    pad_input_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the input sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the inputs to max length."
+        },
+    )
+    pad_target_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the target sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the targets to max length."
+        },
+    )
+    train_split_name: str = field(
+        default="train",
+        metadata={
+            "help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
+        },
+    )
+    eval_split_name: str = field(
+        default="validation",
+        metadata={
+            "help": "The name of the evaluation data set split to use (via the datasets library). Defaults to 'validation'"
+        },
+    )
+    do_lower_case: bool = field(
+        default=False,
+        metadata={"help": "Whether the target text should be lower cased."},
+    )
+    do_remove_punctuation: bool = field(
+        default=False,
+        metadata={
+            "help": "Whether the target text should be striped of punctuation."},
+    )
+    do_normalize_eval: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to normalise the references and predictions in the eval WER calculation."},
+    )
+    language: str = field(
+        default=None,
+        metadata={
+            "help": (
+                "Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning "
+                "only. For English speech recognition, it should be set to `None`."
+            )
+        },
+    )
+    task: str = field(
+        default="transcribe",
+        metadata={
+            "help": "Task, either `transcribe` for speech recognition or `translate` for speech translation."},
+    )
+    num_train_steps: int = field(default=50000, metadata={
+                                 "help": "The number of training steps."})
+    shuffle_buffer_size: Optional[int] = field(
+        default=500,
+        metadata={
+            "help": (
+                "The number of streamed examples to download before shuffling them. The large the buffer, "
+                "the closer it is to real offline shuffling."
+            )
+        },
+    )
+    streaming: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use streaming mode to load and pre-process the data."},
+    )
+    log_max_eval_predictions: Optional[int] = field(
+        default=0,
+        metadata={
+            "help": (
+                "Number of label and prediction pairs to write to the summary at each evaluation step."
+            )
+        },
+    )
+    log_eval_predictions_fn: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Python path to function for logging evaluation predictions. It can be an external function like fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions, labels)."
+            )
+        },
+    )
+    run_description: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "A longer description of the run/experiment."
+            )
+        },
+    )
+    wandb_entity: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases username or entity (organization name)."
+            )
+        },
+    )
+    wandb_project: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases project to log metrics to."
+            )
+        },
+    )
+
+
+def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:
+    """
+    Shift label ids one token to the right.
+    """
+    shifted_label_ids = np.zeros_like(label_ids)
+    shifted_label_ids[:, 1:] = label_ids[:, :-1]
+    shifted_label_ids[:, 0] = decoder_start_token_id
+
+    return shifted_label_ids
+
+
+@flax.struct.dataclass
+class FlaxDataCollatorSpeechSeq2SeqWithPadding:
+    """
+    Data collator that will dynamically pad the inputs received.
+    Args:
+        processor ([`Wav2Vec2Processor`])
+            The processor used for proccessing the data.
+        decoder_start_token_id (:obj: `int`)
+            The begin-of-sentence of the decoder.
+        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)
+            among:
+            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
+              sequence if provided).
+            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
+              maximum acceptable input length for the model if that argument is not provided.
+            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
+              different lengths).
+        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).
+            See above for details.
+        max_input_length (:obj:`float`, `optional`):
+            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
+        max_target_length (:obj:`int`, `optional`):
+            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).
+        pad_input_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the input sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+        pad_target_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the target sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+    """
+
+    processor: Any
+    decoder_start_token_id: int
+    input_padding: Union[bool, str] = "longest"
+    target_padding: Union[bool, str] = "max_length"
+    max_input_length: Optional[float] = None
+    max_target_length: Optional[int] = None
+    pad_input_to_multiple_of: Optional[int] = None
+    pad_target_to_multiple_of: Optional[int] = None
+
+    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:
+        model_input_name = self.processor.model_input_names[0]
+        input_features = {model_input_name: features[model_input_name]}
+        label_features = {"input_ids": features["labels"]}
+
+        # reformat list to dict and set to pytorch format
+        batch = self.processor.feature_extractor.pad(
+            input_features,
+            max_length=self.max_input_length,
+            padding=self.input_padding,
+            pad_to_multiple_of=self.pad_input_to_multiple_of,
+            return_tensors="np",
+        )
+
+        labels_batch = self.processor.tokenizer.pad(
+            label_features,
+            max_length=self.max_target_length,
+            padding=self.target_padding,
+            pad_to_multiple_of=self.pad_target_to_multiple_of,
+            return_tensors="np",
+        )
+
+        # if bos token is appended in previous tokenization step,
+        # cut bos token here as it's append later anyways
+        labels = labels_batch["input_ids"]
+        if (labels[:, 0] == self.decoder_start_token_id).all().item():
+            labels = labels[:, 1:]
+            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]
+
+        decoder_input_ids = shift_tokens_right(
+            labels, self.decoder_start_token_id)
+
+        # replace padding with -100 to ignore correctly when computing the loss
+        labels = np.ma.array(labels, mask=np.not_equal(
+            labels_batch.attention_mask, 1))
+        labels = labels.filled(fill_value=-100)
+
+        batch["labels"] = labels
+        batch["decoder_input_ids"] = decoder_input_ids
+        batch["attention_mask"] = labels_batch.attention_mask  # Add attention_mask to the batch
+
+        return batch
+
+
+def load_maybe_streaming_dataset(dataset_name, dataset_config_name, split="train", streaming=True, **kwargs):
+    """
+    Utility function to load a dataset in streaming mode. For datasets with multiple splits,
+    each split is loaded individually and then splits combined by taking alternating examples from
+    each (interleaving).
+    """
+    if "+" in split:
+        # load multiple splits separated by the `+` symbol with streaming mode
+        dataset_splits = [
+            load_dataset(dataset_name, dataset_config_name,
+                         split=split_name, streaming=streaming, **kwargs)
+            for split_name in split.split("+")
+        ]
+        # interleave multiple splits to form one dataset
+        interleaved_dataset = interleave_datasets(dataset_splits)
+        return interleaved_dataset
+    else:
+        # load a single split *with* streaming mode
+        dataset = load_dataset(
+            dataset_name, dataset_config_name, split=split, streaming=streaming, **kwargs)
+        return dataset
+
+
+def collate_batch(samples):
+    return {key: [feature[key] for feature in samples] for key in samples[0]}
+
+def data_loader(
+    dataset: Dataset,
+    batch_size: int,
+    drop_last: bool=True,
+    num_workers: int=0,
+) -> Generator:
+    """
+    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,
+    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.
+    """
+    data_loader_iterator = iter(torch.utils.data.DataLoader(
+        batch_size=batch_size,
+        dataset=dataset.with_format("torch"),
+        num_workers=num_workers,
+        collate_fn=collate_batch,
+        drop_last=drop_last,
+    ))
+    return data_loader_iterator
+
+
+class TrainState(train_state.TrainState):
+    dropout_rng: jnp.ndarray
+
+    def replicate(self):
+        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))
+
+
+def create_learning_rate_fn(
+    num_train_steps: int, num_warmup_steps: int, learning_rate: float, start_step: int=0, warmup_init_value: float=0.0, decay_end_value: float=0.0,
+) -> Callable[[int], jnp.array]:
+    """Returns a linear warmup, linear_decay learning rate function."""
+    warmup_fn = optax.linear_schedule(
+        init_value=warmup_init_value, end_value=learning_rate, transition_steps=num_warmup_steps)
+    decay_fn = optax.linear_schedule(
+        init_value=learning_rate, end_value=decay_end_value, transition_steps=num_train_steps - num_warmup_steps
+    )
+    schedule_fn = optax.join_schedules(
+        schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])
+    
+    def learning_rate_fn(step: int) -> jnp.array:
+        return schedule_fn(step + start_step)
+    
+    return learning_rate_fn
+
+
+def main():
+    # Parse input arguments
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+    parser = HfArgumentParser(
+        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
+
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args = parser.parse_json_file(
+            json_file=os.path.abspath(sys.argv[1]))
+    else:
+        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
+    # information sent is the one passed as arguments along with your JAX/Flax versions.
+    send_example_telemetry("run_speech_recognition_seq2seq",
+                           model_args, data_args, framework="flax")
+
+    # Setup logging
+    # Make one log on every process with the configuration for debugging.
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+    # Set the verbosity to info of the Transformers logger.
+    # We only want one process per machine to log things on the screen.
+
+    # logger.setLevel(logging.INFO if jax.local_devices()[0].id%jax.local_device_count() == 0 else logging.ERROR)
+
+    # logger.setLevel(logging.INFO if jax.process_index()
+    #                == 0 else logging.ERROR)
+    
+    # Number of hosts
+    num_of_hosts = jax.process_count()
+    current_host_idx = jax.process_index()
+
+    if current_host_idx == 0:
+        datasets.utils.logging.set_verbosity_warning()
+        transformers.utils.logging.set_verbosity_info()
+    else:
+        datasets.utils.logging.set_verbosity_error()
+        transformers.utils.logging.set_verbosity_error()
+    
+    logger.setLevel(logging.INFO)
+    logger.info("Training/evaluation parameters %s", training_args)
+
+    if num_of_hosts and not training_args.push_to_hub:
+        logger.warning(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --push_to_hub to be able to save checkpoints to the hub."
+        )
+    if num_of_hosts and not training_args.overwrite_output_dir and training_args.resume_from_checkpoint:
+        logger.error(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --overwrite_output_dir to be able to resume from a pushed checkpoint."
+        )
+        sys.exit(1)
+
+    # Check the output dir is valid
+    if os.path.exists(training_args.output_dir):
+        if (
+            os.listdir(training_args.output_dir)
+            and training_args.do_train
+            and not training_args.overwrite_output_dir
+        ):
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use `--overwrite_output_dir` to overcome."
+            )
+        elif training_args.overwrite_output_dir:
+            logger.warning(f"Removing path {training_args.output_dir}")
+            shutil.rmtree(training_args.output_dir)
+      
+    # Handle the repository creation
+    output_dir = Path(training_args.output_dir)
+    if training_args.push_to_hub:
+        if training_args.hub_model_id is None:
+            repo_name = get_full_repo_name(
+                output_dir.absolute().name,
+                token=training_args.hub_token,
+                organization=training_args.push_to_hub_organization,
+            )
+        else:
+            repo_name = training_args.hub_model_id
+         
+        repo_url = None  
+        while not repo_url:
+            # Workaround for an internal HuggingFace error if the repo is being created by another worker
+            try:
+                repo_url = create_repo(
+                    repo_name, exist_ok=True, token=training_args.hub_token, private=training_args.hub_private_repo
+                )
+            except:
+                time.sleep(1)
+
+        repo = Repository(training_args.output_dir,
+                          clone_from=repo_name, token=training_args.hub_token)
+
+    # Set the model_name_or_path
+    model_name_or_path = model_args.model_name_or_path
+
+    # Try to detect last checkpoint and continue if possible
+    training_state = {"step": 0, "eval_lines": []}
+    if training_args.resume_from_checkpoint:
+        if (output_dir / "flax_model.msgpack").exists() and (output_dir / "training_state.bin").exists():
+            training_state = json.loads((output_dir / "training_state.bin").read_text())
+            model_name_or_path = os.path.join(training_args.output_dir)
+            logger.info(
+                f"Checkpoint detected, resuming training from {training_args.output_dir} at step {training_state['step']}."
+            )
+        else:
+            logger.info(
+                f"No valid checkpoint found in {training_args.output_dir}. Starting from {model_name_or_path}."
+            )
+    
+    
+    # Load dataset
+    raw_datasets = IterableDatasetDict() if data_args.streaming else DatasetDict()
+
+    if training_args.do_train:
+        raw_datasets["train"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.train_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if training_args.do_eval:
+        raw_datasets["eval"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.eval_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if not training_args.do_train and not training_args.do_eval:
+        raise ValueError(
+            "Cannot not train and not do evaluation. At least one of training or evaluation has to be performed."
+        )
+
+    raw_datasets_features = list(
+        next(iter(raw_datasets.values())).features.keys())
+
+    if data_args.audio_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--audio_column_name` to the correct audio column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    if data_args.text_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--text_column_name` to the correct text column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    # Load pretrained model, tokenizer, and feature extractor
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    feature_extractor = AutoFeatureExtractor.from_pretrained(
+        model_args.feature_extractor_name if model_args.feature_extractor_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(
+        model_name_or_path,
+        config=config,
+        dtype=getattr(jnp, model_args.dtype),
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    logger.info(
+        f"Successfully loaded the model '{model_name_or_path}'."
+    )
+    
+    if model.config.decoder_start_token_id is None:
+        raise ValueError(
+            "Make sure that `config.decoder_start_token_id` is correctly defined")
+
+    # Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,
+    # so we just need to set the correct target sampling rate.
+    dataset_sampling_rate = next(
+        iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
+
+    if dataset_sampling_rate != feature_extractor.sampling_rate:
+        raw_datasets = raw_datasets.cast_column(
+            data_args.audio_column_name, datasets.features.Audio(
+                sampling_rate=feature_extractor.sampling_rate)
+        )
+
+    # Preprocessing the datasets.
+    # We need to read the audio files as arrays and tokenize the targets.
+    max_input_length = int(
+        data_args.max_duration_in_seconds * feature_extractor.sampling_rate)
+    min_input_length = int(
+        data_args.min_duration_in_seconds * feature_extractor.sampling_rate)
+    max_label_length = (
+        data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length
+    )
+    pad_input_to_multiple_of = data_args.pad_input_to_multiple_of
+    pad_target_to_multiple_of = data_args.pad_target_to_multiple_of
+    audio_column_name = data_args.audio_column_name
+    num_workers = data_args.preprocessing_num_workers
+    text_column_name = data_args.text_column_name
+    model_input_name = feature_extractor.model_input_names[0]
+    do_lower_case = data_args.do_lower_case
+    do_remove_punctuation = data_args.do_remove_punctuation
+    normalizer = BasicTextNormalizer()  # 'official' text normalizer from OpenAI
+
+    if data_args.language is not None:
+        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)
+        tokenizer.set_prefix_tokens(
+            language=data_args.language, task=data_args.task)
+
+    def prepare_dataset(batch):
+        # Process audio
+        sample = batch[audio_column_name]
+        inputs = feature_extractor(
+            sample["array"], sampling_rate=sample["sampling_rate"])
+        # Process audio length
+        batch[model_input_name] = inputs.get(model_input_name)[0]
+        batch["input_length"] = len(sample["array"])
+
+        # Process targets
+        input_str = batch[text_column_name].lower(
+        ) if do_lower_case else batch[text_column_name]
+        if do_remove_punctuation:
+            input_str = normalizer(input_str).strip()
+        batch["labels"] = tokenizer(input_str).input_ids
+        return batch
+
+    with training_args.main_process_first(desc="dataset map pre-processing"):
+        vectorized_datasets = raw_datasets.map(
+            prepare_dataset,
+            remove_columns=raw_datasets_features,
+        )
+
+    # Filter training data with inputs longer than max_input_length
+    def is_audio_in_length_range(length):
+        return min_input_length < length < max_input_length
+
+    if training_args.do_train:
+        vectorized_datasets["train"] = vectorized_datasets["train"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    if training_args.do_eval:
+        vectorized_datasets["eval"] = vectorized_datasets["eval"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    # Load metrics and write stats
+    metric_wer = evaluate.load("wer")
+    metric_cer = evaluate.load("cer")
+    do_normalize_eval = data_args.do_normalize_eval
+
+    def compute_metrics(pred_ids, label_ids, return_preds_labels=False):
+        # Replace padded labels by the padding token
+        for idx in range(len(label_ids)):
+            label_ids[idx][label_ids[idx] == -100] = tokenizer.pad_token_id
+
+        predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
+        # We do not want to group tokens when computing the metrics
+        labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
+
+        if do_normalize_eval:
+            pred_str = [normalizer(pred) for pred in predictions]
+            label_str = [normalizer(label) for label in labels]
+            # Filtering step to only evaluate the samples that correspond to non-zero references:
+            pred_str = [pred_str[i]
+                        for i in range(len(pred_str)) if len(label_str[i]) > 0]
+            label_str = [label_str[i]
+                         for i in range(len(label_str)) if len(label_str[i]) > 0]
+        else:
+            pred_str = predictions
+            label_str = labels
+
+        wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)
+        cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)
+
+        if return_preds_labels:
+            return {"wer": wer, "cer": cer}, predictions, labels
+        else:
+            return {"wer": wer, "cer": cer}
+
+    def update_training_state(training_state, train_metrics, eval_metrics, step):
+        safe_value = lambda x: float(x.tolist() if isinstance(x, jnp.ndarray) else x)
+        state = {"step": step}
+        eval_lines = training_state["eval_lines"]
+       
+        train_metrics = get_metrics(train_metrics)
+        train_metrics_dict = {}
+        for metric_name, values in train_metrics.items():
+            tag = f"train_{metric_name}"
+            for i, value in enumerate(values):
+                train_metrics_dict[step - len(values) + i + 1] = {tag: safe_value(value)}
+
+        eval_metrics_dict = {}
+        for metric_name, value in eval_metrics.items():
+            tag = f"eval_{metric_name}"
+            eval_metrics_dict.update({
+                "step": step,
+                tag: safe_value(value),
+            })
+            if step in train_metrics_dict:
+                eval_metrics_dict.update(train_metrics_dict[step])
+        eval_lines.append(eval_metrics_dict)
+        return {**state, "eval_lines": eval_lines}
+
+    def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=None, labels=None):
+        summary_writer.scalar("train_time", train_time, step)
+
+        train_metrics = get_metrics(train_metrics)
+        for key, vals in train_metrics.items():
+            tag = f"train_{key}"
+            for i, val in enumerate(vals):
+                summary_writer.scalar(tag, val, step - len(vals) + i + 1)
+
+        for metric_name, value in eval_metrics.items():
+            summary_writer.scalar(f"eval_{metric_name}", value, step)
+        
+        # Log evaluation predictions
+        if predictions and labels:
+            df = pd.DataFrame({
+                "references": labels,
+                "predictions": predictions,
+            })
+            df["wer"] = df.apply(lambda row: metric_wer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            df["cer"] = df.apply(lambda row: metric_cer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            markdown_table = df.to_markdown(index=False)
+            eval_metrics_table = pd.DataFrame.from_dict([{"step": step, **eval_metrics}]).to_markdown(index=False)
+            summary_writer.text("eval_predictions", eval_metrics_table + "\n\n" + markdown_table, step)
+            # External logging function
+            if data_args.log_eval_predictions_fn:
+                module, fname = data_args.log_eval_predictions_fn.rsplit('.', 1)
+                fn = getattr(import_module(module), fname)
+                fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=predictions, labels=labels, training_args=training_args)
+
+    # Save feature extractor, tokenizer and config
+    feature_extractor.save_pretrained(training_args.output_dir)
+    tokenizer.save_pretrained(training_args.output_dir)
+    config.save_pretrained(training_args.output_dir)
+
+    processor = AutoProcessor.from_pretrained(training_args.output_dir)
+
+    data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(
+        processor=processor,
+        decoder_start_token_id=model.config.decoder_start_token_id,
+        input_padding="longest",
+        target_padding="longest",
+        max_target_length=max_label_length,
+        pad_input_to_multiple_of=pad_input_to_multiple_of,
+        pad_target_to_multiple_of=pad_target_to_multiple_of if pad_target_to_multiple_of else max_label_length,
+    )
+
+    # Enable tensorboard only on the master node
+    has_tensorboard = is_tensorboard_available()
+    if has_tensorboard and current_host_idx == 0:
+        try:
+            # TODO: Decouple wandb from tensorboard
+            import wandb
+
+            has_wandb = True
+        except ImportError:
+            has_wandb = False
+            if data_args.wandb_entity is not None or data_args.wandb_project is not None:
+                logger.warning(
+                    f"Unable to display metrics through Weights & Biases because some packages are not installed: {ie}"
+                )
+        try:
+            if has_wandb:
+                wandb.tensorboard.patch(root_logdir=output_dir / "runs")
+                wandb.init(
+                    entity=data_args.wandb_entity,
+                    project=data_args.wandb_project,
+                    name=training_args.run_name,
+                    notes=data_args.run_description,
+                    save_code=True,
+                    sync_tensorboard=True,
+                )
+                wandb.config.update(training_args)
+                wandb.config.update(model_args)
+                wandb.config.update(data_args)
+            from flax.metrics.tensorboard import SummaryWriter
+
+            summary_writer = SummaryWriter(
+                log_dir=output_dir / "runs" / f"{datetime.now():%b%d_%H-%M-%S}_{socket.gethostname()}")
+        except ImportError as ie:
+            has_tensorboard = False
+            logger.warning(
+                f"Unable to display metrics through TensorBoard because some packages are not installed: {ie}"
+            )
+    else:
+        logger.warning(
+            "Unable to display metrics through TensorBoard because the package is not installed: "
+            "Please run pip install tensorboard to enable."
+        )
+
+    # Initialize our training
+    rng = jax.random.PRNGKey(training_args.seed)
+    rng, dropout_rng = jax.random.split(rng)
+
+    # Store some constant
+    train_batch_size = int(
+        training_args.per_device_train_batch_size) * jax.device_count()
+    eval_batch_size = int(
+        training_args.per_device_eval_batch_size) * jax.device_count()
+
+    # Create learning rate schedule
+    lr_scheduler_types = {"linear", "constant", "constant_with_warmup"}
+    if training_args.lr_scheduler_type not in lr_scheduler_types:
+        raise ValueError(
+            f"lr_scheduler_type of type {training_args.lr_scheduler_type} not supported, choose from {lr_scheduler_types}."
+        )
+    elif training_args.lr_scheduler_type == "constant":
+        warmup_init_value = training_args.learning_rate
+        decay_end_value = training_args.learning_rate
+    elif training_args.lr_scheduler_type == "constant_with_warmup":
+        warmup_init_value = 0.0
+        decay_end_value = training_args.learning_rate
+    else:
+        warmup_init_value = 0.0
+        decay_end_value = 0.0
+        
+    linear_decay_lr_schedule_fn = create_learning_rate_fn(
+        data_args.num_train_steps,
+        training_args.warmup_steps,
+        training_args.learning_rate,
+        start_step=training_state["step"],
+        warmup_init_value=warmup_init_value,
+        decay_end_value=decay_end_value
+    )
+    
+    # We use Optax's "masking" functionality to not apply weight decay
+    # to bias and LayerNorm scale parameters. decay_mask_fn returns a
+    # mask boolean with the same structure as the parameters.
+    # The mask is True for parameters that should be decayed.
+    def decay_mask_fn(params):
+        flat_params = traverse_util.flatten_dict(params)
+        # Find out all LayerNorm parameters
+        layer_norm_candidates = ["layernorm", "layer_norm", "ln"]
+        layer_norm_named_params = set(
+            [
+                layer[-2:]
+                for layer_norm_name in layer_norm_candidates
+                for layer in flat_params.keys()
+                if layer_norm_name in "".join(layer).lower()
+            ]
+        )
+        flat_mask = {path: (path[-1] != "bias" and path[-2:]
+                            not in layer_norm_named_params) for path in flat_params}
+        return traverse_util.unflatten_dict(flat_mask)
+    
+    # Create adam optimizer
+    adamw = optax.adamw(
+        learning_rate=linear_decay_lr_schedule_fn,
+        b1=training_args.adam_beta1,
+        b2=training_args.adam_beta2,
+        eps=training_args.adam_epsilon,
+        weight_decay=training_args.weight_decay,
+        mask=decay_mask_fn,
+    )
+
+    # Setup train state
+    state = TrainState.create(
+        apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)
+
+    # Label smoothed cross entropy
+    def loss_fn(logits, labels, label_smoothing_factor=0.0):
+        """
+        The label smoothing implementation is adapted from Flax's official example:
+        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104
+        """
+        vocab_size = logits.shape[-1]
+        confidence = 1.0 - label_smoothing_factor
+        low_confidence = (1.0 - confidence) / (vocab_size - 1)
+        normalizing_constant = -(
+            confidence * jnp.log(confidence) + (vocab_size - 1) *
+            low_confidence * jnp.log(low_confidence + 1e-20)
+        )
+        soft_labels = onehot(labels, vocab_size,
+                             on_value=confidence, off_value=low_confidence)
+
+        loss = optax.softmax_cross_entropy(logits, soft_labels)
+        loss = loss - normalizing_constant
+
+        # Ignore padded tokens from loss, i.e. where labels are not set to -100
+        padding_mask = labels >= 0
+        loss = loss * padding_mask
+        loss = loss.sum()
+        num_labels = padding_mask.sum()
+        return loss, num_labels
+
+    # Define gradient update step fn
+    def train_step(state, batch, label_smoothing_factor=0.0):
+        
+        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)
+
+        def compute_loss(params):
+            labels = batch.pop("labels")
+            logits = state.apply_fn(
+                **batch, params=params, dropout_rng=dropout_rng, train=True)[0]
+            loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+            return loss, num_labels
+
+        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)
+        (loss, num_labels), grad = grad_fn(state.params)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        # True grad = total grad / total samples
+        grad = jax.lax.psum(grad, "batch")
+        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)
+        new_state = state.apply_gradients(
+            grads=grad, dropout_rng=new_dropout_rng)
+
+        metrics = {"loss": loss,
+                   "learning_rate": linear_decay_lr_schedule_fn(state.step)}
+
+        return new_state, metrics
+
+    # Define eval fn
+    def eval_step(params, batch, label_smoothing_factor=0.0):
+        labels = batch.pop("labels")
+        logits = model(**batch, params=params, train=False)[0]
+
+        loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        metrics = {"loss": loss}
+        return metrics
+
+    # Define generation function
+    num_beams = model_args.num_beams if model_args.num_beams is not None else model.config.num_beams
+    gen_kwargs = {"max_length": max_label_length, "num_beams": num_beams}
+
+    def generate_step(params, batch):
+        model.params = params
+        output_ids = model.generate(batch[model_input_name], attention_mask=batch.get(
+            "attention_mask"), **gen_kwargs)
+        return output_ids.sequences
+
+    # Create parallel version of the train and eval step
+    p_train_step = jax.pmap(
+        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch", donate_argnums=(0, )
+    )
+    p_eval_step = jax.pmap(partial(
+        eval_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch")
+    p_generate_step = jax.pmap(generate_step, "batch")
+
+    # Replicate the train state on each device
+    state = state.replicate()
+    
+    # Logging
+    logger.info("***** Running training *****")
+    logger.info(
+        f"  Dataset name = {data_args.dataset_name}")
+    logger.info(
+        f"  Dataset config name = {data_args.dataset_config_name}")
+    logger.info(
+        f"  Learning rate = {training_args.learning_rate}")
+    logger.info(
+        f"  Scheduler = {training_args.lr_scheduler_type}")
+    logger.info(
+        f"  Num examples = {data_args.num_train_steps * train_batch_size}")
+    if num_of_hosts > 1:
+        logger.info(
+            f"  Number of hosts = {num_of_hosts}")
+        logger.info(
+            f"  Current host idx = {current_host_idx}")
+    logger.info(
+        f"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}")
+    logger.info(
+        f"  Total train batch size per node (w. parallel & distributed) = {train_batch_size // num_of_hosts}")
+    logger.info(
+        f"  Total train batch size (w. parallel & distributed) = {train_batch_size}")
+    logger.info(f"  Total optimization steps = {data_args.num_train_steps - training_state['step']}")
+    if training_state['step'] > 0:
+        logger.info(f"  ↪ Starting at {str(training_state['step'])} and finishing at {str(data_args.num_train_steps)}")
+
+    train_time = 0
+
+    # Training summary
+    language_code = None  # Maybe 'multilingual'?
+    if data_args.language is not None:
+        language = data_args.language.lower()
+        if language in TO_LANGUAGE_CODE:
+            language_code = TO_LANGUAGE_CODE[language]
+        elif len(language) == 2:
+            language_code = language
+    training_summary = {
+        "model_name": repo_name.split("/")[-1],
+        "language": language_code,
+        "tags": ["audio", "asr", "automatic-speech-recognition", "hf-asr-leaderboard"],
+        "license": "apache-2.0",
+        "finetuned_from": model_args.model_name_or_path,
+        "tasks": ["asr"],
+        "dataset": data_args.dataset_name,
+        "dataset_args": {"name": data_args.dataset_config_name},
+        "source": "flax",
+        "eval_lines": [],
+        "eval_results": None,
+        "hyperparameters": {
+            "learning_rate": training_args.learning_rate,
+            "lr_scheduler_type": training_args.lr_scheduler_type,
+            "per_device_train_batch_size": training_args.per_device_train_batch_size,
+            "total_train_batch_size_per_node": train_batch_size // num_of_hosts,
+            "total_train_batch_size": train_batch_size,
+            "total_optimization_steps": data_args.num_train_steps - training_state['step'],
+            "starting_optimization_step": training_state['step'] if training_state['step'] > 0 else None,
+            "finishing_optimization_step": data_args.num_train_steps,
+            "num_train_dataset_workers": f"{num_workers}",
+            "total_num_training_examples": data_args.num_train_steps * train_batch_size,
+        },
+        # TODO: Adapt https://github.com/huggingface/transformers/blob/main/src/transformers/modelcard.py#L855
+        # "hyperparameters": training_args.to_sanitized_dict()
+    }
+    
+    # Create README if it does not exist
+    readme = output_dir / "README.md"
+    if not readme.exists():
+        readme.write_text(TrainingSummary(**training_summary).to_model_card())
+    
+    # ======================== Training ================================
+    train_start = time.time()
+
+    train_metrics = []
+    epoch = 0
+    train_dataset = vectorized_datasets["train"].shuffle(seed=training_args.seed, buffer_size=data_args.shuffle_buffer_size)
+    
+    # Split by node
+    train_dataset = split_dataset_by_node(train_dataset, rank=current_host_idx, world_size=num_of_hosts)   
+    
+    if train_dataset.n_shards < data_args.preprocessing_num_workers:
+        num_workers = train_dataset.n_shards
+
+    logger.info(f"  Number of train dataset workers = {num_workers} {'(Capped by the number of dataset shards)' if train_dataset.n_shards < data_args.preprocessing_num_workers else ''} {'(ADVICE: In most cases you will speed up training considerably if you increase the value of --preprocessing_num_workers!)' if num_workers < 10 else ''}")
+ 
+    eval_dataset = vectorized_datasets["eval"]
+    train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+    
+    if not training_args.ignore_data_skip and training_state["step"] > 0:
+        logger.info(
+            f"  Will skip the first {training_state['step']} steps. If this takes a lot of time,"
+            " you can add the `--ignore_data_skip` flag to your launch command, but you will resume the"
+            " training on data already seen by your model."
+        )
+        for step in tqdm(range(training_state["step"]), desc=f"Skipping data for {training_state['step']} steps...", position=1, leave=False):
+            try:
+                samples = next(train_loader)
+            except StopIteration:
+                epoch += 1
+                train_dataset.set_epoch(epoch)
+                train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+                samples = next(train_loader)
+            batch = data_collator(samples)
+            # batch = shard(batch.data)
+
+    for step in tqdm(range(data_args.num_train_steps), desc="Training...", position=1, leave=False):
+        # Skip initial steps if these are specified. 
+        if step < training_state["step"]:
+            continue
+        
+        # =========================== Training ===========================
+        try:
+            samples = next(train_loader)
+        except StopIteration:
+            epoch += 1
+            train_dataset.set_epoch(epoch)
+            train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+            samples = next(train_loader)
+            logger.info(
+                f"Completed epoch ({epoch} | Loss: {train_metric['loss']}, Learning Rate:"
+                f" {train_metric['learning_rate']})"
+            )
+
+        batch = data_collator(samples)
+        batch = shard(batch.data)
+        
+        state, train_metric = p_train_step(state, batch)
+        
+        train_metrics.append(train_metric)
+        
+        train_time += time.time() - train_start
+        train_metric = unreplicate(train_metric)
+
+        # ========================== Evaluating ==========================
+        # Evaluate at each eval_steps, and at the end of training at num_train_steps
+        if step % training_args.eval_steps == 0 or step == data_args.num_train_steps - 1:
+            logger.info(
+                f"Starting evaluation at step {step} of num_training_step {data_args.num_train_steps} steps. Planned evaluation every {training_args.eval_steps} steps." 
+            )
+            eval_metrics = []
+            eval_preds = []
+            eval_labels = []
+            eval_loader = data_loader(eval_dataset, eval_batch_size, drop_last=False)
+            if data_args.max_eval_samples:
+                max_eval_steps_iter = range(1 + data_args.max_eval_samples // eval_batch_size)
+            else:
+                max_eval_steps_iter = itertools.repeat(None)
+            for _ in tqdm(max_eval_steps_iter, desc="Evaluating...", position=2, leave=False):
+                # Model forward
+                try:
+                    samples = next(eval_loader)
+                except StopIteration:
+                    break
+                batch = data_collator(samples)
+                
+                labels = batch["labels"]
+
+                metrics = pad_shard_unpad(p_eval_step, static_return=True)(
+                    state.params, batch.data, min_device_batch=training_args.per_device_eval_batch_size
+                )
+                eval_metrics.append(metrics)
+
+                # Generation
+                if training_args.predict_with_generate:
+                    generated_ids = pad_shard_unpad(
+                        p_generate_step)(state.params, batch.data)
+                    eval_preds.extend(jax.device_get(
+                        generated_ids.reshape(-1, gen_kwargs["max_length"])))
+                    eval_labels.extend(labels)
+
+            # Normalize eval metrics
+            eval_metrics = get_metrics(eval_metrics)
+            eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)
+
+            # Compute metrics
+            metric_desc = ""
+            if training_args.predict_with_generate:
+                metric_values, pred_str, label_str = compute_metrics(
+                    eval_preds, eval_labels, return_preds_labels=True
+                )
+                eval_metrics.update(metric_values)
+                metric_desc = " | ".join(
+                    [f"Eval {key}: {value}" for key, value in metric_values.items()])
+
+            # Print metrics
+            desc = f"Step: {step} | Epoch: {epoch} (Eval Loss: {eval_metrics['loss']} | {metric_desc})"
+            logger.info(desc)
+
+            # Update training state
+            training_state = update_training_state(
+                training_state,
+                train_metrics,
+                eval_metrics,
+                step,
+            )
+
+            # Save metrics
+            if has_tensorboard and current_host_idx == 0:
+                log_max_predictions = data_args.log_max_eval_predictions if data_args.log_max_eval_predictions else 0
+                write_metric(
+                    summary_writer,
+                    train_metrics,
+                    eval_metrics,
+                    train_time,
+                    step,
+                    predictions=pred_str[:log_max_predictions],
+                    labels=label_str[:log_max_predictions]
+                )
+
+            # Save checkpoint at each eval_steps and push checkpoint to the hub
+            if current_host_idx  == 0:
+                params = jax.device_get(
+                    jax.tree_util.tree_map(lambda x: x[0], state.params))
+                model.save_pretrained(training_args.output_dir, params=params)
+                tokenizer.save_pretrained(training_args.output_dir)
+                # Report eval results if training is done
+                if step == data_args.num_train_steps - 1:
+                    training_summary["eval_results"] = training_state["eval_lines"][-1]
+                else:
+                    training_summary.update({"eval_lines": training_state["eval_lines"]})
+                (output_dir / "training_state.bin").write_text(json.dumps(training_state))
+                # Write model card
+                readme.write_text(TrainingSummary(**training_summary).to_model_card())
+                if training_args.push_to_hub:
+                    repo.push_to_hub(
+                        commit_message=f"Saving weights and logs of step {step} - epoch {epoch}", blocking=False)
+
+if __name__ == "__main__":
+    main()
diff --git a/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/config.yaml b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/config.yaml
new file mode 100644
index 0000000..adc48fc
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/config.yaml
@@ -0,0 +1,512 @@
+wandb_version: 1
+
+__cached__setup_devices:
+  desc: null
+  value: cpu
+_n_gpu:
+  desc: null
+  value: 0
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.14.0
+    code_path: code/run_flax_speech_recognition_seq2seq_streaming.py
+    framework: huggingface
+    huggingface_version: 4.28.0.dev0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.10
+    start_time: 1681314705.934181
+    t:
+      1:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      2:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      3:
+      - 13
+      - 23
+      - 34
+      4: 3.8.10
+      5: 0.14.0
+      6: 4.28.0.dev0
+      8:
+      - 5
+adafactor:
+  desc: null
+  value: false
+adam_beta1:
+  desc: null
+  value: 0.9
+adam_beta2:
+  desc: null
+  value: 0.999
+adam_epsilon:
+  desc: null
+  value: 1.0e-08
+audio_column_name:
+  desc: null
+  value: audio
+auto_find_batch_size:
+  desc: null
+  value: false
+bf16:
+  desc: null
+  value: false
+bf16_full_eval:
+  desc: null
+  value: false
+cache_dir:
+  desc: null
+  value: null
+config_name:
+  desc: null
+  value: null
+data_seed:
+  desc: null
+  value: null
+dataloader_drop_last:
+  desc: null
+  value: false
+dataloader_num_workers:
+  desc: null
+  value: 0
+dataloader_pin_memory:
+  desc: null
+  value: true
+dataset_cache_dir:
+  desc: null
+  value: null
+dataset_config_name:
+  desc: null
+  value: null
+dataset_name:
+  desc: null
+  value: NbAiLab/NCC_speech_all_v5
+ddp_bucket_cap_mb:
+  desc: null
+  value: null
+ddp_find_unused_parameters:
+  desc: null
+  value: null
+ddp_timeout:
+  desc: null
+  value: 1800
+debug:
+  desc: null
+  value: []
+deepspeed:
+  desc: null
+  value: null
+disable_tqdm:
+  desc: null
+  value: false
+do_eval:
+  desc: null
+  value: true
+do_lower_case:
+  desc: null
+  value: false
+do_normalize_eval:
+  desc: null
+  value: true
+do_predict:
+  desc: null
+  value: false
+do_remove_punctuation:
+  desc: null
+  value: false
+do_train:
+  desc: null
+  value: true
+dtype:
+  desc: null
+  value: bfloat16
+eval_accumulation_steps:
+  desc: null
+  value: null
+eval_delay:
+  desc: null
+  value: 0
+eval_split_name:
+  desc: null
+  value: validation
+eval_steps:
+  desc: null
+  value: 10000
+evaluation_strategy:
+  desc: null
+  value: IntervalStrategy.NO
+feature_extractor_name:
+  desc: null
+  value: null
+fp16:
+  desc: null
+  value: false
+fp16_backend:
+  desc: null
+  value: auto
+fp16_full_eval:
+  desc: null
+  value: false
+fp16_opt_level:
+  desc: null
+  value: O1
+fsdp:
+  desc: null
+  value: []
+fsdp_config:
+  desc: null
+  value:
+    fsdp_min_num_params: 0
+    xla: false
+    xla_fsdp_grad_ckpt: false
+fsdp_min_num_params:
+  desc: null
+  value: 0
+fsdp_transformer_layer_cls_to_wrap:
+  desc: null
+  value: null
+full_determinism:
+  desc: null
+  value: false
+generation_config:
+  desc: null
+  value: null
+generation_max_length:
+  desc: null
+  value: null
+generation_num_beams:
+  desc: null
+  value: null
+gradient_accumulation_steps:
+  desc: null
+  value: 1
+gradient_checkpointing:
+  desc: null
+  value: false
+greater_is_better:
+  desc: null
+  value: null
+group_by_length:
+  desc: null
+  value: false
+half_precision_backend:
+  desc: null
+  value: auto
+hub_model_id:
+  desc: null
+  value: NbAiLab/scream_large_oct_debug
+hub_private_repo:
+  desc: null
+  value: true
+hub_strategy:
+  desc: null
+  value: HubStrategy.EVERY_SAVE
+hub_token:
+  desc: null
+  value: null
+ignore_data_skip:
+  desc: null
+  value: true
+include_inputs_for_metrics:
+  desc: null
+  value: false
+jit_mode_eval:
+  desc: null
+  value: false
+label_names:
+  desc: null
+  value: null
+label_smoothing_factor:
+  desc: null
+  value: 0.0
+language:
+  desc: null
+  value: Norwegian
+learning_rate:
+  desc: null
+  value: 5.0e-06
+length_column_name:
+  desc: null
+  value: length
+load_best_model_at_end:
+  desc: null
+  value: false
+local_rank:
+  desc: null
+  value: -1
+log_eval_predictions_fn:
+  desc: null
+  value: log_predictions.write_predictions
+log_level:
+  desc: null
+  value: passive
+log_level_replica:
+  desc: null
+  value: warning
+log_max_eval_predictions:
+  desc: null
+  value: 100
+log_on_each_node:
+  desc: null
+  value: true
+logging_dir:
+  desc: null
+  value: ../../scream_large_oct_debug/runs/Apr12_15-50-59_t1v-n-0a06f6ef-w-0
+logging_first_step:
+  desc: null
+  value: false
+logging_nan_inf_filter:
+  desc: null
+  value: true
+logging_steps:
+  desc: null
+  value: 500
+logging_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+lr_scheduler_type:
+  desc: null
+  value: SchedulerType.LINEAR
+max_duration_in_seconds:
+  desc: null
+  value: 30.0
+max_eval_samples:
+  desc: null
+  value: null
+max_grad_norm:
+  desc: null
+  value: 1.0
+max_label_length:
+  desc: null
+  value: 128
+max_steps:
+  desc: null
+  value: -1
+max_train_samples:
+  desc: null
+  value: null
+metric_for_best_model:
+  desc: null
+  value: null
+min_duration_in_seconds:
+  desc: null
+  value: 0.0
+model_name_or_path:
+  desc: null
+  value: openai/whisper-large-v2
+model_revision:
+  desc: null
+  value: main
+mp_parameters:
+  desc: null
+  value: ''
+no_cuda:
+  desc: null
+  value: false
+num_beams:
+  desc: null
+  value: 5
+num_train_epochs:
+  desc: null
+  value: 3.0
+num_train_steps:
+  desc: null
+  value: 50000
+optim:
+  desc: null
+  value: OptimizerNames.ADAMW_HF
+optim_args:
+  desc: null
+  value: null
+output_dir:
+  desc: null
+  value: ../../scream_large_oct_debug
+overwrite_cache:
+  desc: null
+  value: false
+overwrite_output_dir:
+  desc: null
+  value: true
+pad_input_to_multiple_of:
+  desc: null
+  value: null
+pad_target_to_multiple_of:
+  desc: null
+  value: null
+past_index:
+  desc: null
+  value: -1
+per_device_eval_batch_size:
+  desc: null
+  value: 4
+per_device_train_batch_size:
+  desc: null
+  value: 4
+per_gpu_eval_batch_size:
+  desc: null
+  value: null
+per_gpu_train_batch_size:
+  desc: null
+  value: null
+predict_with_generate:
+  desc: null
+  value: true
+prediction_loss_only:
+  desc: null
+  value: false
+preprocessing_num_workers:
+  desc: null
+  value: 32
+push_to_hub:
+  desc: null
+  value: true
+push_to_hub_model_id:
+  desc: null
+  value: null
+push_to_hub_organization:
+  desc: null
+  value: null
+push_to_hub_token:
+  desc: null
+  value: null
+ray_scope:
+  desc: null
+  value: last
+remove_unused_columns:
+  desc: null
+  value: true
+report_to:
+  desc: null
+  value:
+  - tensorboard
+  - wandb
+resume_from_checkpoint:
+  desc: null
+  value: 'True'
+run_description:
+  desc: null
+  value: A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear
+    decay on the all_v5-corpus.
+run_name:
+  desc: null
+  value: ScreamLarge - debug_beam5_long
+save_on_each_node:
+  desc: null
+  value: false
+save_steps:
+  desc: null
+  value: 500
+save_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+save_total_limit:
+  desc: null
+  value: null
+seed:
+  desc: null
+  value: 42
+sharded_ddp:
+  desc: null
+  value: []
+shuffle_buffer_size:
+  desc: null
+  value: 500
+skip_memory_metrics:
+  desc: null
+  value: true
+sortish_sampler:
+  desc: null
+  value: false
+streaming:
+  desc: null
+  value: true
+task:
+  desc: null
+  value: transcribe
+text_column:
+  desc: null
+  value: null
+text_column_name:
+  desc: null
+  value: text
+tf32:
+  desc: null
+  value: null
+tokenizer_name:
+  desc: null
+  value: null
+torch_compile:
+  desc: null
+  value: false
+torch_compile_backend:
+  desc: null
+  value: null
+torch_compile_mode:
+  desc: null
+  value: null
+torchdynamo:
+  desc: null
+  value: null
+tpu_metrics_debug:
+  desc: null
+  value: false
+tpu_num_cores:
+  desc: null
+  value: null
+train_split_name:
+  desc: null
+  value: train
+use_auth_token:
+  desc: null
+  value: true
+use_fast_tokenizer:
+  desc: null
+  value: true
+use_ipex:
+  desc: null
+  value: false
+use_legacy_prediction_loop:
+  desc: null
+  value: false
+use_mps_device:
+  desc: null
+  value: false
+wandb_entity:
+  desc: null
+  value: nbailab
+wandb_project:
+  desc: null
+  value: Scream - septimus
+warmup_ratio:
+  desc: null
+  value: 0.0
+warmup_steps:
+  desc: null
+  value: 10000
+weight_decay:
+  desc: null
+  value: 0.0
+xpu_backend:
+  desc: null
+  value: null
diff --git a/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/events.out.tfevents.1681314707.t1v-n-0a06f6ef-w-0.1567591.0.v2 b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/events.out.tfevents.1681314707.t1v-n-0a06f6ef-w-0.1567591.0.v2
new file mode 120000
index 0000000..4f1f5c8
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/events.out.tfevents.1681314707.t1v-n-0a06f6ef-w-0.1567591.0.v2
@@ -0,0 +1 @@
+/home/perk/models/scream_large_oct_debug/runs/Apr12_15-51-47_t1v-n-0a06f6ef-w-0/events.out.tfevents.1681314707.t1v-n-0a06f6ef-w-0.1567591.0.v2
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/requirements.txt b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/requirements.txt
new file mode 100644
index 0000000..34d1446
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/requirements.txt
@@ -0,0 +1,140 @@
+absl-py==1.4.0
+aiohttp==3.8.4
+aiosignal==1.3.1
+appdirs==1.4.4
+astunparse==1.6.3
+async-timeout==4.0.2
+attrs==22.2.0
+audioread==3.0.0
+cached-property==1.5.2
+cachetools==5.3.0
+certifi==2022.12.7
+cffi==1.15.1
+charset-normalizer==3.1.0
+chex==0.1.7
+click==8.1.3
+cmake==3.26.1
+datasets==2.11.0
+decorator==5.1.1
+dill==0.3.6
+dm-tree==0.1.8
+docker-pycreds==0.4.0
+etils==1.1.1
+evaluate==0.4.0
+filelock==3.10.7
+flatbuffers==23.3.3
+flax==0.6.8
+frozenlist==1.3.3
+fsspec==2023.3.0
+gast==0.4.0
+gitdb==4.0.10
+gitpython==3.1.31
+google-auth-oauthlib==1.0.0
+google-auth==2.17.1
+google-pasta==0.2.0
+grpcio==1.53.0
+h5py==3.8.0
+huggingface-hub==0.13.3
+idna==3.4
+importlib-metadata==6.1.0
+importlib-resources==5.12.0
+jax==0.4.8
+jaxlib==0.4.7
+jinja2==3.1.2
+jiwer==3.0.1
+joblib==1.2.0
+keras==2.12.0
+lazy-loader==0.2
+libclang==16.0.0
+librosa==0.10.0.post2
+libtpu-nightly==0.1.dev20230327
+lit==16.0.0
+llvmlite==0.39.1
+markdown-it-py==2.2.0
+markdown==3.4.3
+markupsafe==2.1.2
+mdurl==0.1.2
+ml-dtypes==0.0.4
+mpmath==1.3.0
+msgpack==1.0.5
+multidict==6.0.4
+multiprocess==0.70.14
+nest-asyncio==1.5.6
+networkx==3.0
+numba==0.56.4
+numpy==1.23.5
+nvidia-cublas-cu11==11.10.3.66
+nvidia-cuda-cupti-cu11==11.7.101
+nvidia-cuda-nvrtc-cu11==11.7.99
+nvidia-cuda-runtime-cu11==11.7.99
+nvidia-cudnn-cu11==8.5.0.96
+nvidia-cufft-cu11==10.9.0.58
+nvidia-curand-cu11==10.2.10.91
+nvidia-cusolver-cu11==11.4.0.1
+nvidia-cusparse-cu11==11.7.4.91
+nvidia-nccl-cu11==2.14.3
+nvidia-nvtx-cu11==11.7.91
+oauthlib==3.2.2
+opt-einsum==3.3.0
+optax==0.1.4
+orbax==0.1.7
+packaging==23.0
+pandas==1.5.3
+pathtools==0.1.2
+pip==23.0.1
+pkg-resources==0.0.0
+pooch==1.6.0
+protobuf==4.22.1
+psutil==5.9.4
+pyarrow==11.0.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pycparser==2.21
+pydub==0.25.1
+pygments==2.14.0
+python-dateutil==2.8.2
+pytz==2023.3
+pyyaml==6.0
+rapidfuzz==2.13.7
+regex==2023.3.23
+requests-oauthlib==1.3.1
+requests==2.28.2
+responses==0.18.0
+rich==13.3.3
+rsa==4.9
+scikit-learn==1.2.2
+scipy==1.10.1
+sentry-sdk==1.18.0
+setproctitle==1.3.2
+setuptools==44.0.0
+six==1.16.0
+smmap==5.0.0
+soundfile==0.12.1
+soxr==0.3.4
+sympy==1.11.1
+tabulate==0.9.0
+tensorboard-data-server==0.7.0
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.12.1
+tensorflow-estimator==2.12.0
+tensorflow-io-gcs-filesystem==0.32.0
+tensorflow==2.12.0
+tensorstore==0.1.35
+termcolor==2.2.0
+threadpoolctl==3.1.0
+tokenizers==0.13.2
+toolz==0.12.0
+torch==2.0.0
+torchaudio==2.0.1
+tqdm==4.65.0
+transformers==4.28.0.dev0
+triton==2.0.0
+typing-extensions==4.5.0
+urllib3==1.26.15
+wandb==0.14.0
+werkzeug==2.2.3
+wheel==0.40.0
+wrapt==1.14.1
+xxhash==3.2.0
+yarl==1.8.2
+zipp==3.15.0
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/wandb-metadata.json b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/wandb-metadata.json
new file mode 100644
index 0000000..eda79e3
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/wandb-metadata.json
@@ -0,0 +1,1300 @@
+{
+    "os": "Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2023-04-12T15:51:46.928881",
+    "startedAt": "2023-04-12T15:51:45.924940",
+    "docker": null,
+    "cuda": null,
+    "args": [
+        "--model_name_or_path",
+        "openai/whisper-large-v2",
+        "--run_name",
+        "ScreamLarge - debug_beam5_long",
+        "--run_description",
+        "A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear decay on the all_v5-corpus.",
+        "--wandb_entity",
+        "nbailab",
+        "--wandb_project",
+        "Scream - septimus",
+        "--dataset_name",
+        "NbAiLab/NCC_speech_all_v5",
+        "--language",
+        "Norwegian",
+        "--text_column_name",
+        "text",
+        "--train_split_name",
+        "train",
+        "--eval_split_name",
+        "validation",
+        "--output_dir",
+        "../../scream_large_oct_debug",
+        "--overwrite_output_dir",
+        "--warmup_steps",
+        "10000",
+        "--do_train",
+        "--do_eval",
+        "--num_train_steps",
+        "50000",
+        "--lr_scheduler_type",
+        "linear",
+        "--eval_steps",
+        "10000",
+        "--learning_rate",
+        "5e-6",
+        "--preprocessing_num_workers",
+        "32",
+        "--per_device_train_batch_size",
+        "4",
+        "--per_device_eval_batch_size",
+        "4",
+        "--predict_with_generate",
+        "--log_max_eval_predictions",
+        "100",
+        "--log_eval_predictions_fn",
+        "log_predictions.write_predictions",
+        "--streaming",
+        "True",
+        "--use_auth_token",
+        "True",
+        "--dtype",
+        "bfloat16",
+        "--hub_private_repo",
+        "True",
+        "--hub_model_id",
+        "NbAiLab/scream_large_oct_debug",
+        "--resume_from_checkpoint",
+        "True",
+        "--num_beams",
+        "5",
+        "--ignore_data_skip",
+        "--push_to_hub"
+    ],
+    "state": "running",
+    "program": "../run_flax_speech_recognition_seq2seq_streaming.py",
+    "codePath": "run_flax_speech_recognition_seq2seq_streaming.py",
+    "git": {
+        "remote": "https://github.com/NbAiLab/nb-whisper.git",
+        "commit": "c1f99c40043e96c943f8fcd9055bd585885f96f6"
+    },
+    "email": "per@capia.no",
+    "root": "/home/perk/models/nb-whisper",
+    "host": "t1v-n-0a06f6ef-w-0",
+    "username": "perk",
+    "executable": "/home/perk/.whisper/bin/python",
+    "cpu_count": 120,
+    "cpu_count_logical": 240,
+    "cpu_freq": {
+        "current": 2249.9980000000096,
+        "min": 0.0,
+        "max": 0.0
+    },
+    "cpu_freq_per_core": [
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        }
+    ],
+    "disk": {
+        "total": 96.74600601196289,
+        "used": 33.329795837402344
+    },
+    "memory": {
+        "total": 400.47254943847656
+    }
+}
diff --git a/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/wandb-summary.json b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/wandb-summary.json
new file mode 100644
index 0000000..49f30f3
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 708}}
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/run-gogpxnyj.wandb b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/run-gogpxnyj.wandb
new file mode 100644
index 0000000..7dd098d
Binary files /dev/null and b/experiment_scream_octavus/wandb/run-20230412_155145-gogpxnyj/run-gogpxnyj.wandb differ
diff --git a/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/code/run_flax_speech_recognition_seq2seq_streaming.py b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/code/run_flax_speech_recognition_seq2seq_streaming.py
new file mode 100644
index 0000000..fd32b95
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/code/run_flax_speech_recognition_seq2seq_streaming.py
@@ -0,0 +1,1309 @@
+#!/usr/bin/env python
+# coding=utf-8
+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR COND    ITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""
+Fine-tuning the Flax library models for sequence to sequence speech recognition.
+"""
+# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.
+
+import itertools
+import json
+import logging
+import os
+import shutil
+import socket
+import sys
+import tempfile
+import time
+from dataclasses import field
+from datetime import datetime
+from functools import partial
+from importlib import import_module
+from pathlib import Path
+from typing import Any, Callable, Dict, Generator, List, Optional, Union
+
+import flax
+import jax
+import jax.numpy as jnp 
+import numpy as np
+import optax
+import pandas as pd
+import torch
+# from jax.experimental.compilation_cache import compilation_cache; compilation_cache.initialize_cache(tempfile.gettempdir())
+from flax import jax_utils, traverse_util
+from flax.jax_utils import pad_shard_unpad, unreplicate
+from flax.training import train_state
+from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key
+from torch.utils.data import IterableDataset
+from tqdm import tqdm
+
+import datasets
+import evaluate
+import transformers
+from datasets import Dataset, DatasetDict, IterableDatasetDict, interleave_datasets, load_dataset
+from datasets.distributed import split_dataset_by_node
+from huggingface_hub import Repository, create_repo
+from transformers import (
+    AutoConfig,
+    AutoFeatureExtractor,
+    AutoProcessor,
+    AutoTokenizer,
+    FlaxAutoModelForSpeechSeq2Seq,
+    HfArgumentParser,
+    Seq2SeqTrainingArguments,
+    is_tensorboard_available,
+)
+from transformers.modelcard import TrainingSummary
+from transformers.models.whisper.english_normalizer import BasicTextNormalizer
+from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE
+from transformers.file_utils import get_full_repo_name
+from transformers.utils import check_min_version, send_example_telemetry
+from transformers.utils.versions import require_version
+
+from flax.training import checkpoints
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+check_min_version("4.27.0.dev0")
+
+require_version("datasets>=1.18.2",
+                "To fix: pip install -r examples/flax/speech-recogintion/requirements.txt")
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+logger = logging.getLogger(__name__)
+
+
+@flax.struct.dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={
+            "help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    feature_extractor_name: Optional[str] = field(
+        default=None, metadata={"help": "feature extractor name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Where to store the pretrained models downloaded from huggingface.co"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={
+            "help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+    dtype: Optional[str] = field(
+        default="float32",
+        metadata={
+            "help": (
+                "Floating-point format in which the model weights should be initialized and trained. Choose one of"
+                " `[float32, float16, bfloat16]`."
+            )
+        },
+    )
+    num_beams: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Number of beams to use for evaluation. This argument will be passed to `model.generate`, "
+                "which is used during evaluation."
+            )
+        },
+    )
+
+
+@flax.struct.dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    dataset_name: str = field(
+        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
+    )
+    dataset_config_name: Optional[str] = field(
+        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
+    )
+    text_column: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "The name of the column in the datasets containing the full texts (for summarization)."},
+    )
+    dataset_cache_dir: Optional[str] = field(
+        default=None, metadata={"help": "Path to cache directory for saving and loading datasets"}
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
+    )
+    preprocessing_num_workers: Optional[int] = field(
+        default=50,
+        metadata={"help": "The number of processes to use for the preprocessing."},
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    audio_column_name: str = field(
+        default="audio",
+        metadata={
+            "help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
+    )
+    text_column_name: str = field(
+        default="text",
+        metadata={
+            "help": "The name of the dataset column containing the text data. Defaults to 'text'"},
+    )
+    max_duration_in_seconds: float = field(
+        default=30.0,
+        metadata={
+            "help": "Filter audio files that are longer than `max_duration_in_seconds` seconds"},
+    )
+    min_duration_in_seconds: float = field(
+        default=0.0,
+        metadata={
+            "help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"},
+    )
+    max_label_length: float = field(
+        default=128,
+        metadata={
+            "help": "Truncate transcriptions that are longer `max_eval_length` tokens."},
+    )
+    pad_input_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the input sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the inputs to max length."
+        },
+    )
+    pad_target_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the target sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the targets to max length."
+        },
+    )
+    train_split_name: str = field(
+        default="train",
+        metadata={
+            "help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
+        },
+    )
+    eval_split_name: str = field(
+        default="validation",
+        metadata={
+            "help": "The name of the evaluation data set split to use (via the datasets library). Defaults to 'validation'"
+        },
+    )
+    do_lower_case: bool = field(
+        default=False,
+        metadata={"help": "Whether the target text should be lower cased."},
+    )
+    do_remove_punctuation: bool = field(
+        default=False,
+        metadata={
+            "help": "Whether the target text should be striped of punctuation."},
+    )
+    do_normalize_eval: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to normalise the references and predictions in the eval WER calculation."},
+    )
+    language: str = field(
+        default=None,
+        metadata={
+            "help": (
+                "Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning "
+                "only. For English speech recognition, it should be set to `None`."
+            )
+        },
+    )
+    task: str = field(
+        default="transcribe",
+        metadata={
+            "help": "Task, either `transcribe` for speech recognition or `translate` for speech translation."},
+    )
+    num_train_steps: int = field(default=50000, metadata={
+                                 "help": "The number of training steps."})
+    shuffle_buffer_size: Optional[int] = field(
+        default=500,
+        metadata={
+            "help": (
+                "The number of streamed examples to download before shuffling them. The large the buffer, "
+                "the closer it is to real offline shuffling."
+            )
+        },
+    )
+    streaming: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use streaming mode to load and pre-process the data."},
+    )
+    log_max_eval_predictions: Optional[int] = field(
+        default=0,
+        metadata={
+            "help": (
+                "Number of label and prediction pairs to write to the summary at each evaluation step."
+            )
+        },
+    )
+    log_eval_predictions_fn: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Python path to function for logging evaluation predictions. It can be an external function like fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions, labels)."
+            )
+        },
+    )
+    run_description: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "A longer description of the run/experiment."
+            )
+        },
+    )
+    wandb_entity: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases username or entity (organization name)."
+            )
+        },
+    )
+    wandb_project: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases project to log metrics to."
+            )
+        },
+    )
+
+
+def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:
+    """
+    Shift label ids one token to the right.
+    """
+    shifted_label_ids = np.zeros_like(label_ids)
+    shifted_label_ids[:, 1:] = label_ids[:, :-1]
+    shifted_label_ids[:, 0] = decoder_start_token_id
+
+    return shifted_label_ids
+
+
+@flax.struct.dataclass
+class FlaxDataCollatorSpeechSeq2SeqWithPadding:
+    """
+    Data collator that will dynamically pad the inputs received.
+    Args:
+        processor ([`Wav2Vec2Processor`])
+            The processor used for proccessing the data.
+        decoder_start_token_id (:obj: `int`)
+            The begin-of-sentence of the decoder.
+        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)
+            among:
+            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
+              sequence if provided).
+            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
+              maximum acceptable input length for the model if that argument is not provided.
+            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
+              different lengths).
+        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).
+            See above for details.
+        max_input_length (:obj:`float`, `optional`):
+            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
+        max_target_length (:obj:`int`, `optional`):
+            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).
+        pad_input_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the input sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+        pad_target_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the target sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+    """
+
+    processor: Any
+    decoder_start_token_id: int
+    input_padding: Union[bool, str] = "longest"
+    target_padding: Union[bool, str] = "max_length"
+    max_input_length: Optional[float] = None
+    max_target_length: Optional[int] = None
+    pad_input_to_multiple_of: Optional[int] = None
+    pad_target_to_multiple_of: Optional[int] = None
+
+    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:
+        model_input_name = self.processor.model_input_names[0]
+        input_features = {model_input_name: features[model_input_name]}
+        label_features = {"input_ids": features["labels"]}
+
+        # reformat list to dict and set to pytorch format
+        batch = self.processor.feature_extractor.pad(
+            input_features,
+            max_length=self.max_input_length,
+            padding=self.input_padding,
+            pad_to_multiple_of=self.pad_input_to_multiple_of,
+            return_tensors="np",
+        )
+
+        labels_batch = self.processor.tokenizer.pad(
+            label_features,
+            max_length=self.max_target_length,
+            padding=self.target_padding,
+            pad_to_multiple_of=self.pad_target_to_multiple_of,
+            return_tensors="np",
+        )
+
+        # if bos token is appended in previous tokenization step,
+        # cut bos token here as it's append later anyways
+        labels = labels_batch["input_ids"]
+        if (labels[:, 0] == self.decoder_start_token_id).all().item():
+            labels = labels[:, 1:]
+            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]
+
+        decoder_input_ids = shift_tokens_right(
+            labels, self.decoder_start_token_id)
+
+        # replace padding with -100 to ignore correctly when computing the loss
+        labels = np.ma.array(labels, mask=np.not_equal(
+            labels_batch.attention_mask, 1))
+        labels = labels.filled(fill_value=-100)
+
+        batch["labels"] = labels
+        batch["decoder_input_ids"] = decoder_input_ids
+        batch["attention_mask"] = labels_batch.attention_mask  # Add attention_mask to the batch
+
+        return batch
+
+
+def load_maybe_streaming_dataset(dataset_name, dataset_config_name, split="train", streaming=True, **kwargs):
+    """
+    Utility function to load a dataset in streaming mode. For datasets with multiple splits,
+    each split is loaded individually and then splits combined by taking alternating examples from
+    each (interleaving).
+    """
+    if "+" in split:
+        # load multiple splits separated by the `+` symbol with streaming mode
+        dataset_splits = [
+            load_dataset(dataset_name, dataset_config_name,
+                         split=split_name, streaming=streaming, **kwargs)
+            for split_name in split.split("+")
+        ]
+        # interleave multiple splits to form one dataset
+        interleaved_dataset = interleave_datasets(dataset_splits)
+        return interleaved_dataset
+    else:
+        # load a single split *with* streaming mode
+        dataset = load_dataset(
+            dataset_name, dataset_config_name, split=split, streaming=streaming, **kwargs)
+        return dataset
+
+
+def collate_batch(samples):
+    return {key: [feature[key] for feature in samples] for key in samples[0]}
+
+def data_loader(
+    dataset: Dataset,
+    batch_size: int,
+    drop_last: bool=True,
+    num_workers: int=0,
+) -> Generator:
+    """
+    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,
+    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.
+    """
+    data_loader_iterator = iter(torch.utils.data.DataLoader(
+        batch_size=batch_size,
+        dataset=dataset.with_format("torch"),
+        num_workers=num_workers,
+        collate_fn=collate_batch,
+        drop_last=drop_last,
+    ))
+    return data_loader_iterator
+
+
+class TrainState(train_state.TrainState):
+    dropout_rng: jnp.ndarray
+
+    def replicate(self):
+        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))
+
+
+def create_learning_rate_fn(
+    num_train_steps: int, num_warmup_steps: int, learning_rate: float, start_step: int=0, warmup_init_value: float=0.0, decay_end_value: float=0.0,
+) -> Callable[[int], jnp.array]:
+    """Returns a linear warmup, linear_decay learning rate function."""
+    warmup_fn = optax.linear_schedule(
+        init_value=warmup_init_value, end_value=learning_rate, transition_steps=num_warmup_steps)
+    decay_fn = optax.linear_schedule(
+        init_value=learning_rate, end_value=decay_end_value, transition_steps=num_train_steps - num_warmup_steps
+    )
+    schedule_fn = optax.join_schedules(
+        schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])
+    
+    def learning_rate_fn(step: int) -> jnp.array:
+        return schedule_fn(step + start_step)
+    
+    return learning_rate_fn
+
+
+def main():
+    # Parse input arguments
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+    parser = HfArgumentParser(
+        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
+
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args = parser.parse_json_file(
+            json_file=os.path.abspath(sys.argv[1]))
+    else:
+        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
+    # information sent is the one passed as arguments along with your JAX/Flax versions.
+    send_example_telemetry("run_speech_recognition_seq2seq",
+                           model_args, data_args, framework="flax")
+
+    # Setup logging
+    # Make one log on every process with the configuration for debugging.
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+    # Set the verbosity to info of the Transformers logger.
+    # We only want one process per machine to log things on the screen.
+
+    # logger.setLevel(logging.INFO if jax.local_devices()[0].id%jax.local_device_count() == 0 else logging.ERROR)
+
+    # logger.setLevel(logging.INFO if jax.process_index()
+    #                == 0 else logging.ERROR)
+    
+    # Number of hosts
+    num_of_hosts = jax.process_count()
+    current_host_idx = jax.process_index()
+
+    if current_host_idx == 0:
+        datasets.utils.logging.set_verbosity_warning()
+        transformers.utils.logging.set_verbosity_info()
+    else:
+        datasets.utils.logging.set_verbosity_error()
+        transformers.utils.logging.set_verbosity_error()
+    
+    logger.setLevel(logging.INFO)
+    logger.info("Training/evaluation parameters %s", training_args)
+
+    if num_of_hosts and not training_args.push_to_hub:
+        logger.warning(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --push_to_hub to be able to save checkpoints to the hub."
+        )
+    if num_of_hosts and not training_args.overwrite_output_dir and training_args.resume_from_checkpoint:
+        logger.error(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --overwrite_output_dir to be able to resume from a pushed checkpoint."
+        )
+        sys.exit(1)
+
+    # Check the output dir is valid
+    if os.path.exists(training_args.output_dir):
+        if (
+            os.listdir(training_args.output_dir)
+            and training_args.do_train
+            and not training_args.overwrite_output_dir
+        ):
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use `--overwrite_output_dir` to overcome."
+            )
+        elif training_args.overwrite_output_dir:
+            logger.warning(f"Removing path {training_args.output_dir}")
+            shutil.rmtree(training_args.output_dir)
+      
+    # Handle the repository creation
+    output_dir = Path(training_args.output_dir)
+    if training_args.push_to_hub:
+        if training_args.hub_model_id is None:
+            repo_name = get_full_repo_name(
+                output_dir.absolute().name,
+                token=training_args.hub_token,
+                organization=training_args.push_to_hub_organization,
+            )
+        else:
+            repo_name = training_args.hub_model_id
+         
+        repo_url = None  
+        while not repo_url:
+            # Workaround for an internal HuggingFace error if the repo is being created by another worker
+            try:
+                repo_url = create_repo(
+                    repo_name, exist_ok=True, token=training_args.hub_token, private=training_args.hub_private_repo
+                )
+            except:
+                time.sleep(1)
+
+        repo = Repository(training_args.output_dir,
+                          clone_from=repo_name, token=training_args.hub_token)
+
+    # Set the model_name_or_path
+    model_name_or_path = model_args.model_name_or_path
+
+    # Try to detect last checkpoint and continue if possible
+    training_state = {"step": 0, "eval_lines": []}
+    if training_args.resume_from_checkpoint:
+        if (output_dir / "flax_model.msgpack").exists() and (output_dir / "training_state.bin").exists():
+            training_state = json.loads((output_dir / "training_state.bin").read_text())
+            model_name_or_path = os.path.join(training_args.output_dir)
+            logger.info(
+                f"Checkpoint detected, resuming training from {training_args.output_dir} at step {training_state['step']}."
+            )
+        else:
+            logger.info(
+                f"No valid checkpoint found in {training_args.output_dir}. Starting from {model_name_or_path}."
+            )
+    
+    
+    # Load dataset
+    raw_datasets = IterableDatasetDict() if data_args.streaming else DatasetDict()
+
+    if training_args.do_train:
+        raw_datasets["train"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.train_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if training_args.do_eval:
+        raw_datasets["eval"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.eval_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if not training_args.do_train and not training_args.do_eval:
+        raise ValueError(
+            "Cannot not train and not do evaluation. At least one of training or evaluation has to be performed."
+        )
+
+    raw_datasets_features = list(
+        next(iter(raw_datasets.values())).features.keys())
+
+    if data_args.audio_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--audio_column_name` to the correct audio column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    if data_args.text_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--text_column_name` to the correct text column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    # Load pretrained model, tokenizer, and feature extractor
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    feature_extractor = AutoFeatureExtractor.from_pretrained(
+        model_args.feature_extractor_name if model_args.feature_extractor_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(
+        model_name_or_path,
+        config=config,
+        dtype=getattr(jnp, model_args.dtype),
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    logger.info(
+        f"Successfully loaded the model '{model_name_or_path}'."
+    )
+    
+    if model.config.decoder_start_token_id is None:
+        raise ValueError(
+            "Make sure that `config.decoder_start_token_id` is correctly defined")
+
+    # Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,
+    # so we just need to set the correct target sampling rate.
+    dataset_sampling_rate = next(
+        iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
+
+    if dataset_sampling_rate != feature_extractor.sampling_rate:
+        raw_datasets = raw_datasets.cast_column(
+            data_args.audio_column_name, datasets.features.Audio(
+                sampling_rate=feature_extractor.sampling_rate)
+        )
+
+    # Preprocessing the datasets.
+    # We need to read the audio files as arrays and tokenize the targets.
+    max_input_length = int(
+        data_args.max_duration_in_seconds * feature_extractor.sampling_rate)
+    min_input_length = int(
+        data_args.min_duration_in_seconds * feature_extractor.sampling_rate)
+    max_label_length = (
+        data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length
+    )
+    pad_input_to_multiple_of = data_args.pad_input_to_multiple_of
+    pad_target_to_multiple_of = data_args.pad_target_to_multiple_of
+    audio_column_name = data_args.audio_column_name
+    num_workers = data_args.preprocessing_num_workers
+    text_column_name = data_args.text_column_name
+    model_input_name = feature_extractor.model_input_names[0]
+    do_lower_case = data_args.do_lower_case
+    do_remove_punctuation = data_args.do_remove_punctuation
+    normalizer = BasicTextNormalizer()  # 'official' text normalizer from OpenAI
+
+    if data_args.language is not None:
+        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)
+        tokenizer.set_prefix_tokens(
+            language=data_args.language, task=data_args.task)
+
+    def prepare_dataset(batch):
+        # Process audio
+        sample = batch[audio_column_name]
+        inputs = feature_extractor(
+            sample["array"], sampling_rate=sample["sampling_rate"])
+        # Process audio length
+        batch[model_input_name] = inputs.get(model_input_name)[0]
+        batch["input_length"] = len(sample["array"])
+
+        # Process targets
+        input_str = batch[text_column_name].lower(
+        ) if do_lower_case else batch[text_column_name]
+        if do_remove_punctuation:
+            input_str = normalizer(input_str).strip()
+        batch["labels"] = tokenizer(input_str).input_ids
+        return batch
+
+    with training_args.main_process_first(desc="dataset map pre-processing"):
+        vectorized_datasets = raw_datasets.map(
+            prepare_dataset,
+            remove_columns=raw_datasets_features,
+        )
+
+    # Filter training data with inputs longer than max_input_length
+    def is_audio_in_length_range(length):
+        return min_input_length < length < max_input_length
+
+    if training_args.do_train:
+        vectorized_datasets["train"] = vectorized_datasets["train"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    if training_args.do_eval:
+        vectorized_datasets["eval"] = vectorized_datasets["eval"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    # Load metrics and write stats
+    metric_wer = evaluate.load("wer")
+    metric_cer = evaluate.load("cer")
+    do_normalize_eval = data_args.do_normalize_eval
+
+    def compute_metrics(pred_ids, label_ids, return_preds_labels=False):
+        # Replace padded labels by the padding token
+        for idx in range(len(label_ids)):
+            label_ids[idx][label_ids[idx] == -100] = tokenizer.pad_token_id
+
+        predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
+        # We do not want to group tokens when computing the metrics
+        labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
+
+        if do_normalize_eval:
+            pred_str = [normalizer(pred) for pred in predictions]
+            label_str = [normalizer(label) for label in labels]
+            # Filtering step to only evaluate the samples that correspond to non-zero references:
+            pred_str = [pred_str[i]
+                        for i in range(len(pred_str)) if len(label_str[i]) > 0]
+            label_str = [label_str[i]
+                         for i in range(len(label_str)) if len(label_str[i]) > 0]
+        else:
+            pred_str = predictions
+            label_str = labels
+
+        wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)
+        cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)
+
+        if return_preds_labels:
+            return {"wer": wer, "cer": cer}, predictions, labels
+        else:
+            return {"wer": wer, "cer": cer}
+
+    def update_training_state(training_state, train_metrics, eval_metrics, step):
+        safe_value = lambda x: float(x.tolist() if isinstance(x, jnp.ndarray) else x)
+        state = {"step": step}
+        eval_lines = training_state["eval_lines"]
+       
+        train_metrics = get_metrics(train_metrics)
+        train_metrics_dict = {}
+        for metric_name, values in train_metrics.items():
+            tag = f"train_{metric_name}"
+            for i, value in enumerate(values):
+                train_metrics_dict[step - len(values) + i + 1] = {tag: safe_value(value)}
+
+        eval_metrics_dict = {}
+        for metric_name, value in eval_metrics.items():
+            tag = f"eval_{metric_name}"
+            eval_metrics_dict.update({
+                "step": step,
+                tag: safe_value(value),
+            })
+            if step in train_metrics_dict:
+                eval_metrics_dict.update(train_metrics_dict[step])
+        eval_lines.append(eval_metrics_dict)
+        return {**state, "eval_lines": eval_lines}
+
+    def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=None, labels=None):
+        summary_writer.scalar("train_time", train_time, step)
+
+        train_metrics = get_metrics(train_metrics)
+        for key, vals in train_metrics.items():
+            tag = f"train_{key}"
+            for i, val in enumerate(vals):
+                summary_writer.scalar(tag, val, step - len(vals) + i + 1)
+
+        for metric_name, value in eval_metrics.items():
+            summary_writer.scalar(f"eval_{metric_name}", value, step)
+        
+        # Log evaluation predictions
+        if predictions and labels:
+            df = pd.DataFrame({
+                "references": labels,
+                "predictions": predictions,
+            })
+            df["wer"] = df.apply(lambda row: metric_wer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            df["cer"] = df.apply(lambda row: metric_cer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            markdown_table = df.to_markdown(index=False)
+            eval_metrics_table = pd.DataFrame.from_dict([{"step": step, **eval_metrics}]).to_markdown(index=False)
+            summary_writer.text("eval_predictions", eval_metrics_table + "\n\n" + markdown_table, step)
+            # External logging function
+            if data_args.log_eval_predictions_fn:
+                module, fname = data_args.log_eval_predictions_fn.rsplit('.', 1)
+                fn = getattr(import_module(module), fname)
+                fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=predictions, labels=labels, training_args=training_args)
+
+    # Save feature extractor, tokenizer and config
+    feature_extractor.save_pretrained(training_args.output_dir)
+    tokenizer.save_pretrained(training_args.output_dir)
+    config.save_pretrained(training_args.output_dir)
+
+    processor = AutoProcessor.from_pretrained(training_args.output_dir)
+
+    data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(
+        processor=processor,
+        decoder_start_token_id=model.config.decoder_start_token_id,
+        input_padding="longest",
+        target_padding="longest",
+        max_target_length=max_label_length,
+        pad_input_to_multiple_of=pad_input_to_multiple_of,
+        pad_target_to_multiple_of=pad_target_to_multiple_of if pad_target_to_multiple_of else max_label_length,
+    )
+
+    # Enable tensorboard only on the master node
+    has_tensorboard = is_tensorboard_available()
+    if has_tensorboard and current_host_idx == 0:
+        try:
+            # TODO: Decouple wandb from tensorboard
+            import wandb
+
+            has_wandb = True
+        except ImportError:
+            has_wandb = False
+            if data_args.wandb_entity is not None or data_args.wandb_project is not None:
+                logger.warning(
+                    f"Unable to display metrics through Weights & Biases because some packages are not installed: {ie}"
+                )
+        try:
+            if has_wandb:
+                wandb.tensorboard.patch(root_logdir=output_dir / "runs")
+                wandb.init(
+                    entity=data_args.wandb_entity,
+                    project=data_args.wandb_project,
+                    name=training_args.run_name,
+                    notes=data_args.run_description,
+                    save_code=True,
+                    sync_tensorboard=True,
+                )
+                wandb.config.update(training_args)
+                wandb.config.update(model_args)
+                wandb.config.update(data_args)
+            from flax.metrics.tensorboard import SummaryWriter
+
+            summary_writer = SummaryWriter(
+                log_dir=output_dir / "runs" / f"{datetime.now():%b%d_%H-%M-%S}_{socket.gethostname()}")
+        except ImportError as ie:
+            has_tensorboard = False
+            logger.warning(
+                f"Unable to display metrics through TensorBoard because some packages are not installed: {ie}"
+            )
+    else:
+        logger.warning(
+            "Unable to display metrics through TensorBoard because the package is not installed: "
+            "Please run pip install tensorboard to enable."
+        )
+
+    # Initialize our training
+    rng = jax.random.PRNGKey(training_args.seed)
+    rng, dropout_rng = jax.random.split(rng)
+
+    # Store some constant
+    train_batch_size = int(
+        training_args.per_device_train_batch_size) * jax.device_count()
+    eval_batch_size = int(
+        training_args.per_device_eval_batch_size) * jax.device_count()
+
+    # Create learning rate schedule
+    lr_scheduler_types = {"linear", "constant", "constant_with_warmup"}
+    if training_args.lr_scheduler_type not in lr_scheduler_types:
+        raise ValueError(
+            f"lr_scheduler_type of type {training_args.lr_scheduler_type} not supported, choose from {lr_scheduler_types}."
+        )
+    elif training_args.lr_scheduler_type == "constant":
+        warmup_init_value = training_args.learning_rate
+        decay_end_value = training_args.learning_rate
+    elif training_args.lr_scheduler_type == "constant_with_warmup":
+        warmup_init_value = 0.0
+        decay_end_value = training_args.learning_rate
+    else:
+        warmup_init_value = 0.0
+        decay_end_value = 0.0
+        
+    linear_decay_lr_schedule_fn = create_learning_rate_fn(
+        data_args.num_train_steps,
+        training_args.warmup_steps,
+        training_args.learning_rate,
+        start_step=training_state["step"],
+        warmup_init_value=warmup_init_value,
+        decay_end_value=decay_end_value
+    )
+    
+    # We use Optax's "masking" functionality to not apply weight decay
+    # to bias and LayerNorm scale parameters. decay_mask_fn returns a
+    # mask boolean with the same structure as the parameters.
+    # The mask is True for parameters that should be decayed.
+    def decay_mask_fn(params):
+        flat_params = traverse_util.flatten_dict(params)
+        # Find out all LayerNorm parameters
+        layer_norm_candidates = ["layernorm", "layer_norm", "ln"]
+        layer_norm_named_params = set(
+            [
+                layer[-2:]
+                for layer_norm_name in layer_norm_candidates
+                for layer in flat_params.keys()
+                if layer_norm_name in "".join(layer).lower()
+            ]
+        )
+        flat_mask = {path: (path[-1] != "bias" and path[-2:]
+                            not in layer_norm_named_params) for path in flat_params}
+        return traverse_util.unflatten_dict(flat_mask)
+    
+    # Create adam optimizer
+    adamw = optax.adamw(
+        learning_rate=linear_decay_lr_schedule_fn,
+        b1=training_args.adam_beta1,
+        b2=training_args.adam_beta2,
+        eps=training_args.adam_epsilon,
+        weight_decay=training_args.weight_decay,
+        mask=decay_mask_fn,
+    )
+
+    # Setup train state
+    state = TrainState.create(
+        apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)
+
+    # Label smoothed cross entropy
+    def loss_fn(logits, labels, label_smoothing_factor=0.0):
+        """
+        The label smoothing implementation is adapted from Flax's official example:
+        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104
+        """
+        vocab_size = logits.shape[-1]
+        confidence = 1.0 - label_smoothing_factor
+        low_confidence = (1.0 - confidence) / (vocab_size - 1)
+        normalizing_constant = -(
+            confidence * jnp.log(confidence) + (vocab_size - 1) *
+            low_confidence * jnp.log(low_confidence + 1e-20)
+        )
+        soft_labels = onehot(labels, vocab_size,
+                             on_value=confidence, off_value=low_confidence)
+
+        loss = optax.softmax_cross_entropy(logits, soft_labels)
+        loss = loss - normalizing_constant
+
+        # Ignore padded tokens from loss, i.e. where labels are not set to -100
+        padding_mask = labels >= 0
+        loss = loss * padding_mask
+        loss = loss.sum()
+        num_labels = padding_mask.sum()
+        return loss, num_labels
+
+    # Define gradient update step fn
+    def train_step(state, batch, label_smoothing_factor=0.0):
+        
+        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)
+
+        def compute_loss(params):
+            labels = batch.pop("labels")
+            logits = state.apply_fn(
+                **batch, params=params, dropout_rng=dropout_rng, train=True)[0]
+            loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+            return loss, num_labels
+
+        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)
+        (loss, num_labels), grad = grad_fn(state.params)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        # True grad = total grad / total samples
+        grad = jax.lax.psum(grad, "batch")
+        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)
+        new_state = state.apply_gradients(
+            grads=grad, dropout_rng=new_dropout_rng)
+
+        metrics = {"loss": loss,
+                   "learning_rate": linear_decay_lr_schedule_fn(state.step)}
+
+        return new_state, metrics
+
+    # Define eval fn
+    def eval_step(params, batch, label_smoothing_factor=0.0):
+        labels = batch.pop("labels")
+        logits = model(**batch, params=params, train=False)[0]
+
+        loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        metrics = {"loss": loss}
+        return metrics
+
+    # Define generation function
+    num_beams = model_args.num_beams if model_args.num_beams is not None else model.config.num_beams
+    gen_kwargs = {"max_length": max_label_length, "num_beams": num_beams}
+
+    def generate_step(params, batch):
+        model.params = params
+        output_ids = model.generate(batch[model_input_name], attention_mask=batch.get(
+            "attention_mask"), **gen_kwargs)
+        return output_ids.sequences
+
+    # Create parallel version of the train and eval step
+    p_train_step = jax.pmap(
+        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch", donate_argnums=(0, )
+    )
+    p_eval_step = jax.pmap(partial(
+        eval_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch")
+    p_generate_step = jax.pmap(generate_step, "batch")
+
+    # Replicate the train state on each device
+    state = state.replicate()
+    
+    # Logging
+    logger.info("***** Running training *****")
+    logger.info(
+        f"  Dataset name = {data_args.dataset_name}")
+    logger.info(
+        f"  Dataset config name = {data_args.dataset_config_name}")
+    logger.info(
+        f"  Learning rate = {training_args.learning_rate}")
+    logger.info(
+        f"  Scheduler = {training_args.lr_scheduler_type}")
+    logger.info(
+        f"  Num examples = {data_args.num_train_steps * train_batch_size}")
+    if num_of_hosts > 1:
+        logger.info(
+            f"  Number of hosts = {num_of_hosts}")
+        logger.info(
+            f"  Current host idx = {current_host_idx}")
+    logger.info(
+        f"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}")
+    logger.info(
+        f"  Total train batch size per node (w. parallel & distributed) = {train_batch_size // num_of_hosts}")
+    logger.info(
+        f"  Total train batch size (w. parallel & distributed) = {train_batch_size}")
+    logger.info(f"  Total optimization steps = {data_args.num_train_steps - training_state['step']}")
+    if training_state['step'] > 0:
+        logger.info(f"  ↪ Starting at {str(training_state['step'])} and finishing at {str(data_args.num_train_steps)}")
+
+    train_time = 0
+
+    # Training summary
+    language_code = None  # Maybe 'multilingual'?
+    if data_args.language is not None:
+        language = data_args.language.lower()
+        if language in TO_LANGUAGE_CODE:
+            language_code = TO_LANGUAGE_CODE[language]
+        elif len(language) == 2:
+            language_code = language
+    training_summary = {
+        "model_name": repo_name.split("/")[-1],
+        "language": language_code,
+        "tags": ["audio", "asr", "automatic-speech-recognition", "hf-asr-leaderboard"],
+        "license": "apache-2.0",
+        "finetuned_from": model_args.model_name_or_path,
+        "tasks": ["asr"],
+        "dataset": data_args.dataset_name,
+        "dataset_args": {"name": data_args.dataset_config_name},
+        "source": "flax",
+        "eval_lines": [],
+        "eval_results": None,
+        "hyperparameters": {
+            "learning_rate": training_args.learning_rate,
+            "lr_scheduler_type": training_args.lr_scheduler_type,
+            "per_device_train_batch_size": training_args.per_device_train_batch_size,
+            "total_train_batch_size_per_node": train_batch_size // num_of_hosts,
+            "total_train_batch_size": train_batch_size,
+            "total_optimization_steps": data_args.num_train_steps - training_state['step'],
+            "starting_optimization_step": training_state['step'] if training_state['step'] > 0 else None,
+            "finishing_optimization_step": data_args.num_train_steps,
+            "num_train_dataset_workers": f"{num_workers}",
+            "total_num_training_examples": data_args.num_train_steps * train_batch_size,
+        },
+        # TODO: Adapt https://github.com/huggingface/transformers/blob/main/src/transformers/modelcard.py#L855
+        # "hyperparameters": training_args.to_sanitized_dict()
+    }
+    
+    # Create README if it does not exist
+    readme = output_dir / "README.md"
+    if not readme.exists():
+        readme.write_text(TrainingSummary(**training_summary).to_model_card())
+    
+    # ======================== Training ================================
+    train_start = time.time()
+
+    train_metrics = []
+    epoch = 0
+    train_dataset = vectorized_datasets["train"].shuffle(seed=training_args.seed, buffer_size=data_args.shuffle_buffer_size)
+    
+    # Split by node
+    train_dataset = split_dataset_by_node(train_dataset, rank=current_host_idx, world_size=num_of_hosts)   
+    
+    if train_dataset.n_shards < data_args.preprocessing_num_workers:
+        num_workers = train_dataset.n_shards
+
+    logger.info(f"  Number of train dataset workers = {num_workers} {'(Capped by the number of dataset shards)' if train_dataset.n_shards < data_args.preprocessing_num_workers else ''} {'(ADVICE: In most cases you will speed up training considerably if you increase the value of --preprocessing_num_workers!)' if num_workers < 10 else ''}")
+ 
+    eval_dataset = vectorized_datasets["eval"]
+    train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+    
+    if not training_args.ignore_data_skip and training_state["step"] > 0:
+        logger.info(
+            f"  Will skip the first {training_state['step']} steps. If this takes a lot of time,"
+            " you can add the `--ignore_data_skip` flag to your launch command, but you will resume the"
+            " training on data already seen by your model."
+        )
+        for step in tqdm(range(training_state["step"]), desc=f"Skipping data for {training_state['step']} steps...", position=1, leave=False):
+            try:
+                samples = next(train_loader)
+            except StopIteration:
+                epoch += 1
+                train_dataset.set_epoch(epoch)
+                train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+                samples = next(train_loader)
+            batch = data_collator(samples)
+            # batch = shard(batch.data)
+
+    for step in tqdm(range(data_args.num_train_steps), desc="Training...", position=1, leave=False):
+        # Skip initial steps if these are specified. 
+        if step < training_state["step"]:
+            continue
+        
+        # =========================== Training ===========================
+        try:
+            samples = next(train_loader)
+        except StopIteration:
+            epoch += 1
+            train_dataset.set_epoch(epoch)
+            train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+            samples = next(train_loader)
+            logger.info(
+                f"Completed epoch ({epoch} | Loss: {train_metric['loss']}, Learning Rate:"
+                f" {train_metric['learning_rate']})"
+            )
+
+        batch = data_collator(samples)
+        batch = shard(batch.data)
+        
+        state, train_metric = p_train_step(state, batch)
+        
+        train_metrics.append(train_metric)
+        
+        train_time += time.time() - train_start
+        train_metric = unreplicate(train_metric)
+
+        # ========================== Evaluating ==========================
+        # Evaluate at each eval_steps, and at the end of training at num_train_steps
+        if step % training_args.eval_steps == 0 or step == data_args.num_train_steps - 1:
+            logger.info(
+                f"Starting evaluation at step {step} of num_training_step {data_args.num_train_steps} steps. Planned evaluation every {training_args.eval_steps} steps." 
+            )
+            eval_metrics = []
+            eval_preds = []
+            eval_labels = []
+            eval_loader = data_loader(eval_dataset, eval_batch_size, drop_last=False)
+            if data_args.max_eval_samples:
+                max_eval_steps_iter = range(1 + data_args.max_eval_samples // eval_batch_size)
+            else:
+                max_eval_steps_iter = itertools.repeat(None)
+            for _ in tqdm(max_eval_steps_iter, desc="Evaluating...", position=2, leave=False):
+                # Model forward
+                try:
+                    samples = next(eval_loader)
+                except StopIteration:
+                    break
+                batch = data_collator(samples)
+                
+                labels = batch["labels"]
+
+                metrics = pad_shard_unpad(p_eval_step, static_return=True)(
+                    state.params, batch.data, min_device_batch=training_args.per_device_eval_batch_size
+                )
+                eval_metrics.append(metrics)
+
+                # Generation
+                if training_args.predict_with_generate:
+                    generated_ids = pad_shard_unpad(
+                        p_generate_step)(state.params, batch.data)
+                    eval_preds.extend(jax.device_get(
+                        generated_ids.reshape(-1, gen_kwargs["max_length"])))
+                    eval_labels.extend(labels)
+
+            # Normalize eval metrics
+            eval_metrics = get_metrics(eval_metrics)
+            eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)
+
+            # Compute metrics
+            metric_desc = ""
+            if training_args.predict_with_generate:
+                metric_values, pred_str, label_str = compute_metrics(
+                    eval_preds, eval_labels, return_preds_labels=True
+                )
+                eval_metrics.update(metric_values)
+                metric_desc = " | ".join(
+                    [f"Eval {key}: {value}" for key, value in metric_values.items()])
+
+            # Print metrics
+            desc = f"Step: {step} | Epoch: {epoch} (Eval Loss: {eval_metrics['loss']} | {metric_desc})"
+            logger.info(desc)
+
+            # Update training state
+            training_state = update_training_state(
+                training_state,
+                train_metrics,
+                eval_metrics,
+                step,
+            )
+
+            # Save metrics
+            if has_tensorboard and current_host_idx == 0:
+                log_max_predictions = data_args.log_max_eval_predictions if data_args.log_max_eval_predictions else 0
+                write_metric(
+                    summary_writer,
+                    train_metrics,
+                    eval_metrics,
+                    train_time,
+                    step,
+                    predictions=pred_str[:log_max_predictions],
+                    labels=label_str[:log_max_predictions]
+                )
+
+            # Save checkpoint at each eval_steps and push checkpoint to the hub
+            if current_host_idx  == 0:
+                params = jax.device_get(
+                    jax.tree_util.tree_map(lambda x: x[0], state.params))
+                model.save_pretrained(training_args.output_dir, params=params)
+                tokenizer.save_pretrained(training_args.output_dir)
+                # Report eval results if training is done
+                if step == data_args.num_train_steps - 1:
+                    training_summary["eval_results"] = training_state["eval_lines"][-1]
+                else:
+                    training_summary.update({"eval_lines": training_state["eval_lines"]})
+                (output_dir / "training_state.bin").write_text(json.dumps(training_state))
+                # Write model card
+                readme.write_text(TrainingSummary(**training_summary).to_model_card())
+                if training_args.push_to_hub:
+                    repo.push_to_hub(
+                        commit_message=f"Saving weights and logs of step {step} - epoch {epoch}", blocking=False)
+
+if __name__ == "__main__":
+    main()
diff --git a/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/config.yaml b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/config.yaml
new file mode 100644
index 0000000..03ad059
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/config.yaml
@@ -0,0 +1,512 @@
+wandb_version: 1
+
+__cached__setup_devices:
+  desc: null
+  value: cpu
+_n_gpu:
+  desc: null
+  value: 0
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.14.0
+    code_path: code/run_flax_speech_recognition_seq2seq_streaming.py
+    framework: huggingface
+    huggingface_version: 4.28.0.dev0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.10
+    start_time: 1681316198.495191
+    t:
+      1:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      2:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      3:
+      - 13
+      - 23
+      - 34
+      4: 3.8.10
+      5: 0.14.0
+      6: 4.28.0.dev0
+      8:
+      - 5
+adafactor:
+  desc: null
+  value: false
+adam_beta1:
+  desc: null
+  value: 0.9
+adam_beta2:
+  desc: null
+  value: 0.999
+adam_epsilon:
+  desc: null
+  value: 1.0e-08
+audio_column_name:
+  desc: null
+  value: audio
+auto_find_batch_size:
+  desc: null
+  value: false
+bf16:
+  desc: null
+  value: false
+bf16_full_eval:
+  desc: null
+  value: false
+cache_dir:
+  desc: null
+  value: null
+config_name:
+  desc: null
+  value: null
+data_seed:
+  desc: null
+  value: null
+dataloader_drop_last:
+  desc: null
+  value: false
+dataloader_num_workers:
+  desc: null
+  value: 0
+dataloader_pin_memory:
+  desc: null
+  value: true
+dataset_cache_dir:
+  desc: null
+  value: null
+dataset_config_name:
+  desc: null
+  value: null
+dataset_name:
+  desc: null
+  value: NbAiLab/NCC_speech_all_v5
+ddp_bucket_cap_mb:
+  desc: null
+  value: null
+ddp_find_unused_parameters:
+  desc: null
+  value: null
+ddp_timeout:
+  desc: null
+  value: 1800
+debug:
+  desc: null
+  value: []
+deepspeed:
+  desc: null
+  value: null
+disable_tqdm:
+  desc: null
+  value: false
+do_eval:
+  desc: null
+  value: true
+do_lower_case:
+  desc: null
+  value: false
+do_normalize_eval:
+  desc: null
+  value: true
+do_predict:
+  desc: null
+  value: false
+do_remove_punctuation:
+  desc: null
+  value: false
+do_train:
+  desc: null
+  value: true
+dtype:
+  desc: null
+  value: bfloat16
+eval_accumulation_steps:
+  desc: null
+  value: null
+eval_delay:
+  desc: null
+  value: 0
+eval_split_name:
+  desc: null
+  value: validation
+eval_steps:
+  desc: null
+  value: 10000
+evaluation_strategy:
+  desc: null
+  value: IntervalStrategy.NO
+feature_extractor_name:
+  desc: null
+  value: null
+fp16:
+  desc: null
+  value: false
+fp16_backend:
+  desc: null
+  value: auto
+fp16_full_eval:
+  desc: null
+  value: false
+fp16_opt_level:
+  desc: null
+  value: O1
+fsdp:
+  desc: null
+  value: []
+fsdp_config:
+  desc: null
+  value:
+    fsdp_min_num_params: 0
+    xla: false
+    xla_fsdp_grad_ckpt: false
+fsdp_min_num_params:
+  desc: null
+  value: 0
+fsdp_transformer_layer_cls_to_wrap:
+  desc: null
+  value: null
+full_determinism:
+  desc: null
+  value: false
+generation_config:
+  desc: null
+  value: null
+generation_max_length:
+  desc: null
+  value: null
+generation_num_beams:
+  desc: null
+  value: null
+gradient_accumulation_steps:
+  desc: null
+  value: 1
+gradient_checkpointing:
+  desc: null
+  value: false
+greater_is_better:
+  desc: null
+  value: null
+group_by_length:
+  desc: null
+  value: false
+half_precision_backend:
+  desc: null
+  value: auto
+hub_model_id:
+  desc: null
+  value: NbAiLab/scream_large_oct_debug
+hub_private_repo:
+  desc: null
+  value: true
+hub_strategy:
+  desc: null
+  value: HubStrategy.EVERY_SAVE
+hub_token:
+  desc: null
+  value: null
+ignore_data_skip:
+  desc: null
+  value: true
+include_inputs_for_metrics:
+  desc: null
+  value: false
+jit_mode_eval:
+  desc: null
+  value: false
+label_names:
+  desc: null
+  value: null
+label_smoothing_factor:
+  desc: null
+  value: 0.0
+language:
+  desc: null
+  value: Norwegian
+learning_rate:
+  desc: null
+  value: 5.0e-06
+length_column_name:
+  desc: null
+  value: length
+load_best_model_at_end:
+  desc: null
+  value: false
+local_rank:
+  desc: null
+  value: -1
+log_eval_predictions_fn:
+  desc: null
+  value: log_predictions.write_predictions
+log_level:
+  desc: null
+  value: passive
+log_level_replica:
+  desc: null
+  value: warning
+log_max_eval_predictions:
+  desc: null
+  value: 100
+log_on_each_node:
+  desc: null
+  value: true
+logging_dir:
+  desc: null
+  value: ../../scream_large_oct_debug/runs/Apr12_16-15-50_t1v-n-0a06f6ef-w-0
+logging_first_step:
+  desc: null
+  value: false
+logging_nan_inf_filter:
+  desc: null
+  value: true
+logging_steps:
+  desc: null
+  value: 500
+logging_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+lr_scheduler_type:
+  desc: null
+  value: SchedulerType.LINEAR
+max_duration_in_seconds:
+  desc: null
+  value: 30.0
+max_eval_samples:
+  desc: null
+  value: null
+max_grad_norm:
+  desc: null
+  value: 1.0
+max_label_length:
+  desc: null
+  value: 128
+max_steps:
+  desc: null
+  value: -1
+max_train_samples:
+  desc: null
+  value: null
+metric_for_best_model:
+  desc: null
+  value: null
+min_duration_in_seconds:
+  desc: null
+  value: 0.0
+model_name_or_path:
+  desc: null
+  value: openai/whisper-large-v2
+model_revision:
+  desc: null
+  value: main
+mp_parameters:
+  desc: null
+  value: ''
+no_cuda:
+  desc: null
+  value: false
+num_beams:
+  desc: null
+  value: 5
+num_train_epochs:
+  desc: null
+  value: 3.0
+num_train_steps:
+  desc: null
+  value: 50000
+optim:
+  desc: null
+  value: OptimizerNames.ADAMW_HF
+optim_args:
+  desc: null
+  value: null
+output_dir:
+  desc: null
+  value: ../../scream_large_oct_debug
+overwrite_cache:
+  desc: null
+  value: false
+overwrite_output_dir:
+  desc: null
+  value: true
+pad_input_to_multiple_of:
+  desc: null
+  value: null
+pad_target_to_multiple_of:
+  desc: null
+  value: null
+past_index:
+  desc: null
+  value: -1
+per_device_eval_batch_size:
+  desc: null
+  value: 3
+per_device_train_batch_size:
+  desc: null
+  value: 3
+per_gpu_eval_batch_size:
+  desc: null
+  value: null
+per_gpu_train_batch_size:
+  desc: null
+  value: null
+predict_with_generate:
+  desc: null
+  value: true
+prediction_loss_only:
+  desc: null
+  value: false
+preprocessing_num_workers:
+  desc: null
+  value: 32
+push_to_hub:
+  desc: null
+  value: true
+push_to_hub_model_id:
+  desc: null
+  value: null
+push_to_hub_organization:
+  desc: null
+  value: null
+push_to_hub_token:
+  desc: null
+  value: null
+ray_scope:
+  desc: null
+  value: last
+remove_unused_columns:
+  desc: null
+  value: true
+report_to:
+  desc: null
+  value:
+  - tensorboard
+  - wandb
+resume_from_checkpoint:
+  desc: null
+  value: 'True'
+run_description:
+  desc: null
+  value: A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear
+    decay on the all_v5-corpus.
+run_name:
+  desc: null
+  value: ScreamLarge - debug_beam5_long
+save_on_each_node:
+  desc: null
+  value: false
+save_steps:
+  desc: null
+  value: 500
+save_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+save_total_limit:
+  desc: null
+  value: null
+seed:
+  desc: null
+  value: 42
+sharded_ddp:
+  desc: null
+  value: []
+shuffle_buffer_size:
+  desc: null
+  value: 500
+skip_memory_metrics:
+  desc: null
+  value: true
+sortish_sampler:
+  desc: null
+  value: false
+streaming:
+  desc: null
+  value: true
+task:
+  desc: null
+  value: transcribe
+text_column:
+  desc: null
+  value: null
+text_column_name:
+  desc: null
+  value: text
+tf32:
+  desc: null
+  value: null
+tokenizer_name:
+  desc: null
+  value: null
+torch_compile:
+  desc: null
+  value: false
+torch_compile_backend:
+  desc: null
+  value: null
+torch_compile_mode:
+  desc: null
+  value: null
+torchdynamo:
+  desc: null
+  value: null
+tpu_metrics_debug:
+  desc: null
+  value: false
+tpu_num_cores:
+  desc: null
+  value: null
+train_split_name:
+  desc: null
+  value: train
+use_auth_token:
+  desc: null
+  value: true
+use_fast_tokenizer:
+  desc: null
+  value: true
+use_ipex:
+  desc: null
+  value: false
+use_legacy_prediction_loop:
+  desc: null
+  value: false
+use_mps_device:
+  desc: null
+  value: false
+wandb_entity:
+  desc: null
+  value: nbailab
+wandb_project:
+  desc: null
+  value: Scream - septimus
+warmup_ratio:
+  desc: null
+  value: 0.0
+warmup_steps:
+  desc: null
+  value: 10000
+weight_decay:
+  desc: null
+  value: 0.0
+xpu_backend:
+  desc: null
+  value: null
diff --git a/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/events.out.tfevents.1681316199.t1v-n-0a06f6ef-w-0.1576101.0.v2 b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/events.out.tfevents.1681316199.t1v-n-0a06f6ef-w-0.1576101.0.v2
new file mode 120000
index 0000000..da984aa
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/events.out.tfevents.1681316199.t1v-n-0a06f6ef-w-0.1576101.0.v2
@@ -0,0 +1 @@
+/home/perk/models/scream_large_oct_debug/runs/Apr12_16-16-39_t1v-n-0a06f6ef-w-0/events.out.tfevents.1681316199.t1v-n-0a06f6ef-w-0.1576101.0.v2
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/requirements.txt b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/requirements.txt
new file mode 100644
index 0000000..34d1446
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/requirements.txt
@@ -0,0 +1,140 @@
+absl-py==1.4.0
+aiohttp==3.8.4
+aiosignal==1.3.1
+appdirs==1.4.4
+astunparse==1.6.3
+async-timeout==4.0.2
+attrs==22.2.0
+audioread==3.0.0
+cached-property==1.5.2
+cachetools==5.3.0
+certifi==2022.12.7
+cffi==1.15.1
+charset-normalizer==3.1.0
+chex==0.1.7
+click==8.1.3
+cmake==3.26.1
+datasets==2.11.0
+decorator==5.1.1
+dill==0.3.6
+dm-tree==0.1.8
+docker-pycreds==0.4.0
+etils==1.1.1
+evaluate==0.4.0
+filelock==3.10.7
+flatbuffers==23.3.3
+flax==0.6.8
+frozenlist==1.3.3
+fsspec==2023.3.0
+gast==0.4.0
+gitdb==4.0.10
+gitpython==3.1.31
+google-auth-oauthlib==1.0.0
+google-auth==2.17.1
+google-pasta==0.2.0
+grpcio==1.53.0
+h5py==3.8.0
+huggingface-hub==0.13.3
+idna==3.4
+importlib-metadata==6.1.0
+importlib-resources==5.12.0
+jax==0.4.8
+jaxlib==0.4.7
+jinja2==3.1.2
+jiwer==3.0.1
+joblib==1.2.0
+keras==2.12.0
+lazy-loader==0.2
+libclang==16.0.0
+librosa==0.10.0.post2
+libtpu-nightly==0.1.dev20230327
+lit==16.0.0
+llvmlite==0.39.1
+markdown-it-py==2.2.0
+markdown==3.4.3
+markupsafe==2.1.2
+mdurl==0.1.2
+ml-dtypes==0.0.4
+mpmath==1.3.0
+msgpack==1.0.5
+multidict==6.0.4
+multiprocess==0.70.14
+nest-asyncio==1.5.6
+networkx==3.0
+numba==0.56.4
+numpy==1.23.5
+nvidia-cublas-cu11==11.10.3.66
+nvidia-cuda-cupti-cu11==11.7.101
+nvidia-cuda-nvrtc-cu11==11.7.99
+nvidia-cuda-runtime-cu11==11.7.99
+nvidia-cudnn-cu11==8.5.0.96
+nvidia-cufft-cu11==10.9.0.58
+nvidia-curand-cu11==10.2.10.91
+nvidia-cusolver-cu11==11.4.0.1
+nvidia-cusparse-cu11==11.7.4.91
+nvidia-nccl-cu11==2.14.3
+nvidia-nvtx-cu11==11.7.91
+oauthlib==3.2.2
+opt-einsum==3.3.0
+optax==0.1.4
+orbax==0.1.7
+packaging==23.0
+pandas==1.5.3
+pathtools==0.1.2
+pip==23.0.1
+pkg-resources==0.0.0
+pooch==1.6.0
+protobuf==4.22.1
+psutil==5.9.4
+pyarrow==11.0.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pycparser==2.21
+pydub==0.25.1
+pygments==2.14.0
+python-dateutil==2.8.2
+pytz==2023.3
+pyyaml==6.0
+rapidfuzz==2.13.7
+regex==2023.3.23
+requests-oauthlib==1.3.1
+requests==2.28.2
+responses==0.18.0
+rich==13.3.3
+rsa==4.9
+scikit-learn==1.2.2
+scipy==1.10.1
+sentry-sdk==1.18.0
+setproctitle==1.3.2
+setuptools==44.0.0
+six==1.16.0
+smmap==5.0.0
+soundfile==0.12.1
+soxr==0.3.4
+sympy==1.11.1
+tabulate==0.9.0
+tensorboard-data-server==0.7.0
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.12.1
+tensorflow-estimator==2.12.0
+tensorflow-io-gcs-filesystem==0.32.0
+tensorflow==2.12.0
+tensorstore==0.1.35
+termcolor==2.2.0
+threadpoolctl==3.1.0
+tokenizers==0.13.2
+toolz==0.12.0
+torch==2.0.0
+torchaudio==2.0.1
+tqdm==4.65.0
+transformers==4.28.0.dev0
+triton==2.0.0
+typing-extensions==4.5.0
+urllib3==1.26.15
+wandb==0.14.0
+werkzeug==2.2.3
+wheel==0.40.0
+wrapt==1.14.1
+xxhash==3.2.0
+yarl==1.8.2
+zipp==3.15.0
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/wandb-metadata.json b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/wandb-metadata.json
new file mode 100644
index 0000000..dedd364
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/wandb-metadata.json
@@ -0,0 +1,1300 @@
+{
+    "os": "Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2023-04-12T16:16:39.484679",
+    "startedAt": "2023-04-12T16:16:38.485893",
+    "docker": null,
+    "cuda": null,
+    "args": [
+        "--model_name_or_path",
+        "openai/whisper-large-v2",
+        "--run_name",
+        "ScreamLarge - debug_beam5_long",
+        "--run_description",
+        "A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear decay on the all_v5-corpus.",
+        "--wandb_entity",
+        "nbailab",
+        "--wandb_project",
+        "Scream - septimus",
+        "--dataset_name",
+        "NbAiLab/NCC_speech_all_v5",
+        "--language",
+        "Norwegian",
+        "--text_column_name",
+        "text",
+        "--train_split_name",
+        "train",
+        "--eval_split_name",
+        "validation",
+        "--output_dir",
+        "../../scream_large_oct_debug",
+        "--overwrite_output_dir",
+        "--warmup_steps",
+        "10000",
+        "--do_train",
+        "--do_eval",
+        "--num_train_steps",
+        "50000",
+        "--lr_scheduler_type",
+        "linear",
+        "--eval_steps",
+        "10000",
+        "--learning_rate",
+        "5e-6",
+        "--preprocessing_num_workers",
+        "32",
+        "--per_device_train_batch_size",
+        "3",
+        "--per_device_eval_batch_size",
+        "3",
+        "--predict_with_generate",
+        "--log_max_eval_predictions",
+        "100",
+        "--log_eval_predictions_fn",
+        "log_predictions.write_predictions",
+        "--streaming",
+        "True",
+        "--use_auth_token",
+        "True",
+        "--dtype",
+        "bfloat16",
+        "--hub_private_repo",
+        "True",
+        "--hub_model_id",
+        "NbAiLab/scream_large_oct_debug",
+        "--resume_from_checkpoint",
+        "True",
+        "--num_beams",
+        "5",
+        "--ignore_data_skip",
+        "--push_to_hub"
+    ],
+    "state": "running",
+    "program": "../run_flax_speech_recognition_seq2seq_streaming.py",
+    "codePath": "run_flax_speech_recognition_seq2seq_streaming.py",
+    "git": {
+        "remote": "https://github.com/NbAiLab/nb-whisper.git",
+        "commit": "d24e9d7ae43239a33775e3eb92be3ca3efc38e4a"
+    },
+    "email": "per@capia.no",
+    "root": "/home/perk/models/nb-whisper",
+    "host": "t1v-n-0a06f6ef-w-0",
+    "username": "perk",
+    "executable": "/home/perk/.whisper/bin/python",
+    "cpu_count": 120,
+    "cpu_count_logical": 240,
+    "cpu_freq": {
+        "current": 2249.9980000000096,
+        "min": 0.0,
+        "max": 0.0
+    },
+    "cpu_freq_per_core": [
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        }
+    ],
+    "disk": {
+        "total": 96.74600601196289,
+        "used": 33.35831832885742
+    },
+    "memory": {
+        "total": 400.47254943847656
+    }
+}
diff --git a/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/wandb-summary.json b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/wandb-summary.json
new file mode 100644
index 0000000..ea969ce
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 897}}
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/run-c0mpekug.wandb b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/run-c0mpekug.wandb
new file mode 100644
index 0000000..9f15d58
Binary files /dev/null and b/experiment_scream_octavus/wandb/run-20230412_161638-c0mpekug/run-c0mpekug.wandb differ
diff --git a/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/code/run_flax_speech_recognition_seq2seq_streaming.py b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/code/run_flax_speech_recognition_seq2seq_streaming.py
new file mode 100644
index 0000000..fd32b95
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/code/run_flax_speech_recognition_seq2seq_streaming.py
@@ -0,0 +1,1309 @@
+#!/usr/bin/env python
+# coding=utf-8
+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR COND    ITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""
+Fine-tuning the Flax library models for sequence to sequence speech recognition.
+"""
+# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.
+
+import itertools
+import json
+import logging
+import os
+import shutil
+import socket
+import sys
+import tempfile
+import time
+from dataclasses import field
+from datetime import datetime
+from functools import partial
+from importlib import import_module
+from pathlib import Path
+from typing import Any, Callable, Dict, Generator, List, Optional, Union
+
+import flax
+import jax
+import jax.numpy as jnp 
+import numpy as np
+import optax
+import pandas as pd
+import torch
+# from jax.experimental.compilation_cache import compilation_cache; compilation_cache.initialize_cache(tempfile.gettempdir())
+from flax import jax_utils, traverse_util
+from flax.jax_utils import pad_shard_unpad, unreplicate
+from flax.training import train_state
+from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key
+from torch.utils.data import IterableDataset
+from tqdm import tqdm
+
+import datasets
+import evaluate
+import transformers
+from datasets import Dataset, DatasetDict, IterableDatasetDict, interleave_datasets, load_dataset
+from datasets.distributed import split_dataset_by_node
+from huggingface_hub import Repository, create_repo
+from transformers import (
+    AutoConfig,
+    AutoFeatureExtractor,
+    AutoProcessor,
+    AutoTokenizer,
+    FlaxAutoModelForSpeechSeq2Seq,
+    HfArgumentParser,
+    Seq2SeqTrainingArguments,
+    is_tensorboard_available,
+)
+from transformers.modelcard import TrainingSummary
+from transformers.models.whisper.english_normalizer import BasicTextNormalizer
+from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE
+from transformers.file_utils import get_full_repo_name
+from transformers.utils import check_min_version, send_example_telemetry
+from transformers.utils.versions import require_version
+
+from flax.training import checkpoints
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+check_min_version("4.27.0.dev0")
+
+require_version("datasets>=1.18.2",
+                "To fix: pip install -r examples/flax/speech-recogintion/requirements.txt")
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+logger = logging.getLogger(__name__)
+
+
+@flax.struct.dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={
+            "help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    feature_extractor_name: Optional[str] = field(
+        default=None, metadata={"help": "feature extractor name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Where to store the pretrained models downloaded from huggingface.co"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={
+            "help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+    dtype: Optional[str] = field(
+        default="float32",
+        metadata={
+            "help": (
+                "Floating-point format in which the model weights should be initialized and trained. Choose one of"
+                " `[float32, float16, bfloat16]`."
+            )
+        },
+    )
+    num_beams: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Number of beams to use for evaluation. This argument will be passed to `model.generate`, "
+                "which is used during evaluation."
+            )
+        },
+    )
+
+
+@flax.struct.dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    dataset_name: str = field(
+        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
+    )
+    dataset_config_name: Optional[str] = field(
+        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
+    )
+    text_column: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "The name of the column in the datasets containing the full texts (for summarization)."},
+    )
+    dataset_cache_dir: Optional[str] = field(
+        default=None, metadata={"help": "Path to cache directory for saving and loading datasets"}
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
+    )
+    preprocessing_num_workers: Optional[int] = field(
+        default=50,
+        metadata={"help": "The number of processes to use for the preprocessing."},
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    audio_column_name: str = field(
+        default="audio",
+        metadata={
+            "help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
+    )
+    text_column_name: str = field(
+        default="text",
+        metadata={
+            "help": "The name of the dataset column containing the text data. Defaults to 'text'"},
+    )
+    max_duration_in_seconds: float = field(
+        default=30.0,
+        metadata={
+            "help": "Filter audio files that are longer than `max_duration_in_seconds` seconds"},
+    )
+    min_duration_in_seconds: float = field(
+        default=0.0,
+        metadata={
+            "help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"},
+    )
+    max_label_length: float = field(
+        default=128,
+        metadata={
+            "help": "Truncate transcriptions that are longer `max_eval_length` tokens."},
+    )
+    pad_input_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the input sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the inputs to max length."
+        },
+    )
+    pad_target_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the target sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the targets to max length."
+        },
+    )
+    train_split_name: str = field(
+        default="train",
+        metadata={
+            "help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
+        },
+    )
+    eval_split_name: str = field(
+        default="validation",
+        metadata={
+            "help": "The name of the evaluation data set split to use (via the datasets library). Defaults to 'validation'"
+        },
+    )
+    do_lower_case: bool = field(
+        default=False,
+        metadata={"help": "Whether the target text should be lower cased."},
+    )
+    do_remove_punctuation: bool = field(
+        default=False,
+        metadata={
+            "help": "Whether the target text should be striped of punctuation."},
+    )
+    do_normalize_eval: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to normalise the references and predictions in the eval WER calculation."},
+    )
+    language: str = field(
+        default=None,
+        metadata={
+            "help": (
+                "Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning "
+                "only. For English speech recognition, it should be set to `None`."
+            )
+        },
+    )
+    task: str = field(
+        default="transcribe",
+        metadata={
+            "help": "Task, either `transcribe` for speech recognition or `translate` for speech translation."},
+    )
+    num_train_steps: int = field(default=50000, metadata={
+                                 "help": "The number of training steps."})
+    shuffle_buffer_size: Optional[int] = field(
+        default=500,
+        metadata={
+            "help": (
+                "The number of streamed examples to download before shuffling them. The large the buffer, "
+                "the closer it is to real offline shuffling."
+            )
+        },
+    )
+    streaming: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use streaming mode to load and pre-process the data."},
+    )
+    log_max_eval_predictions: Optional[int] = field(
+        default=0,
+        metadata={
+            "help": (
+                "Number of label and prediction pairs to write to the summary at each evaluation step."
+            )
+        },
+    )
+    log_eval_predictions_fn: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Python path to function for logging evaluation predictions. It can be an external function like fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions, labels)."
+            )
+        },
+    )
+    run_description: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "A longer description of the run/experiment."
+            )
+        },
+    )
+    wandb_entity: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases username or entity (organization name)."
+            )
+        },
+    )
+    wandb_project: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases project to log metrics to."
+            )
+        },
+    )
+
+
+def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:
+    """
+    Shift label ids one token to the right.
+    """
+    shifted_label_ids = np.zeros_like(label_ids)
+    shifted_label_ids[:, 1:] = label_ids[:, :-1]
+    shifted_label_ids[:, 0] = decoder_start_token_id
+
+    return shifted_label_ids
+
+
+@flax.struct.dataclass
+class FlaxDataCollatorSpeechSeq2SeqWithPadding:
+    """
+    Data collator that will dynamically pad the inputs received.
+    Args:
+        processor ([`Wav2Vec2Processor`])
+            The processor used for proccessing the data.
+        decoder_start_token_id (:obj: `int`)
+            The begin-of-sentence of the decoder.
+        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)
+            among:
+            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
+              sequence if provided).
+            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
+              maximum acceptable input length for the model if that argument is not provided.
+            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
+              different lengths).
+        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).
+            See above for details.
+        max_input_length (:obj:`float`, `optional`):
+            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
+        max_target_length (:obj:`int`, `optional`):
+            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).
+        pad_input_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the input sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+        pad_target_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the target sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+    """
+
+    processor: Any
+    decoder_start_token_id: int
+    input_padding: Union[bool, str] = "longest"
+    target_padding: Union[bool, str] = "max_length"
+    max_input_length: Optional[float] = None
+    max_target_length: Optional[int] = None
+    pad_input_to_multiple_of: Optional[int] = None
+    pad_target_to_multiple_of: Optional[int] = None
+
+    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:
+        model_input_name = self.processor.model_input_names[0]
+        input_features = {model_input_name: features[model_input_name]}
+        label_features = {"input_ids": features["labels"]}
+
+        # reformat list to dict and set to pytorch format
+        batch = self.processor.feature_extractor.pad(
+            input_features,
+            max_length=self.max_input_length,
+            padding=self.input_padding,
+            pad_to_multiple_of=self.pad_input_to_multiple_of,
+            return_tensors="np",
+        )
+
+        labels_batch = self.processor.tokenizer.pad(
+            label_features,
+            max_length=self.max_target_length,
+            padding=self.target_padding,
+            pad_to_multiple_of=self.pad_target_to_multiple_of,
+            return_tensors="np",
+        )
+
+        # if bos token is appended in previous tokenization step,
+        # cut bos token here as it's append later anyways
+        labels = labels_batch["input_ids"]
+        if (labels[:, 0] == self.decoder_start_token_id).all().item():
+            labels = labels[:, 1:]
+            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]
+
+        decoder_input_ids = shift_tokens_right(
+            labels, self.decoder_start_token_id)
+
+        # replace padding with -100 to ignore correctly when computing the loss
+        labels = np.ma.array(labels, mask=np.not_equal(
+            labels_batch.attention_mask, 1))
+        labels = labels.filled(fill_value=-100)
+
+        batch["labels"] = labels
+        batch["decoder_input_ids"] = decoder_input_ids
+        batch["attention_mask"] = labels_batch.attention_mask  # Add attention_mask to the batch
+
+        return batch
+
+
+def load_maybe_streaming_dataset(dataset_name, dataset_config_name, split="train", streaming=True, **kwargs):
+    """
+    Utility function to load a dataset in streaming mode. For datasets with multiple splits,
+    each split is loaded individually and then splits combined by taking alternating examples from
+    each (interleaving).
+    """
+    if "+" in split:
+        # load multiple splits separated by the `+` symbol with streaming mode
+        dataset_splits = [
+            load_dataset(dataset_name, dataset_config_name,
+                         split=split_name, streaming=streaming, **kwargs)
+            for split_name in split.split("+")
+        ]
+        # interleave multiple splits to form one dataset
+        interleaved_dataset = interleave_datasets(dataset_splits)
+        return interleaved_dataset
+    else:
+        # load a single split *with* streaming mode
+        dataset = load_dataset(
+            dataset_name, dataset_config_name, split=split, streaming=streaming, **kwargs)
+        return dataset
+
+
+def collate_batch(samples):
+    return {key: [feature[key] for feature in samples] for key in samples[0]}
+
+def data_loader(
+    dataset: Dataset,
+    batch_size: int,
+    drop_last: bool=True,
+    num_workers: int=0,
+) -> Generator:
+    """
+    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,
+    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.
+    """
+    data_loader_iterator = iter(torch.utils.data.DataLoader(
+        batch_size=batch_size,
+        dataset=dataset.with_format("torch"),
+        num_workers=num_workers,
+        collate_fn=collate_batch,
+        drop_last=drop_last,
+    ))
+    return data_loader_iterator
+
+
+class TrainState(train_state.TrainState):
+    dropout_rng: jnp.ndarray
+
+    def replicate(self):
+        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))
+
+
+def create_learning_rate_fn(
+    num_train_steps: int, num_warmup_steps: int, learning_rate: float, start_step: int=0, warmup_init_value: float=0.0, decay_end_value: float=0.0,
+) -> Callable[[int], jnp.array]:
+    """Returns a linear warmup, linear_decay learning rate function."""
+    warmup_fn = optax.linear_schedule(
+        init_value=warmup_init_value, end_value=learning_rate, transition_steps=num_warmup_steps)
+    decay_fn = optax.linear_schedule(
+        init_value=learning_rate, end_value=decay_end_value, transition_steps=num_train_steps - num_warmup_steps
+    )
+    schedule_fn = optax.join_schedules(
+        schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])
+    
+    def learning_rate_fn(step: int) -> jnp.array:
+        return schedule_fn(step + start_step)
+    
+    return learning_rate_fn
+
+
+def main():
+    # Parse input arguments
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+    parser = HfArgumentParser(
+        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
+
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args = parser.parse_json_file(
+            json_file=os.path.abspath(sys.argv[1]))
+    else:
+        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
+    # information sent is the one passed as arguments along with your JAX/Flax versions.
+    send_example_telemetry("run_speech_recognition_seq2seq",
+                           model_args, data_args, framework="flax")
+
+    # Setup logging
+    # Make one log on every process with the configuration for debugging.
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+    # Set the verbosity to info of the Transformers logger.
+    # We only want one process per machine to log things on the screen.
+
+    # logger.setLevel(logging.INFO if jax.local_devices()[0].id%jax.local_device_count() == 0 else logging.ERROR)
+
+    # logger.setLevel(logging.INFO if jax.process_index()
+    #                == 0 else logging.ERROR)
+    
+    # Number of hosts
+    num_of_hosts = jax.process_count()
+    current_host_idx = jax.process_index()
+
+    if current_host_idx == 0:
+        datasets.utils.logging.set_verbosity_warning()
+        transformers.utils.logging.set_verbosity_info()
+    else:
+        datasets.utils.logging.set_verbosity_error()
+        transformers.utils.logging.set_verbosity_error()
+    
+    logger.setLevel(logging.INFO)
+    logger.info("Training/evaluation parameters %s", training_args)
+
+    if num_of_hosts and not training_args.push_to_hub:
+        logger.warning(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --push_to_hub to be able to save checkpoints to the hub."
+        )
+    if num_of_hosts and not training_args.overwrite_output_dir and training_args.resume_from_checkpoint:
+        logger.error(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --overwrite_output_dir to be able to resume from a pushed checkpoint."
+        )
+        sys.exit(1)
+
+    # Check the output dir is valid
+    if os.path.exists(training_args.output_dir):
+        if (
+            os.listdir(training_args.output_dir)
+            and training_args.do_train
+            and not training_args.overwrite_output_dir
+        ):
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use `--overwrite_output_dir` to overcome."
+            )
+        elif training_args.overwrite_output_dir:
+            logger.warning(f"Removing path {training_args.output_dir}")
+            shutil.rmtree(training_args.output_dir)
+      
+    # Handle the repository creation
+    output_dir = Path(training_args.output_dir)
+    if training_args.push_to_hub:
+        if training_args.hub_model_id is None:
+            repo_name = get_full_repo_name(
+                output_dir.absolute().name,
+                token=training_args.hub_token,
+                organization=training_args.push_to_hub_organization,
+            )
+        else:
+            repo_name = training_args.hub_model_id
+         
+        repo_url = None  
+        while not repo_url:
+            # Workaround for an internal HuggingFace error if the repo is being created by another worker
+            try:
+                repo_url = create_repo(
+                    repo_name, exist_ok=True, token=training_args.hub_token, private=training_args.hub_private_repo
+                )
+            except:
+                time.sleep(1)
+
+        repo = Repository(training_args.output_dir,
+                          clone_from=repo_name, token=training_args.hub_token)
+
+    # Set the model_name_or_path
+    model_name_or_path = model_args.model_name_or_path
+
+    # Try to detect last checkpoint and continue if possible
+    training_state = {"step": 0, "eval_lines": []}
+    if training_args.resume_from_checkpoint:
+        if (output_dir / "flax_model.msgpack").exists() and (output_dir / "training_state.bin").exists():
+            training_state = json.loads((output_dir / "training_state.bin").read_text())
+            model_name_or_path = os.path.join(training_args.output_dir)
+            logger.info(
+                f"Checkpoint detected, resuming training from {training_args.output_dir} at step {training_state['step']}."
+            )
+        else:
+            logger.info(
+                f"No valid checkpoint found in {training_args.output_dir}. Starting from {model_name_or_path}."
+            )
+    
+    
+    # Load dataset
+    raw_datasets = IterableDatasetDict() if data_args.streaming else DatasetDict()
+
+    if training_args.do_train:
+        raw_datasets["train"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.train_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if training_args.do_eval:
+        raw_datasets["eval"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.eval_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if not training_args.do_train and not training_args.do_eval:
+        raise ValueError(
+            "Cannot not train and not do evaluation. At least one of training or evaluation has to be performed."
+        )
+
+    raw_datasets_features = list(
+        next(iter(raw_datasets.values())).features.keys())
+
+    if data_args.audio_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--audio_column_name` to the correct audio column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    if data_args.text_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--text_column_name` to the correct text column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    # Load pretrained model, tokenizer, and feature extractor
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    feature_extractor = AutoFeatureExtractor.from_pretrained(
+        model_args.feature_extractor_name if model_args.feature_extractor_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(
+        model_name_or_path,
+        config=config,
+        dtype=getattr(jnp, model_args.dtype),
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    logger.info(
+        f"Successfully loaded the model '{model_name_or_path}'."
+    )
+    
+    if model.config.decoder_start_token_id is None:
+        raise ValueError(
+            "Make sure that `config.decoder_start_token_id` is correctly defined")
+
+    # Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,
+    # so we just need to set the correct target sampling rate.
+    dataset_sampling_rate = next(
+        iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
+
+    if dataset_sampling_rate != feature_extractor.sampling_rate:
+        raw_datasets = raw_datasets.cast_column(
+            data_args.audio_column_name, datasets.features.Audio(
+                sampling_rate=feature_extractor.sampling_rate)
+        )
+
+    # Preprocessing the datasets.
+    # We need to read the audio files as arrays and tokenize the targets.
+    max_input_length = int(
+        data_args.max_duration_in_seconds * feature_extractor.sampling_rate)
+    min_input_length = int(
+        data_args.min_duration_in_seconds * feature_extractor.sampling_rate)
+    max_label_length = (
+        data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length
+    )
+    pad_input_to_multiple_of = data_args.pad_input_to_multiple_of
+    pad_target_to_multiple_of = data_args.pad_target_to_multiple_of
+    audio_column_name = data_args.audio_column_name
+    num_workers = data_args.preprocessing_num_workers
+    text_column_name = data_args.text_column_name
+    model_input_name = feature_extractor.model_input_names[0]
+    do_lower_case = data_args.do_lower_case
+    do_remove_punctuation = data_args.do_remove_punctuation
+    normalizer = BasicTextNormalizer()  # 'official' text normalizer from OpenAI
+
+    if data_args.language is not None:
+        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)
+        tokenizer.set_prefix_tokens(
+            language=data_args.language, task=data_args.task)
+
+    def prepare_dataset(batch):
+        # Process audio
+        sample = batch[audio_column_name]
+        inputs = feature_extractor(
+            sample["array"], sampling_rate=sample["sampling_rate"])
+        # Process audio length
+        batch[model_input_name] = inputs.get(model_input_name)[0]
+        batch["input_length"] = len(sample["array"])
+
+        # Process targets
+        input_str = batch[text_column_name].lower(
+        ) if do_lower_case else batch[text_column_name]
+        if do_remove_punctuation:
+            input_str = normalizer(input_str).strip()
+        batch["labels"] = tokenizer(input_str).input_ids
+        return batch
+
+    with training_args.main_process_first(desc="dataset map pre-processing"):
+        vectorized_datasets = raw_datasets.map(
+            prepare_dataset,
+            remove_columns=raw_datasets_features,
+        )
+
+    # Filter training data with inputs longer than max_input_length
+    def is_audio_in_length_range(length):
+        return min_input_length < length < max_input_length
+
+    if training_args.do_train:
+        vectorized_datasets["train"] = vectorized_datasets["train"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    if training_args.do_eval:
+        vectorized_datasets["eval"] = vectorized_datasets["eval"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    # Load metrics and write stats
+    metric_wer = evaluate.load("wer")
+    metric_cer = evaluate.load("cer")
+    do_normalize_eval = data_args.do_normalize_eval
+
+    def compute_metrics(pred_ids, label_ids, return_preds_labels=False):
+        # Replace padded labels by the padding token
+        for idx in range(len(label_ids)):
+            label_ids[idx][label_ids[idx] == -100] = tokenizer.pad_token_id
+
+        predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
+        # We do not want to group tokens when computing the metrics
+        labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
+
+        if do_normalize_eval:
+            pred_str = [normalizer(pred) for pred in predictions]
+            label_str = [normalizer(label) for label in labels]
+            # Filtering step to only evaluate the samples that correspond to non-zero references:
+            pred_str = [pred_str[i]
+                        for i in range(len(pred_str)) if len(label_str[i]) > 0]
+            label_str = [label_str[i]
+                         for i in range(len(label_str)) if len(label_str[i]) > 0]
+        else:
+            pred_str = predictions
+            label_str = labels
+
+        wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)
+        cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)
+
+        if return_preds_labels:
+            return {"wer": wer, "cer": cer}, predictions, labels
+        else:
+            return {"wer": wer, "cer": cer}
+
+    def update_training_state(training_state, train_metrics, eval_metrics, step):
+        safe_value = lambda x: float(x.tolist() if isinstance(x, jnp.ndarray) else x)
+        state = {"step": step}
+        eval_lines = training_state["eval_lines"]
+       
+        train_metrics = get_metrics(train_metrics)
+        train_metrics_dict = {}
+        for metric_name, values in train_metrics.items():
+            tag = f"train_{metric_name}"
+            for i, value in enumerate(values):
+                train_metrics_dict[step - len(values) + i + 1] = {tag: safe_value(value)}
+
+        eval_metrics_dict = {}
+        for metric_name, value in eval_metrics.items():
+            tag = f"eval_{metric_name}"
+            eval_metrics_dict.update({
+                "step": step,
+                tag: safe_value(value),
+            })
+            if step in train_metrics_dict:
+                eval_metrics_dict.update(train_metrics_dict[step])
+        eval_lines.append(eval_metrics_dict)
+        return {**state, "eval_lines": eval_lines}
+
+    def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=None, labels=None):
+        summary_writer.scalar("train_time", train_time, step)
+
+        train_metrics = get_metrics(train_metrics)
+        for key, vals in train_metrics.items():
+            tag = f"train_{key}"
+            for i, val in enumerate(vals):
+                summary_writer.scalar(tag, val, step - len(vals) + i + 1)
+
+        for metric_name, value in eval_metrics.items():
+            summary_writer.scalar(f"eval_{metric_name}", value, step)
+        
+        # Log evaluation predictions
+        if predictions and labels:
+            df = pd.DataFrame({
+                "references": labels,
+                "predictions": predictions,
+            })
+            df["wer"] = df.apply(lambda row: metric_wer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            df["cer"] = df.apply(lambda row: metric_cer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            markdown_table = df.to_markdown(index=False)
+            eval_metrics_table = pd.DataFrame.from_dict([{"step": step, **eval_metrics}]).to_markdown(index=False)
+            summary_writer.text("eval_predictions", eval_metrics_table + "\n\n" + markdown_table, step)
+            # External logging function
+            if data_args.log_eval_predictions_fn:
+                module, fname = data_args.log_eval_predictions_fn.rsplit('.', 1)
+                fn = getattr(import_module(module), fname)
+                fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=predictions, labels=labels, training_args=training_args)
+
+    # Save feature extractor, tokenizer and config
+    feature_extractor.save_pretrained(training_args.output_dir)
+    tokenizer.save_pretrained(training_args.output_dir)
+    config.save_pretrained(training_args.output_dir)
+
+    processor = AutoProcessor.from_pretrained(training_args.output_dir)
+
+    data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(
+        processor=processor,
+        decoder_start_token_id=model.config.decoder_start_token_id,
+        input_padding="longest",
+        target_padding="longest",
+        max_target_length=max_label_length,
+        pad_input_to_multiple_of=pad_input_to_multiple_of,
+        pad_target_to_multiple_of=pad_target_to_multiple_of if pad_target_to_multiple_of else max_label_length,
+    )
+
+    # Enable tensorboard only on the master node
+    has_tensorboard = is_tensorboard_available()
+    if has_tensorboard and current_host_idx == 0:
+        try:
+            # TODO: Decouple wandb from tensorboard
+            import wandb
+
+            has_wandb = True
+        except ImportError:
+            has_wandb = False
+            if data_args.wandb_entity is not None or data_args.wandb_project is not None:
+                logger.warning(
+                    f"Unable to display metrics through Weights & Biases because some packages are not installed: {ie}"
+                )
+        try:
+            if has_wandb:
+                wandb.tensorboard.patch(root_logdir=output_dir / "runs")
+                wandb.init(
+                    entity=data_args.wandb_entity,
+                    project=data_args.wandb_project,
+                    name=training_args.run_name,
+                    notes=data_args.run_description,
+                    save_code=True,
+                    sync_tensorboard=True,
+                )
+                wandb.config.update(training_args)
+                wandb.config.update(model_args)
+                wandb.config.update(data_args)
+            from flax.metrics.tensorboard import SummaryWriter
+
+            summary_writer = SummaryWriter(
+                log_dir=output_dir / "runs" / f"{datetime.now():%b%d_%H-%M-%S}_{socket.gethostname()}")
+        except ImportError as ie:
+            has_tensorboard = False
+            logger.warning(
+                f"Unable to display metrics through TensorBoard because some packages are not installed: {ie}"
+            )
+    else:
+        logger.warning(
+            "Unable to display metrics through TensorBoard because the package is not installed: "
+            "Please run pip install tensorboard to enable."
+        )
+
+    # Initialize our training
+    rng = jax.random.PRNGKey(training_args.seed)
+    rng, dropout_rng = jax.random.split(rng)
+
+    # Store some constant
+    train_batch_size = int(
+        training_args.per_device_train_batch_size) * jax.device_count()
+    eval_batch_size = int(
+        training_args.per_device_eval_batch_size) * jax.device_count()
+
+    # Create learning rate schedule
+    lr_scheduler_types = {"linear", "constant", "constant_with_warmup"}
+    if training_args.lr_scheduler_type not in lr_scheduler_types:
+        raise ValueError(
+            f"lr_scheduler_type of type {training_args.lr_scheduler_type} not supported, choose from {lr_scheduler_types}."
+        )
+    elif training_args.lr_scheduler_type == "constant":
+        warmup_init_value = training_args.learning_rate
+        decay_end_value = training_args.learning_rate
+    elif training_args.lr_scheduler_type == "constant_with_warmup":
+        warmup_init_value = 0.0
+        decay_end_value = training_args.learning_rate
+    else:
+        warmup_init_value = 0.0
+        decay_end_value = 0.0
+        
+    linear_decay_lr_schedule_fn = create_learning_rate_fn(
+        data_args.num_train_steps,
+        training_args.warmup_steps,
+        training_args.learning_rate,
+        start_step=training_state["step"],
+        warmup_init_value=warmup_init_value,
+        decay_end_value=decay_end_value
+    )
+    
+    # We use Optax's "masking" functionality to not apply weight decay
+    # to bias and LayerNorm scale parameters. decay_mask_fn returns a
+    # mask boolean with the same structure as the parameters.
+    # The mask is True for parameters that should be decayed.
+    def decay_mask_fn(params):
+        flat_params = traverse_util.flatten_dict(params)
+        # Find out all LayerNorm parameters
+        layer_norm_candidates = ["layernorm", "layer_norm", "ln"]
+        layer_norm_named_params = set(
+            [
+                layer[-2:]
+                for layer_norm_name in layer_norm_candidates
+                for layer in flat_params.keys()
+                if layer_norm_name in "".join(layer).lower()
+            ]
+        )
+        flat_mask = {path: (path[-1] != "bias" and path[-2:]
+                            not in layer_norm_named_params) for path in flat_params}
+        return traverse_util.unflatten_dict(flat_mask)
+    
+    # Create adam optimizer
+    adamw = optax.adamw(
+        learning_rate=linear_decay_lr_schedule_fn,
+        b1=training_args.adam_beta1,
+        b2=training_args.adam_beta2,
+        eps=training_args.adam_epsilon,
+        weight_decay=training_args.weight_decay,
+        mask=decay_mask_fn,
+    )
+
+    # Setup train state
+    state = TrainState.create(
+        apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)
+
+    # Label smoothed cross entropy
+    def loss_fn(logits, labels, label_smoothing_factor=0.0):
+        """
+        The label smoothing implementation is adapted from Flax's official example:
+        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104
+        """
+        vocab_size = logits.shape[-1]
+        confidence = 1.0 - label_smoothing_factor
+        low_confidence = (1.0 - confidence) / (vocab_size - 1)
+        normalizing_constant = -(
+            confidence * jnp.log(confidence) + (vocab_size - 1) *
+            low_confidence * jnp.log(low_confidence + 1e-20)
+        )
+        soft_labels = onehot(labels, vocab_size,
+                             on_value=confidence, off_value=low_confidence)
+
+        loss = optax.softmax_cross_entropy(logits, soft_labels)
+        loss = loss - normalizing_constant
+
+        # Ignore padded tokens from loss, i.e. where labels are not set to -100
+        padding_mask = labels >= 0
+        loss = loss * padding_mask
+        loss = loss.sum()
+        num_labels = padding_mask.sum()
+        return loss, num_labels
+
+    # Define gradient update step fn
+    def train_step(state, batch, label_smoothing_factor=0.0):
+        
+        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)
+
+        def compute_loss(params):
+            labels = batch.pop("labels")
+            logits = state.apply_fn(
+                **batch, params=params, dropout_rng=dropout_rng, train=True)[0]
+            loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+            return loss, num_labels
+
+        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)
+        (loss, num_labels), grad = grad_fn(state.params)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        # True grad = total grad / total samples
+        grad = jax.lax.psum(grad, "batch")
+        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)
+        new_state = state.apply_gradients(
+            grads=grad, dropout_rng=new_dropout_rng)
+
+        metrics = {"loss": loss,
+                   "learning_rate": linear_decay_lr_schedule_fn(state.step)}
+
+        return new_state, metrics
+
+    # Define eval fn
+    def eval_step(params, batch, label_smoothing_factor=0.0):
+        labels = batch.pop("labels")
+        logits = model(**batch, params=params, train=False)[0]
+
+        loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        metrics = {"loss": loss}
+        return metrics
+
+    # Define generation function
+    num_beams = model_args.num_beams if model_args.num_beams is not None else model.config.num_beams
+    gen_kwargs = {"max_length": max_label_length, "num_beams": num_beams}
+
+    def generate_step(params, batch):
+        model.params = params
+        output_ids = model.generate(batch[model_input_name], attention_mask=batch.get(
+            "attention_mask"), **gen_kwargs)
+        return output_ids.sequences
+
+    # Create parallel version of the train and eval step
+    p_train_step = jax.pmap(
+        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch", donate_argnums=(0, )
+    )
+    p_eval_step = jax.pmap(partial(
+        eval_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch")
+    p_generate_step = jax.pmap(generate_step, "batch")
+
+    # Replicate the train state on each device
+    state = state.replicate()
+    
+    # Logging
+    logger.info("***** Running training *****")
+    logger.info(
+        f"  Dataset name = {data_args.dataset_name}")
+    logger.info(
+        f"  Dataset config name = {data_args.dataset_config_name}")
+    logger.info(
+        f"  Learning rate = {training_args.learning_rate}")
+    logger.info(
+        f"  Scheduler = {training_args.lr_scheduler_type}")
+    logger.info(
+        f"  Num examples = {data_args.num_train_steps * train_batch_size}")
+    if num_of_hosts > 1:
+        logger.info(
+            f"  Number of hosts = {num_of_hosts}")
+        logger.info(
+            f"  Current host idx = {current_host_idx}")
+    logger.info(
+        f"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}")
+    logger.info(
+        f"  Total train batch size per node (w. parallel & distributed) = {train_batch_size // num_of_hosts}")
+    logger.info(
+        f"  Total train batch size (w. parallel & distributed) = {train_batch_size}")
+    logger.info(f"  Total optimization steps = {data_args.num_train_steps - training_state['step']}")
+    if training_state['step'] > 0:
+        logger.info(f"  ↪ Starting at {str(training_state['step'])} and finishing at {str(data_args.num_train_steps)}")
+
+    train_time = 0
+
+    # Training summary
+    language_code = None  # Maybe 'multilingual'?
+    if data_args.language is not None:
+        language = data_args.language.lower()
+        if language in TO_LANGUAGE_CODE:
+            language_code = TO_LANGUAGE_CODE[language]
+        elif len(language) == 2:
+            language_code = language
+    training_summary = {
+        "model_name": repo_name.split("/")[-1],
+        "language": language_code,
+        "tags": ["audio", "asr", "automatic-speech-recognition", "hf-asr-leaderboard"],
+        "license": "apache-2.0",
+        "finetuned_from": model_args.model_name_or_path,
+        "tasks": ["asr"],
+        "dataset": data_args.dataset_name,
+        "dataset_args": {"name": data_args.dataset_config_name},
+        "source": "flax",
+        "eval_lines": [],
+        "eval_results": None,
+        "hyperparameters": {
+            "learning_rate": training_args.learning_rate,
+            "lr_scheduler_type": training_args.lr_scheduler_type,
+            "per_device_train_batch_size": training_args.per_device_train_batch_size,
+            "total_train_batch_size_per_node": train_batch_size // num_of_hosts,
+            "total_train_batch_size": train_batch_size,
+            "total_optimization_steps": data_args.num_train_steps - training_state['step'],
+            "starting_optimization_step": training_state['step'] if training_state['step'] > 0 else None,
+            "finishing_optimization_step": data_args.num_train_steps,
+            "num_train_dataset_workers": f"{num_workers}",
+            "total_num_training_examples": data_args.num_train_steps * train_batch_size,
+        },
+        # TODO: Adapt https://github.com/huggingface/transformers/blob/main/src/transformers/modelcard.py#L855
+        # "hyperparameters": training_args.to_sanitized_dict()
+    }
+    
+    # Create README if it does not exist
+    readme = output_dir / "README.md"
+    if not readme.exists():
+        readme.write_text(TrainingSummary(**training_summary).to_model_card())
+    
+    # ======================== Training ================================
+    train_start = time.time()
+
+    train_metrics = []
+    epoch = 0
+    train_dataset = vectorized_datasets["train"].shuffle(seed=training_args.seed, buffer_size=data_args.shuffle_buffer_size)
+    
+    # Split by node
+    train_dataset = split_dataset_by_node(train_dataset, rank=current_host_idx, world_size=num_of_hosts)   
+    
+    if train_dataset.n_shards < data_args.preprocessing_num_workers:
+        num_workers = train_dataset.n_shards
+
+    logger.info(f"  Number of train dataset workers = {num_workers} {'(Capped by the number of dataset shards)' if train_dataset.n_shards < data_args.preprocessing_num_workers else ''} {'(ADVICE: In most cases you will speed up training considerably if you increase the value of --preprocessing_num_workers!)' if num_workers < 10 else ''}")
+ 
+    eval_dataset = vectorized_datasets["eval"]
+    train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+    
+    if not training_args.ignore_data_skip and training_state["step"] > 0:
+        logger.info(
+            f"  Will skip the first {training_state['step']} steps. If this takes a lot of time,"
+            " you can add the `--ignore_data_skip` flag to your launch command, but you will resume the"
+            " training on data already seen by your model."
+        )
+        for step in tqdm(range(training_state["step"]), desc=f"Skipping data for {training_state['step']} steps...", position=1, leave=False):
+            try:
+                samples = next(train_loader)
+            except StopIteration:
+                epoch += 1
+                train_dataset.set_epoch(epoch)
+                train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+                samples = next(train_loader)
+            batch = data_collator(samples)
+            # batch = shard(batch.data)
+
+    for step in tqdm(range(data_args.num_train_steps), desc="Training...", position=1, leave=False):
+        # Skip initial steps if these are specified. 
+        if step < training_state["step"]:
+            continue
+        
+        # =========================== Training ===========================
+        try:
+            samples = next(train_loader)
+        except StopIteration:
+            epoch += 1
+            train_dataset.set_epoch(epoch)
+            train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+            samples = next(train_loader)
+            logger.info(
+                f"Completed epoch ({epoch} | Loss: {train_metric['loss']}, Learning Rate:"
+                f" {train_metric['learning_rate']})"
+            )
+
+        batch = data_collator(samples)
+        batch = shard(batch.data)
+        
+        state, train_metric = p_train_step(state, batch)
+        
+        train_metrics.append(train_metric)
+        
+        train_time += time.time() - train_start
+        train_metric = unreplicate(train_metric)
+
+        # ========================== Evaluating ==========================
+        # Evaluate at each eval_steps, and at the end of training at num_train_steps
+        if step % training_args.eval_steps == 0 or step == data_args.num_train_steps - 1:
+            logger.info(
+                f"Starting evaluation at step {step} of num_training_step {data_args.num_train_steps} steps. Planned evaluation every {training_args.eval_steps} steps." 
+            )
+            eval_metrics = []
+            eval_preds = []
+            eval_labels = []
+            eval_loader = data_loader(eval_dataset, eval_batch_size, drop_last=False)
+            if data_args.max_eval_samples:
+                max_eval_steps_iter = range(1 + data_args.max_eval_samples // eval_batch_size)
+            else:
+                max_eval_steps_iter = itertools.repeat(None)
+            for _ in tqdm(max_eval_steps_iter, desc="Evaluating...", position=2, leave=False):
+                # Model forward
+                try:
+                    samples = next(eval_loader)
+                except StopIteration:
+                    break
+                batch = data_collator(samples)
+                
+                labels = batch["labels"]
+
+                metrics = pad_shard_unpad(p_eval_step, static_return=True)(
+                    state.params, batch.data, min_device_batch=training_args.per_device_eval_batch_size
+                )
+                eval_metrics.append(metrics)
+
+                # Generation
+                if training_args.predict_with_generate:
+                    generated_ids = pad_shard_unpad(
+                        p_generate_step)(state.params, batch.data)
+                    eval_preds.extend(jax.device_get(
+                        generated_ids.reshape(-1, gen_kwargs["max_length"])))
+                    eval_labels.extend(labels)
+
+            # Normalize eval metrics
+            eval_metrics = get_metrics(eval_metrics)
+            eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)
+
+            # Compute metrics
+            metric_desc = ""
+            if training_args.predict_with_generate:
+                metric_values, pred_str, label_str = compute_metrics(
+                    eval_preds, eval_labels, return_preds_labels=True
+                )
+                eval_metrics.update(metric_values)
+                metric_desc = " | ".join(
+                    [f"Eval {key}: {value}" for key, value in metric_values.items()])
+
+            # Print metrics
+            desc = f"Step: {step} | Epoch: {epoch} (Eval Loss: {eval_metrics['loss']} | {metric_desc})"
+            logger.info(desc)
+
+            # Update training state
+            training_state = update_training_state(
+                training_state,
+                train_metrics,
+                eval_metrics,
+                step,
+            )
+
+            # Save metrics
+            if has_tensorboard and current_host_idx == 0:
+                log_max_predictions = data_args.log_max_eval_predictions if data_args.log_max_eval_predictions else 0
+                write_metric(
+                    summary_writer,
+                    train_metrics,
+                    eval_metrics,
+                    train_time,
+                    step,
+                    predictions=pred_str[:log_max_predictions],
+                    labels=label_str[:log_max_predictions]
+                )
+
+            # Save checkpoint at each eval_steps and push checkpoint to the hub
+            if current_host_idx  == 0:
+                params = jax.device_get(
+                    jax.tree_util.tree_map(lambda x: x[0], state.params))
+                model.save_pretrained(training_args.output_dir, params=params)
+                tokenizer.save_pretrained(training_args.output_dir)
+                # Report eval results if training is done
+                if step == data_args.num_train_steps - 1:
+                    training_summary["eval_results"] = training_state["eval_lines"][-1]
+                else:
+                    training_summary.update({"eval_lines": training_state["eval_lines"]})
+                (output_dir / "training_state.bin").write_text(json.dumps(training_state))
+                # Write model card
+                readme.write_text(TrainingSummary(**training_summary).to_model_card())
+                if training_args.push_to_hub:
+                    repo.push_to_hub(
+                        commit_message=f"Saving weights and logs of step {step} - epoch {epoch}", blocking=False)
+
+if __name__ == "__main__":
+    main()
diff --git a/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/config.yaml b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/config.yaml
new file mode 100644
index 0000000..1fdc8fa
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/config.yaml
@@ -0,0 +1,512 @@
+wandb_version: 1
+
+__cached__setup_devices:
+  desc: null
+  value: cpu
+_n_gpu:
+  desc: null
+  value: 0
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.14.0
+    code_path: code/run_flax_speech_recognition_seq2seq_streaming.py
+    framework: huggingface
+    huggingface_version: 4.28.0.dev0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.10
+    start_time: 1681318768.043231
+    t:
+      1:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      2:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      3:
+      - 13
+      - 23
+      - 34
+      4: 3.8.10
+      5: 0.14.0
+      6: 4.28.0.dev0
+      8:
+      - 5
+adafactor:
+  desc: null
+  value: false
+adam_beta1:
+  desc: null
+  value: 0.9
+adam_beta2:
+  desc: null
+  value: 0.999
+adam_epsilon:
+  desc: null
+  value: 1.0e-08
+audio_column_name:
+  desc: null
+  value: audio
+auto_find_batch_size:
+  desc: null
+  value: false
+bf16:
+  desc: null
+  value: false
+bf16_full_eval:
+  desc: null
+  value: false
+cache_dir:
+  desc: null
+  value: null
+config_name:
+  desc: null
+  value: null
+data_seed:
+  desc: null
+  value: null
+dataloader_drop_last:
+  desc: null
+  value: false
+dataloader_num_workers:
+  desc: null
+  value: 0
+dataloader_pin_memory:
+  desc: null
+  value: true
+dataset_cache_dir:
+  desc: null
+  value: null
+dataset_config_name:
+  desc: null
+  value: null
+dataset_name:
+  desc: null
+  value: NbAiLab/NCC_speech_all_v5
+ddp_bucket_cap_mb:
+  desc: null
+  value: null
+ddp_find_unused_parameters:
+  desc: null
+  value: null
+ddp_timeout:
+  desc: null
+  value: 1800
+debug:
+  desc: null
+  value: []
+deepspeed:
+  desc: null
+  value: null
+disable_tqdm:
+  desc: null
+  value: false
+do_eval:
+  desc: null
+  value: true
+do_lower_case:
+  desc: null
+  value: false
+do_normalize_eval:
+  desc: null
+  value: true
+do_predict:
+  desc: null
+  value: false
+do_remove_punctuation:
+  desc: null
+  value: false
+do_train:
+  desc: null
+  value: true
+dtype:
+  desc: null
+  value: bfloat16
+eval_accumulation_steps:
+  desc: null
+  value: null
+eval_delay:
+  desc: null
+  value: 0
+eval_split_name:
+  desc: null
+  value: validation
+eval_steps:
+  desc: null
+  value: 10000
+evaluation_strategy:
+  desc: null
+  value: IntervalStrategy.NO
+feature_extractor_name:
+  desc: null
+  value: null
+fp16:
+  desc: null
+  value: false
+fp16_backend:
+  desc: null
+  value: auto
+fp16_full_eval:
+  desc: null
+  value: false
+fp16_opt_level:
+  desc: null
+  value: O1
+fsdp:
+  desc: null
+  value: []
+fsdp_config:
+  desc: null
+  value:
+    fsdp_min_num_params: 0
+    xla: false
+    xla_fsdp_grad_ckpt: false
+fsdp_min_num_params:
+  desc: null
+  value: 0
+fsdp_transformer_layer_cls_to_wrap:
+  desc: null
+  value: null
+full_determinism:
+  desc: null
+  value: false
+generation_config:
+  desc: null
+  value: null
+generation_max_length:
+  desc: null
+  value: null
+generation_num_beams:
+  desc: null
+  value: null
+gradient_accumulation_steps:
+  desc: null
+  value: 1
+gradient_checkpointing:
+  desc: null
+  value: false
+greater_is_better:
+  desc: null
+  value: null
+group_by_length:
+  desc: null
+  value: false
+half_precision_backend:
+  desc: null
+  value: auto
+hub_model_id:
+  desc: null
+  value: NbAiLab/scream_large_oct_debug
+hub_private_repo:
+  desc: null
+  value: true
+hub_strategy:
+  desc: null
+  value: HubStrategy.EVERY_SAVE
+hub_token:
+  desc: null
+  value: null
+ignore_data_skip:
+  desc: null
+  value: true
+include_inputs_for_metrics:
+  desc: null
+  value: false
+jit_mode_eval:
+  desc: null
+  value: false
+label_names:
+  desc: null
+  value: null
+label_smoothing_factor:
+  desc: null
+  value: 0.0
+language:
+  desc: null
+  value: Norwegian
+learning_rate:
+  desc: null
+  value: 5.0e-06
+length_column_name:
+  desc: null
+  value: length
+load_best_model_at_end:
+  desc: null
+  value: false
+local_rank:
+  desc: null
+  value: -1
+log_eval_predictions_fn:
+  desc: null
+  value: log_predictions.write_predictions
+log_level:
+  desc: null
+  value: passive
+log_level_replica:
+  desc: null
+  value: warning
+log_max_eval_predictions:
+  desc: null
+  value: 100
+log_on_each_node:
+  desc: null
+  value: true
+logging_dir:
+  desc: null
+  value: ../../scream_large_oct_debug/runs/Apr12_16-58-40_t1v-n-0a06f6ef-w-0
+logging_first_step:
+  desc: null
+  value: false
+logging_nan_inf_filter:
+  desc: null
+  value: true
+logging_steps:
+  desc: null
+  value: 500
+logging_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+lr_scheduler_type:
+  desc: null
+  value: SchedulerType.LINEAR
+max_duration_in_seconds:
+  desc: null
+  value: 30.0
+max_eval_samples:
+  desc: null
+  value: null
+max_grad_norm:
+  desc: null
+  value: 1.0
+max_label_length:
+  desc: null
+  value: 128
+max_steps:
+  desc: null
+  value: -1
+max_train_samples:
+  desc: null
+  value: null
+metric_for_best_model:
+  desc: null
+  value: null
+min_duration_in_seconds:
+  desc: null
+  value: 0.0
+model_name_or_path:
+  desc: null
+  value: openai/whisper-large-v2
+model_revision:
+  desc: null
+  value: main
+mp_parameters:
+  desc: null
+  value: ''
+no_cuda:
+  desc: null
+  value: false
+num_beams:
+  desc: null
+  value: 5
+num_train_epochs:
+  desc: null
+  value: 3.0
+num_train_steps:
+  desc: null
+  value: 50000
+optim:
+  desc: null
+  value: OptimizerNames.ADAMW_HF
+optim_args:
+  desc: null
+  value: null
+output_dir:
+  desc: null
+  value: ../../scream_large_oct_debug
+overwrite_cache:
+  desc: null
+  value: false
+overwrite_output_dir:
+  desc: null
+  value: true
+pad_input_to_multiple_of:
+  desc: null
+  value: null
+pad_target_to_multiple_of:
+  desc: null
+  value: null
+past_index:
+  desc: null
+  value: -1
+per_device_eval_batch_size:
+  desc: null
+  value: 2
+per_device_train_batch_size:
+  desc: null
+  value: 2
+per_gpu_eval_batch_size:
+  desc: null
+  value: null
+per_gpu_train_batch_size:
+  desc: null
+  value: null
+predict_with_generate:
+  desc: null
+  value: true
+prediction_loss_only:
+  desc: null
+  value: false
+preprocessing_num_workers:
+  desc: null
+  value: 32
+push_to_hub:
+  desc: null
+  value: true
+push_to_hub_model_id:
+  desc: null
+  value: null
+push_to_hub_organization:
+  desc: null
+  value: null
+push_to_hub_token:
+  desc: null
+  value: null
+ray_scope:
+  desc: null
+  value: last
+remove_unused_columns:
+  desc: null
+  value: true
+report_to:
+  desc: null
+  value:
+  - tensorboard
+  - wandb
+resume_from_checkpoint:
+  desc: null
+  value: 'True'
+run_description:
+  desc: null
+  value: A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear
+    decay on the all_v5-corpus.
+run_name:
+  desc: null
+  value: ScreamLarge - debug_beam5_long
+save_on_each_node:
+  desc: null
+  value: false
+save_steps:
+  desc: null
+  value: 500
+save_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+save_total_limit:
+  desc: null
+  value: null
+seed:
+  desc: null
+  value: 42
+sharded_ddp:
+  desc: null
+  value: []
+shuffle_buffer_size:
+  desc: null
+  value: 500
+skip_memory_metrics:
+  desc: null
+  value: true
+sortish_sampler:
+  desc: null
+  value: false
+streaming:
+  desc: null
+  value: true
+task:
+  desc: null
+  value: transcribe
+text_column:
+  desc: null
+  value: null
+text_column_name:
+  desc: null
+  value: text
+tf32:
+  desc: null
+  value: null
+tokenizer_name:
+  desc: null
+  value: null
+torch_compile:
+  desc: null
+  value: false
+torch_compile_backend:
+  desc: null
+  value: null
+torch_compile_mode:
+  desc: null
+  value: null
+torchdynamo:
+  desc: null
+  value: null
+tpu_metrics_debug:
+  desc: null
+  value: false
+tpu_num_cores:
+  desc: null
+  value: null
+train_split_name:
+  desc: null
+  value: train
+use_auth_token:
+  desc: null
+  value: true
+use_fast_tokenizer:
+  desc: null
+  value: true
+use_ipex:
+  desc: null
+  value: false
+use_legacy_prediction_loop:
+  desc: null
+  value: false
+use_mps_device:
+  desc: null
+  value: false
+wandb_entity:
+  desc: null
+  value: nbailab
+wandb_project:
+  desc: null
+  value: Scream - septimus
+warmup_ratio:
+  desc: null
+  value: 0.0
+warmup_steps:
+  desc: null
+  value: 10000
+weight_decay:
+  desc: null
+  value: 0.0
+xpu_backend:
+  desc: null
+  value: null
diff --git a/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/events.out.tfevents.1681318769.t1v-n-0a06f6ef-w-0.1586006.0.v2 b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/events.out.tfevents.1681318769.t1v-n-0a06f6ef-w-0.1586006.0.v2
new file mode 120000
index 0000000..62a515d
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/events.out.tfevents.1681318769.t1v-n-0a06f6ef-w-0.1586006.0.v2
@@ -0,0 +1 @@
+/home/perk/models/scream_large_oct_debug/runs/Apr12_16-59-29_t1v-n-0a06f6ef-w-0/events.out.tfevents.1681318769.t1v-n-0a06f6ef-w-0.1586006.0.v2
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/requirements.txt b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/requirements.txt
new file mode 100644
index 0000000..34d1446
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/requirements.txt
@@ -0,0 +1,140 @@
+absl-py==1.4.0
+aiohttp==3.8.4
+aiosignal==1.3.1
+appdirs==1.4.4
+astunparse==1.6.3
+async-timeout==4.0.2
+attrs==22.2.0
+audioread==3.0.0
+cached-property==1.5.2
+cachetools==5.3.0
+certifi==2022.12.7
+cffi==1.15.1
+charset-normalizer==3.1.0
+chex==0.1.7
+click==8.1.3
+cmake==3.26.1
+datasets==2.11.0
+decorator==5.1.1
+dill==0.3.6
+dm-tree==0.1.8
+docker-pycreds==0.4.0
+etils==1.1.1
+evaluate==0.4.0
+filelock==3.10.7
+flatbuffers==23.3.3
+flax==0.6.8
+frozenlist==1.3.3
+fsspec==2023.3.0
+gast==0.4.0
+gitdb==4.0.10
+gitpython==3.1.31
+google-auth-oauthlib==1.0.0
+google-auth==2.17.1
+google-pasta==0.2.0
+grpcio==1.53.0
+h5py==3.8.0
+huggingface-hub==0.13.3
+idna==3.4
+importlib-metadata==6.1.0
+importlib-resources==5.12.0
+jax==0.4.8
+jaxlib==0.4.7
+jinja2==3.1.2
+jiwer==3.0.1
+joblib==1.2.0
+keras==2.12.0
+lazy-loader==0.2
+libclang==16.0.0
+librosa==0.10.0.post2
+libtpu-nightly==0.1.dev20230327
+lit==16.0.0
+llvmlite==0.39.1
+markdown-it-py==2.2.0
+markdown==3.4.3
+markupsafe==2.1.2
+mdurl==0.1.2
+ml-dtypes==0.0.4
+mpmath==1.3.0
+msgpack==1.0.5
+multidict==6.0.4
+multiprocess==0.70.14
+nest-asyncio==1.5.6
+networkx==3.0
+numba==0.56.4
+numpy==1.23.5
+nvidia-cublas-cu11==11.10.3.66
+nvidia-cuda-cupti-cu11==11.7.101
+nvidia-cuda-nvrtc-cu11==11.7.99
+nvidia-cuda-runtime-cu11==11.7.99
+nvidia-cudnn-cu11==8.5.0.96
+nvidia-cufft-cu11==10.9.0.58
+nvidia-curand-cu11==10.2.10.91
+nvidia-cusolver-cu11==11.4.0.1
+nvidia-cusparse-cu11==11.7.4.91
+nvidia-nccl-cu11==2.14.3
+nvidia-nvtx-cu11==11.7.91
+oauthlib==3.2.2
+opt-einsum==3.3.0
+optax==0.1.4
+orbax==0.1.7
+packaging==23.0
+pandas==1.5.3
+pathtools==0.1.2
+pip==23.0.1
+pkg-resources==0.0.0
+pooch==1.6.0
+protobuf==4.22.1
+psutil==5.9.4
+pyarrow==11.0.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pycparser==2.21
+pydub==0.25.1
+pygments==2.14.0
+python-dateutil==2.8.2
+pytz==2023.3
+pyyaml==6.0
+rapidfuzz==2.13.7
+regex==2023.3.23
+requests-oauthlib==1.3.1
+requests==2.28.2
+responses==0.18.0
+rich==13.3.3
+rsa==4.9
+scikit-learn==1.2.2
+scipy==1.10.1
+sentry-sdk==1.18.0
+setproctitle==1.3.2
+setuptools==44.0.0
+six==1.16.0
+smmap==5.0.0
+soundfile==0.12.1
+soxr==0.3.4
+sympy==1.11.1
+tabulate==0.9.0
+tensorboard-data-server==0.7.0
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.12.1
+tensorflow-estimator==2.12.0
+tensorflow-io-gcs-filesystem==0.32.0
+tensorflow==2.12.0
+tensorstore==0.1.35
+termcolor==2.2.0
+threadpoolctl==3.1.0
+tokenizers==0.13.2
+toolz==0.12.0
+torch==2.0.0
+torchaudio==2.0.1
+tqdm==4.65.0
+transformers==4.28.0.dev0
+triton==2.0.0
+typing-extensions==4.5.0
+urllib3==1.26.15
+wandb==0.14.0
+werkzeug==2.2.3
+wheel==0.40.0
+wrapt==1.14.1
+xxhash==3.2.0
+yarl==1.8.2
+zipp==3.15.0
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/wandb-metadata.json b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/wandb-metadata.json
new file mode 100644
index 0000000..a687bf5
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/wandb-metadata.json
@@ -0,0 +1,1300 @@
+{
+    "os": "Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2023-04-12T16:59:28.998930",
+    "startedAt": "2023-04-12T16:59:28.033777",
+    "docker": null,
+    "cuda": null,
+    "args": [
+        "--model_name_or_path",
+        "openai/whisper-large-v2",
+        "--run_name",
+        "ScreamLarge - debug_beam5_long",
+        "--run_description",
+        "A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear decay on the all_v5-corpus.",
+        "--wandb_entity",
+        "nbailab",
+        "--wandb_project",
+        "Scream - septimus",
+        "--dataset_name",
+        "NbAiLab/NCC_speech_all_v5",
+        "--language",
+        "Norwegian",
+        "--text_column_name",
+        "text",
+        "--train_split_name",
+        "train",
+        "--eval_split_name",
+        "validation",
+        "--output_dir",
+        "../../scream_large_oct_debug",
+        "--overwrite_output_dir",
+        "--warmup_steps",
+        "10000",
+        "--do_train",
+        "--do_eval",
+        "--num_train_steps",
+        "50000",
+        "--lr_scheduler_type",
+        "linear",
+        "--eval_steps",
+        "10000",
+        "--learning_rate",
+        "5e-6",
+        "--preprocessing_num_workers",
+        "32",
+        "--per_device_train_batch_size",
+        "2",
+        "--per_device_eval_batch_size",
+        "2",
+        "--predict_with_generate",
+        "--log_max_eval_predictions",
+        "100",
+        "--log_eval_predictions_fn",
+        "log_predictions.write_predictions",
+        "--streaming",
+        "True",
+        "--use_auth_token",
+        "True",
+        "--dtype",
+        "bfloat16",
+        "--hub_private_repo",
+        "True",
+        "--hub_model_id",
+        "NbAiLab/scream_large_oct_debug",
+        "--resume_from_checkpoint",
+        "True",
+        "--num_beams",
+        "5",
+        "--ignore_data_skip",
+        "--push_to_hub"
+    ],
+    "state": "running",
+    "program": "../run_flax_speech_recognition_seq2seq_streaming.py",
+    "codePath": "run_flax_speech_recognition_seq2seq_streaming.py",
+    "git": {
+        "remote": "https://github.com/NbAiLab/nb-whisper.git",
+        "commit": "e5b7f52184c9d5849707b37c56004016508841ec"
+    },
+    "email": "per@capia.no",
+    "root": "/home/perk/models/nb-whisper",
+    "host": "t1v-n-0a06f6ef-w-0",
+    "username": "perk",
+    "executable": "/home/perk/.whisper/bin/python",
+    "cpu_count": 120,
+    "cpu_count_logical": 240,
+    "cpu_freq": {
+        "current": 2249.9980000000096,
+        "min": 0.0,
+        "max": 0.0
+    },
+    "cpu_freq_per_core": [
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        }
+    ],
+    "disk": {
+        "total": 96.74600601196289,
+        "used": 33.3736686706543
+    },
+    "memory": {
+        "total": 400.47254943847656
+    }
+}
diff --git a/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/wandb-summary.json b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/wandb-summary.json
new file mode 100644
index 0000000..9e26dfe
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/files/wandb-summary.json
@@ -0,0 +1 @@
+{}
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/run-be72b9r5.wandb b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/run-be72b9r5.wandb
new file mode 100644
index 0000000..17ba4f6
Binary files /dev/null and b/experiment_scream_octavus/wandb/run-20230412_165928-be72b9r5/run-be72b9r5.wandb differ
diff --git a/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/code/run_flax_speech_recognition_seq2seq_streaming_debug.py b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/code/run_flax_speech_recognition_seq2seq_streaming_debug.py
new file mode 100644
index 0000000..b938a35
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/code/run_flax_speech_recognition_seq2seq_streaming_debug.py
@@ -0,0 +1,1318 @@
+#!/usr/bin/env python
+# coding=utf-8
+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR COND    ITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""
+Fine-tuning the Flax library models for sequence to sequence speech recognition.
+"""
+# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.
+
+import itertools
+import json
+import logging
+import os
+import shutil
+import socket
+import sys
+import tempfile
+import time
+from dataclasses import field
+from datetime import datetime
+from functools import partial
+from importlib import import_module
+from pathlib import Path
+from typing import Any, Callable, Dict, Generator, List, Optional, Union
+
+import flax
+import jax
+import jax.numpy as jnp 
+import numpy as np
+import optax
+import pandas as pd
+import torch
+# from jax.experimental.compilation_cache import compilation_cache; compilation_cache.initialize_cache(tempfile.gettempdir())
+from flax import jax_utils, traverse_util
+from flax.jax_utils import pad_shard_unpad, unreplicate
+from flax.training import train_state
+from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key
+from torch.utils.data import IterableDataset
+from tqdm import tqdm
+
+import datasets
+import evaluate
+import transformers
+from datasets import Dataset, DatasetDict, IterableDatasetDict, interleave_datasets, load_dataset
+from datasets.distributed import split_dataset_by_node
+from huggingface_hub import Repository, create_repo
+from transformers import (
+    AutoConfig,
+    AutoFeatureExtractor,
+    AutoProcessor,
+    AutoTokenizer,
+    FlaxAutoModelForSpeechSeq2Seq,
+    HfArgumentParser,
+    Seq2SeqTrainingArguments,
+    is_tensorboard_available,
+)
+from transformers.modelcard import TrainingSummary
+from transformers.models.whisper.english_normalizer import BasicTextNormalizer
+from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE
+from transformers.file_utils import get_full_repo_name
+from transformers.utils import check_min_version, send_example_telemetry
+from transformers.utils.versions import require_version
+
+from flax.training import checkpoints
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+check_min_version("4.27.0.dev0")
+
+require_version("datasets>=1.18.2",
+                "To fix: pip install -r examples/flax/speech-recogintion/requirements.txt")
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+logger = logging.getLogger(__name__)
+
+
+@flax.struct.dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={
+            "help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    feature_extractor_name: Optional[str] = field(
+        default=None, metadata={"help": "feature extractor name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Where to store the pretrained models downloaded from huggingface.co"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={
+            "help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+    dtype: Optional[str] = field(
+        default="float32",
+        metadata={
+            "help": (
+                "Floating-point format in which the model weights should be initialized and trained. Choose one of"
+                " `[float32, float16, bfloat16]`."
+            )
+        },
+    )
+    num_beams: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Number of beams to use for evaluation. This argument will be passed to `model.generate`, "
+                "which is used during evaluation."
+            )
+        },
+    )
+
+
+@flax.struct.dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    dataset_name: str = field(
+        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
+    )
+    dataset_config_name: Optional[str] = field(
+        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
+    )
+    text_column: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "The name of the column in the datasets containing the full texts (for summarization)."},
+    )
+    dataset_cache_dir: Optional[str] = field(
+        default=None, metadata={"help": "Path to cache directory for saving and loading datasets"}
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
+    )
+    preprocessing_num_workers: Optional[int] = field(
+        default=50,
+        metadata={"help": "The number of processes to use for the preprocessing."},
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    audio_column_name: str = field(
+        default="audio",
+        metadata={
+            "help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
+    )
+    text_column_name: str = field(
+        default="text",
+        metadata={
+            "help": "The name of the dataset column containing the text data. Defaults to 'text'"},
+    )
+    max_duration_in_seconds: float = field(
+        default=30.0,
+        metadata={
+            "help": "Filter audio files that are longer than `max_duration_in_seconds` seconds"},
+    )
+    min_duration_in_seconds: float = field(
+        default=0.0,
+        metadata={
+            "help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"},
+    )
+    max_label_length: float = field(
+        default=256,
+        metadata={
+            "help": "Truncate transcriptions that are longer `max_label_length` tokens."},
+    )
+    pad_input_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the input sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the inputs to max length."
+        },
+    )
+    pad_target_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the target sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the targets to max length."
+        },
+    )
+    train_split_name: str = field(
+        default="train",
+        metadata={
+            "help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
+        },
+    )
+    eval_split_name: str = field(
+        default="validation",
+        metadata={
+            "help": "The name of the evaluation data set split to use (via the datasets library). Defaults to 'validation'"
+        },
+    )
+    do_lower_case: bool = field(
+        default=False,
+        metadata={"help": "Whether the target text should be lower cased."},
+    )
+    do_remove_punctuation: bool = field(
+        default=False,
+        metadata={
+            "help": "Whether the target text should be striped of punctuation."},
+    )
+    do_normalize_eval: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to normalise the references and predictions in the eval WER calculation."},
+    )
+    language: str = field(
+        default=None,
+        metadata={
+            "help": (
+                "Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning "
+                "only. For English speech recognition, it should be set to `None`."
+            )
+        },
+    )
+    task: str = field(
+        default="transcribe",
+        metadata={
+            "help": "Task, either `transcribe` for speech recognition or `translate` for speech translation."},
+    )
+    num_train_steps: int = field(default=50000, metadata={
+                                 "help": "The number of training steps."})
+    shuffle_buffer_size: Optional[int] = field(
+        default=500,
+        metadata={
+            "help": (
+                "The number of streamed examples to download before shuffling them. The large the buffer, "
+                "the closer it is to real offline shuffling."
+            )
+        },
+    )
+    streaming: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use streaming mode to load and pre-process the data."},
+    )
+    log_max_eval_predictions: Optional[int] = field(
+        default=0,
+        metadata={
+            "help": (
+                "Number of label and prediction pairs to write to the summary at each evaluation step."
+            )
+        },
+    )
+    log_eval_predictions_fn: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Python path to function for logging evaluation predictions. It can be an external function like fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions, labels)."
+            )
+        },
+    )
+    run_description: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "A longer description of the run/experiment."
+            )
+        },
+    )
+    wandb_entity: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases username or entity (organization name)."
+            )
+        },
+    )
+    wandb_project: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases project to log metrics to."
+            )
+        },
+    )
+
+
+def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:
+    """
+    Shift label ids one token to the right.
+    """
+    shifted_label_ids = np.zeros_like(label_ids)
+    shifted_label_ids[:, 1:] = label_ids[:, :-1]
+    shifted_label_ids[:, 0] = decoder_start_token_id
+
+    return shifted_label_ids
+
+
+@flax.struct.dataclass
+class FlaxDataCollatorSpeechSeq2SeqWithPadding:
+    """
+    Data collator that will dynamically pad the inputs received.
+    Args:
+        processor ([`Wav2Vec2Processor`])
+            The processor used for proccessing the data.
+        decoder_start_token_id (:obj: `int`)
+            The begin-of-sentence of the decoder.
+        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)
+            among:
+            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
+              sequence if provided).
+            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
+              maximum acceptable input length for the model if that argument is not provided.
+            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
+              different lengths).
+        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).
+            See above for details.
+        max_input_length (:obj:`float`, `optional`):
+            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
+        max_target_length (:obj:`int`, `optional`):
+            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).
+        pad_input_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the input sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+        pad_target_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the target sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+    """
+
+    processor: Any
+    decoder_start_token_id: int
+    input_padding: Union[bool, str] = "longest"
+    target_padding: Union[bool, str] = "max_length"
+    max_input_length: Optional[float] = None
+    max_target_length: Optional[int] = None
+    pad_input_to_multiple_of: Optional[int] = None
+    pad_target_to_multiple_of: Optional[int] = None
+
+    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:
+        model_input_name = self.processor.model_input_names[0]
+        input_features = {model_input_name: features[model_input_name]}
+        label_features = {"input_ids": features["labels"]}
+
+        # reformat list to dict and set to pytorch format
+        batch = self.processor.feature_extractor.pad(
+            input_features,
+            max_length=self.max_input_length,
+            padding=self.input_padding,
+            pad_to_multiple_of=self.pad_input_to_multiple_of,
+            return_tensors="np",
+        )
+
+        labels_batch = self.processor.tokenizer.pad(
+            label_features,
+            max_length=self.max_target_length,
+            padding=self.target_padding,
+            pad_to_multiple_of=self.pad_target_to_multiple_of,
+            return_tensors="np",
+        )
+
+        # if bos token is appended in previous tokenization step,
+        # cut bos token here as it's append later anyways
+        labels = labels_batch["input_ids"]
+        if (labels[:, 0] == self.decoder_start_token_id).all().item():
+            labels = labels[:, 1:]
+            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]
+        
+        
+            
+        decoder_input_ids = shift_tokens_right(
+            labels, self.decoder_start_token_id)
+
+        # replace padding with -100 to ignore correctly when computing the loss
+        labels = np.ma.array(labels, mask=np.not_equal(
+            labels_batch.attention_mask, 1))
+        labels = labels.filled(fill_value=-100)
+
+        batch["labels"] = labels
+        batch["decoder_input_ids"] = decoder_input_ids
+        batch["attention_mask"] = labels_batch.attention_mask  # Add attention_mask to the batch
+        
+        return batch
+
+
+def load_maybe_streaming_dataset(dataset_name, dataset_config_name, split="train", streaming=True, **kwargs):
+    """
+    Utility function to load a dataset in streaming mode. For datasets with multiple splits,
+    each split is loaded individually and then splits combined by taking alternating examples from
+    each (interleaving).
+    """
+    if "+" in split:
+        # load multiple splits separated by the `+` symbol with streaming mode
+        dataset_splits = [
+            load_dataset(dataset_name, dataset_config_name,
+                         split=split_name, streaming=streaming, **kwargs)
+            for split_name in split.split("+")
+        ]
+        # interleave multiple splits to form one dataset
+        interleaved_dataset = interleave_datasets(dataset_splits)
+        return interleaved_dataset
+    else:
+        # load a single split *with* streaming mode
+        dataset = load_dataset(
+            dataset_name, dataset_config_name, split=split, streaming=streaming, **kwargs)
+        return dataset
+
+
+def collate_batch(samples):
+    return {key: [feature[key] for feature in samples] for key in samples[0]}
+
+def data_loader(
+    dataset: Dataset,
+    batch_size: int,
+    drop_last: bool=True,
+    num_workers: int=0,
+) -> Generator:
+    """
+    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,
+    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.
+    """
+    data_loader_iterator = iter(torch.utils.data.DataLoader(
+        batch_size=batch_size,
+        dataset=dataset.with_format("torch"),
+        num_workers=num_workers,
+        collate_fn=collate_batch,
+        drop_last=drop_last,
+    ))
+    return data_loader_iterator
+
+
+class TrainState(train_state.TrainState):
+    dropout_rng: jnp.ndarray
+
+    def replicate(self):
+        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))
+
+
+def create_learning_rate_fn(
+    num_train_steps: int, num_warmup_steps: int, learning_rate: float, start_step: int=0, warmup_init_value: float=0.0, decay_end_value: float=0.0,
+) -> Callable[[int], jnp.array]:
+    """Returns a linear warmup, linear_decay learning rate function."""
+    warmup_fn = optax.linear_schedule(
+        init_value=warmup_init_value, end_value=learning_rate, transition_steps=num_warmup_steps)
+    decay_fn = optax.linear_schedule(
+        init_value=learning_rate, end_value=decay_end_value, transition_steps=num_train_steps - num_warmup_steps
+    )
+    schedule_fn = optax.join_schedules(
+        schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])
+    
+    def learning_rate_fn(step: int) -> jnp.array:
+        return schedule_fn(step + start_step)
+    
+    return learning_rate_fn
+
+
+def main():
+    # Parse input arguments
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+    parser = HfArgumentParser(
+        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
+
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args = parser.parse_json_file(
+            json_file=os.path.abspath(sys.argv[1]))
+    else:
+        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
+    # information sent is the one passed as arguments along with your JAX/Flax versions.
+    send_example_telemetry("run_speech_recognition_seq2seq",
+                           model_args, data_args, framework="flax")
+
+    # Setup logging
+    # Make one log on every process with the configuration for debugging.
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+    # Set the verbosity to info of the Transformers logger.
+    # We only want one process per machine to log things on the screen.
+
+    # logger.setLevel(logging.INFO if jax.local_devices()[0].id%jax.local_device_count() == 0 else logging.ERROR)
+
+    # logger.setLevel(logging.INFO if jax.process_index()
+    #                == 0 else logging.ERROR)
+    
+    # Number of hosts
+    num_of_hosts = jax.process_count()
+    current_host_idx = jax.process_index()
+
+    if current_host_idx == 0:
+        datasets.utils.logging.set_verbosity_warning()
+        transformers.utils.logging.set_verbosity_info()
+    else:
+        datasets.utils.logging.set_verbosity_error()
+        transformers.utils.logging.set_verbosity_error()
+    
+    logger.setLevel(logging.INFO)
+    logger.info("Training/evaluation parameters %s", training_args)
+
+    if num_of_hosts and not training_args.push_to_hub:
+        logger.warning(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --push_to_hub to be able to save checkpoints to the hub."
+        )
+    if num_of_hosts and not training_args.overwrite_output_dir and training_args.resume_from_checkpoint:
+        logger.error(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --overwrite_output_dir to be able to resume from a pushed checkpoint."
+        )
+        sys.exit(1)
+
+    # Check the output dir is valid
+    if os.path.exists(training_args.output_dir):
+        if (
+            os.listdir(training_args.output_dir)
+            and training_args.do_train
+            and not training_args.overwrite_output_dir
+        ):
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use `--overwrite_output_dir` to overcome."
+            )
+        elif training_args.overwrite_output_dir:
+            logger.warning(f"Removing path {training_args.output_dir}")
+            shutil.rmtree(training_args.output_dir)
+      
+    # Handle the repository creation
+    output_dir = Path(training_args.output_dir)
+    if training_args.push_to_hub:
+        if training_args.hub_model_id is None:
+            repo_name = get_full_repo_name(
+                output_dir.absolute().name,
+                token=training_args.hub_token,
+                organization=training_args.push_to_hub_organization,
+            )
+        else:
+            repo_name = training_args.hub_model_id
+         
+        repo_url = None  
+        while not repo_url:
+            # Workaround for an internal HuggingFace error if the repo is being created by another worker
+            try:
+                repo_url = create_repo(
+                    repo_name, exist_ok=True, token=training_args.hub_token, private=training_args.hub_private_repo
+                )
+            except:
+                time.sleep(1)
+
+        repo = Repository(training_args.output_dir,
+                          clone_from=repo_name, token=training_args.hub_token)
+
+    # Set the model_name_or_path
+    model_name_or_path = model_args.model_name_or_path
+
+    # Try to detect last checkpoint and continue if possible
+    training_state = {"step": 0, "eval_lines": []}
+    if training_args.resume_from_checkpoint:
+        if (output_dir / "flax_model.msgpack").exists() and (output_dir / "training_state.bin").exists():
+            training_state = json.loads((output_dir / "training_state.bin").read_text())
+            model_name_or_path = os.path.join(training_args.output_dir)
+            logger.info(
+                f"Checkpoint detected, resuming training from {training_args.output_dir} at step {training_state['step']}."
+            )
+        else:
+            logger.info(
+                f"No valid checkpoint found in {training_args.output_dir}. Starting from {model_name_or_path}."
+            )
+    
+    
+    # Load dataset
+    raw_datasets = IterableDatasetDict() if data_args.streaming else DatasetDict()
+
+    if training_args.do_train:
+        raw_datasets["train"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.train_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if training_args.do_eval:
+        raw_datasets["eval"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.eval_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if not training_args.do_train and not training_args.do_eval:
+        raise ValueError(
+            "Cannot not train and not do evaluation. At least one of training or evaluation has to be performed."
+        )
+
+    raw_datasets_features = list(
+        next(iter(raw_datasets.values())).features.keys())
+
+    if data_args.audio_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--audio_column_name` to the correct audio column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    if data_args.text_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--text_column_name` to the correct text column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    # Load pretrained model, tokenizer, and feature extractor
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    feature_extractor = AutoFeatureExtractor.from_pretrained(
+        model_args.feature_extractor_name if model_args.feature_extractor_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(
+        model_name_or_path,
+        config=config,
+        dtype=getattr(jnp, model_args.dtype),
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    logger.info(
+        f"Successfully loaded the model '{model_name_or_path}'."
+    )
+    
+    if model.config.decoder_start_token_id is None:
+        raise ValueError(
+            "Make sure that `config.decoder_start_token_id` is correctly defined")
+
+    # Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,
+    # so we just need to set the correct target sampling rate.
+    dataset_sampling_rate = next(
+        iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
+
+    if dataset_sampling_rate != feature_extractor.sampling_rate:
+        raw_datasets = raw_datasets.cast_column(
+            data_args.audio_column_name, datasets.features.Audio(
+                sampling_rate=feature_extractor.sampling_rate)
+        )
+
+    # Preprocessing the datasets.
+    # We need to read the audio files as arrays and tokenize the targets.
+    max_input_length = int(
+        data_args.max_duration_in_seconds * feature_extractor.sampling_rate)
+    min_input_length = int(
+        data_args.min_duration_in_seconds * feature_extractor.sampling_rate)
+    max_label_length = (
+        data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length
+    )
+    pad_input_to_multiple_of = data_args.pad_input_to_multiple_of
+    pad_target_to_multiple_of = data_args.pad_target_to_multiple_of
+    audio_column_name = data_args.audio_column_name
+    num_workers = data_args.preprocessing_num_workers
+    text_column_name = data_args.text_column_name
+    model_input_name = feature_extractor.model_input_names[0]
+    do_lower_case = data_args.do_lower_case
+    do_remove_punctuation = data_args.do_remove_punctuation
+    normalizer = BasicTextNormalizer()  # 'official' text normalizer from OpenAI
+
+    if data_args.language is not None:
+        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)
+        tokenizer.set_prefix_tokens(
+            language=data_args.language, task=data_args.task)
+    
+    
+    def prepare_dataset(batch):
+        # Process audio
+        sample = batch[audio_column_name]
+        inputs = feature_extractor(
+            sample["array"], sampling_rate=sample["sampling_rate"])
+        # Process audio length
+        batch[model_input_name] = inputs.get(model_input_name)[0]
+        batch["input_length"] = len(sample["array"])
+
+        # Process targets
+        input_str = batch[text_column_name].lower(
+        ) if do_lower_case else batch[text_column_name]
+        if do_remove_punctuation:
+            input_str = normalizer(input_str).strip()
+        batch["labels"] = tokenizer(input_str,truncation=True, max_length=max_label_length).input_ids
+        return batch
+
+    with training_args.main_process_first(desc="dataset map pre-processing"):
+        vectorized_datasets = raw_datasets.map(
+            prepare_dataset,
+            remove_columns=raw_datasets_features,
+        )
+
+    # Filter training data with inputs longer than max_input_length
+    def is_audio_in_length_range(length):
+        return min_input_length < length < max_input_length
+
+    if training_args.do_train:
+        vectorized_datasets["train"] = vectorized_datasets["train"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    if training_args.do_eval:
+        vectorized_datasets["eval"] = vectorized_datasets["eval"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    # Load metrics and write stats
+    metric_wer = evaluate.load("wer")
+    metric_cer = evaluate.load("cer")
+    do_normalize_eval = data_args.do_normalize_eval
+
+    def compute_metrics(pred_ids, label_ids, return_preds_labels=False):
+        # Replace padded labels by the padding token
+        for idx in range(len(label_ids)):
+            label_ids[idx][label_ids[idx] == -100] = tokenizer.pad_token_id
+
+        predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
+        # We do not want to group tokens when computing the metrics
+        labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
+
+        if do_normalize_eval:
+            pred_str = [normalizer(pred) for pred in predictions]
+            label_str = [normalizer(label) for label in labels]
+            # Filtering step to only evaluate the samples that correspond to non-zero references:
+            pred_str = [pred_str[i]
+                        for i in range(len(pred_str)) if len(label_str[i]) > 0]
+            label_str = [label_str[i]
+                         for i in range(len(label_str)) if len(label_str[i]) > 0]
+        else:
+            pred_str = predictions
+            label_str = labels
+
+        wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)
+        cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)
+
+        if return_preds_labels:
+            return {"wer": wer, "cer": cer}, predictions, labels
+        else:
+            return {"wer": wer, "cer": cer}
+
+    def update_training_state(training_state, train_metrics, eval_metrics, step):
+        safe_value = lambda x: float(x.tolist() if isinstance(x, jnp.ndarray) else x)
+        state = {"step": step}
+        eval_lines = training_state["eval_lines"]
+       
+        train_metrics = get_metrics(train_metrics)
+        train_metrics_dict = {}
+        for metric_name, values in train_metrics.items():
+            tag = f"train_{metric_name}"
+            for i, value in enumerate(values):
+                train_metrics_dict[step - len(values) + i + 1] = {tag: safe_value(value)}
+
+        eval_metrics_dict = {}
+        for metric_name, value in eval_metrics.items():
+            tag = f"eval_{metric_name}"
+            eval_metrics_dict.update({
+                "step": step,
+                tag: safe_value(value),
+            })
+            if step in train_metrics_dict:
+                eval_metrics_dict.update(train_metrics_dict[step])
+        eval_lines.append(eval_metrics_dict)
+        return {**state, "eval_lines": eval_lines}
+
+    def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=None, labels=None):
+        summary_writer.scalar("train_time", train_time, step)
+
+        train_metrics = get_metrics(train_metrics)
+        for key, vals in train_metrics.items():
+            tag = f"train_{key}"
+            for i, val in enumerate(vals):
+                summary_writer.scalar(tag, val, step - len(vals) + i + 1)
+
+        for metric_name, value in eval_metrics.items():
+            summary_writer.scalar(f"eval_{metric_name}", value, step)
+        
+        # Log evaluation predictions
+        if predictions and labels:
+            df = pd.DataFrame({
+                "references": labels,
+                "predictions": predictions,
+            })
+            df["wer"] = df.apply(lambda row: metric_wer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            df["cer"] = df.apply(lambda row: metric_cer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            markdown_table = df.to_markdown(index=False)
+            eval_metrics_table = pd.DataFrame.from_dict([{"step": step, **eval_metrics}]).to_markdown(index=False)
+            summary_writer.text("eval_predictions", eval_metrics_table + "\n\n" + markdown_table, step)
+            # External logging function
+            if data_args.log_eval_predictions_fn:
+                module, fname = data_args.log_eval_predictions_fn.rsplit('.', 1)
+                fn = getattr(import_module(module), fname)
+                fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=predictions, labels=labels, training_args=training_args)
+
+    # Save feature extractor, tokenizer and config
+    feature_extractor.save_pretrained(training_args.output_dir)
+    tokenizer.save_pretrained(training_args.output_dir)
+    config.save_pretrained(training_args.output_dir)
+
+    processor = AutoProcessor.from_pretrained(training_args.output_dir)
+
+    data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(
+        processor=processor,
+        decoder_start_token_id=model.config.decoder_start_token_id,
+        input_padding="longest",
+        target_padding="longest",
+        max_target_length=max_label_length,
+        pad_input_to_multiple_of=pad_input_to_multiple_of,
+        pad_target_to_multiple_of=pad_target_to_multiple_of if pad_target_to_multiple_of else max_label_length,
+    )
+
+    # Enable tensorboard only on the master node
+    has_tensorboard = is_tensorboard_available()
+    if has_tensorboard and current_host_idx == 0:
+        try:
+            # TODO: Decouple wandb from tensorboard
+            import wandb
+
+            has_wandb = True
+        except ImportError:
+            has_wandb = False
+            if data_args.wandb_entity is not None or data_args.wandb_project is not None:
+                logger.warning(
+                    f"Unable to display metrics through Weights & Biases because some packages are not installed: {ie}"
+                )
+        try:
+            if has_wandb:
+                wandb.tensorboard.patch(root_logdir=output_dir / "runs")
+                wandb.init(
+                    entity=data_args.wandb_entity,
+                    project=data_args.wandb_project,
+                    name=training_args.run_name,
+                    notes=data_args.run_description,
+                    save_code=True,
+                    sync_tensorboard=True,
+                )
+                wandb.config.update(training_args)
+                wandb.config.update(model_args)
+                wandb.config.update(data_args)
+            from flax.metrics.tensorboard import SummaryWriter
+
+            summary_writer = SummaryWriter(
+                log_dir=output_dir / "runs" / f"{datetime.now():%b%d_%H-%M-%S}_{socket.gethostname()}")
+        except ImportError as ie:
+            has_tensorboard = False
+            logger.warning(
+                f"Unable to display metrics through TensorBoard because some packages are not installed: {ie}"
+            )
+    else:
+        logger.warning(
+            "Unable to display metrics through TensorBoard because the package is not installed: "
+            "Please run pip install tensorboard to enable."
+        )
+
+    # Initialize our training
+    rng = jax.random.PRNGKey(training_args.seed)
+    rng, dropout_rng = jax.random.split(rng)
+
+    # Store some constant
+    train_batch_size = int(
+        training_args.per_device_train_batch_size) * jax.device_count()
+    eval_batch_size = int(
+        training_args.per_device_eval_batch_size) * jax.device_count()
+
+    # Create learning rate schedule
+    lr_scheduler_types = {"linear", "constant", "constant_with_warmup"}
+    if training_args.lr_scheduler_type not in lr_scheduler_types:
+        raise ValueError(
+            f"lr_scheduler_type of type {training_args.lr_scheduler_type} not supported, choose from {lr_scheduler_types}."
+        )
+    elif training_args.lr_scheduler_type == "constant":
+        warmup_init_value = training_args.learning_rate
+        decay_end_value = training_args.learning_rate
+    elif training_args.lr_scheduler_type == "constant_with_warmup":
+        warmup_init_value = 0.0
+        decay_end_value = training_args.learning_rate
+    else:
+        warmup_init_value = 0.0
+        decay_end_value = 0.0
+        
+    linear_decay_lr_schedule_fn = create_learning_rate_fn(
+        data_args.num_train_steps,
+        training_args.warmup_steps,
+        training_args.learning_rate,
+        start_step=training_state["step"],
+        warmup_init_value=warmup_init_value,
+        decay_end_value=decay_end_value
+    )
+    
+    # We use Optax's "masking" functionality to not apply weight decay
+    # to bias and LayerNorm scale parameters. decay_mask_fn returns a
+    # mask boolean with the same structure as the parameters.
+    # The mask is True for parameters that should be decayed.
+    def decay_mask_fn(params):
+        flat_params = traverse_util.flatten_dict(params)
+        # Find out all LayerNorm parameters
+        layer_norm_candidates = ["layernorm", "layer_norm", "ln"]
+        layer_norm_named_params = set(
+            [
+                layer[-2:]
+                for layer_norm_name in layer_norm_candidates
+                for layer in flat_params.keys()
+                if layer_norm_name in "".join(layer).lower()
+            ]
+        )
+        flat_mask = {path: (path[-1] != "bias" and path[-2:]
+                            not in layer_norm_named_params) for path in flat_params}
+        return traverse_util.unflatten_dict(flat_mask)
+    
+    # Create adam optimizer
+    adamw = optax.adamw(
+        learning_rate=linear_decay_lr_schedule_fn,
+        b1=training_args.adam_beta1,
+        b2=training_args.adam_beta2,
+        eps=training_args.adam_epsilon,
+        weight_decay=training_args.weight_decay,
+        mask=decay_mask_fn,
+    )
+
+    # Setup train state
+    state = TrainState.create(
+        apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)
+
+    # Label smoothed cross entropy
+    def loss_fn(logits, labels, label_smoothing_factor=0.0):
+        """
+        The label smoothing implementation is adapted from Flax's official example:
+        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104
+        """
+        vocab_size = logits.shape[-1]
+        confidence = 1.0 - label_smoothing_factor
+        low_confidence = (1.0 - confidence) / (vocab_size - 1)
+        normalizing_constant = -(
+            confidence * jnp.log(confidence) + (vocab_size - 1) *
+            low_confidence * jnp.log(low_confidence + 1e-20)
+        )
+        soft_labels = onehot(labels, vocab_size,
+                             on_value=confidence, off_value=low_confidence)
+
+        loss = optax.softmax_cross_entropy(logits, soft_labels)
+        loss = loss - normalizing_constant
+
+        # Ignore padded tokens from loss, i.e. where labels are not set to -100
+        padding_mask = labels >= 0
+        loss = loss * padding_mask
+        loss = loss.sum()
+        num_labels = padding_mask.sum()
+        return loss, num_labels
+
+    # Define gradient update step fn
+    def train_step(state, batch, label_smoothing_factor=0.0):
+        
+        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)
+
+        def compute_loss(params):
+            labels = batch.pop("labels")
+            logits = state.apply_fn(
+                **batch, params=params, dropout_rng=dropout_rng, train=True)[0]
+            loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+            return loss, num_labels
+
+        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)
+        (loss, num_labels), grad = grad_fn(state.params)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        # True grad = total grad / total samples
+        grad = jax.lax.psum(grad, "batch")
+        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)
+        new_state = state.apply_gradients(
+            grads=grad, dropout_rng=new_dropout_rng)
+
+        metrics = {"loss": loss,
+                   "learning_rate": linear_decay_lr_schedule_fn(state.step)}
+
+        return new_state, metrics
+
+    # Define eval fn
+    def eval_step(params, batch, label_smoothing_factor=0.0):
+        labels = batch.pop("labels")
+        logits = model(**batch, params=params, train=False)[0]
+
+        loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        metrics = {"loss": loss}
+        return metrics
+
+    # Define generation function
+    num_beams = model_args.num_beams if model_args.num_beams is not None else model.config.num_beams
+    gen_kwargs = {"max_length": max_label_length, "num_beams": num_beams}
+
+     
+    def generate_step(params, batch):
+        model.params = params
+        
+        attention_mask = batch.get("attention_mask")
+        
+        #if attention_mask is not None:
+        output_ids = model.generate(batch[model_input_name], attention_mask=attention_mask, **gen_kwargs)
+        #else:
+        #    output_ids = model.generate(batch[model_input_name], **gen_kwargs)
+        
+        return output_ids.sequences
+
+    # Create parallel version of the train and eval step
+    p_train_step = jax.pmap(
+        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch", donate_argnums=(0, )
+    )
+    p_eval_step = jax.pmap(partial(
+        eval_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch")
+    p_generate_step = jax.pmap(generate_step, "batch")
+
+    # Replicate the train state on each device
+    state = state.replicate()
+    
+    # Logging
+    logger.info("***** Running training *****")
+    logger.info(
+        f"  Dataset name = {data_args.dataset_name}")
+    logger.info(
+        f"  Dataset config name = {data_args.dataset_config_name}")
+    logger.info(
+        f"  Learning rate = {training_args.learning_rate}")
+    logger.info(
+        f"  Scheduler = {training_args.lr_scheduler_type}")
+    logger.info(
+        f"  Num examples = {data_args.num_train_steps * train_batch_size}")
+    if num_of_hosts > 1:
+        logger.info(
+            f"  Number of hosts = {num_of_hosts}")
+        logger.info(
+            f"  Current host idx = {current_host_idx}")
+    logger.info(
+        f"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}")
+    logger.info(
+        f"  Total train batch size per node (w. parallel & distributed) = {train_batch_size // num_of_hosts}")
+    logger.info(
+        f"  Total train batch size (w. parallel & distributed) = {train_batch_size}")
+    logger.info(f"  Total optimization steps = {data_args.num_train_steps - training_state['step']}")
+    if training_state['step'] > 0:
+        logger.info(f"  ↪ Starting at {str(training_state['step'])} and finishing at {str(data_args.num_train_steps)}")
+
+    train_time = 0
+
+    # Training summary
+    language_code = None  # Maybe 'multilingual'?
+    if data_args.language is not None:
+        language = data_args.language.lower()
+        if language in TO_LANGUAGE_CODE:
+            language_code = TO_LANGUAGE_CODE[language]
+        elif len(language) == 2:
+            language_code = language
+    training_summary = {
+        "model_name": repo_name.split("/")[-1],
+        "language": language_code,
+        "tags": ["audio", "asr", "automatic-speech-recognition", "hf-asr-leaderboard"],
+        "license": "apache-2.0",
+        "finetuned_from": model_args.model_name_or_path,
+        "tasks": ["asr"],
+        "dataset": data_args.dataset_name,
+        "dataset_args": {"name": data_args.dataset_config_name},
+        "source": "flax",
+        "eval_lines": [],
+        "eval_results": None,
+        "hyperparameters": {
+            "learning_rate": training_args.learning_rate,
+            "lr_scheduler_type": training_args.lr_scheduler_type,
+            "per_device_train_batch_size": training_args.per_device_train_batch_size,
+            "total_train_batch_size_per_node": train_batch_size // num_of_hosts,
+            "total_train_batch_size": train_batch_size,
+            "total_optimization_steps": data_args.num_train_steps - training_state['step'],
+            "starting_optimization_step": training_state['step'] if training_state['step'] > 0 else None,
+            "finishing_optimization_step": data_args.num_train_steps,
+            "num_train_dataset_workers": f"{num_workers}",
+            "total_num_training_examples": data_args.num_train_steps * train_batch_size,
+        },
+        # TODO: Adapt https://github.com/huggingface/transformers/blob/main/src/transformers/modelcard.py#L855
+        # "hyperparameters": training_args.to_sanitized_dict()
+    }
+    
+    # Create README if it does not exist
+    readme = output_dir / "README.md"
+    if not readme.exists():
+        readme.write_text(TrainingSummary(**training_summary).to_model_card())
+    
+    # ======================== Training ================================
+    train_start = time.time()
+
+    train_metrics = []
+    epoch = 0
+    train_dataset = vectorized_datasets["train"].shuffle(seed=training_args.seed, buffer_size=data_args.shuffle_buffer_size)
+    
+    # Split by node
+    train_dataset = split_dataset_by_node(train_dataset, rank=current_host_idx, world_size=num_of_hosts)   
+    
+    if train_dataset.n_shards < data_args.preprocessing_num_workers:
+        num_workers = train_dataset.n_shards
+
+    logger.info(f"  Number of train dataset workers = {num_workers} {'(Capped by the number of dataset shards)' if train_dataset.n_shards < data_args.preprocessing_num_workers else ''} {'(ADVICE: In most cases you will speed up training considerably if you increase the value of --preprocessing_num_workers!)' if num_workers < 10 else ''}")
+ 
+    eval_dataset = vectorized_datasets["eval"]
+    train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+    
+    if not training_args.ignore_data_skip and training_state["step"] > 0:
+        logger.info(
+            f"  Will skip the first {training_state['step']} steps. If this takes a lot of time,"
+            " you can add the `--ignore_data_skip` flag to your launch command, but you will resume the"
+            " training on data already seen by your model."
+        )
+        for step in tqdm(range(training_state["step"]), desc=f"Skipping data for {training_state['step']} steps...", position=1, leave=False):
+            try:
+                samples = next(train_loader)
+            except StopIteration:
+                epoch += 1
+                train_dataset.set_epoch(epoch)
+                train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+                samples = next(train_loader)
+            batch = data_collator(samples)
+            # batch = shard(batch.data)
+    
+    
+    for step in tqdm(range(data_args.num_train_steps), desc="Training...", position=1, leave=False):
+        # Skip initial steps if these are specified. 
+        if step < training_state["step"]:
+            continue
+        
+        # =========================== Training ===========================
+        try:
+            samples = next(train_loader)
+        except StopIteration:
+            epoch += 1
+            train_dataset.set_epoch(epoch)
+            train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+            samples = next(train_loader)
+            logger.info(
+                f"Completed epoch ({epoch} | Loss: {train_metric['loss']}, Learning Rate:"
+                f" {train_metric['learning_rate']})"
+            )
+        
+        batch = data_collator(samples)
+        batch = shard(batch.data)
+        state, train_metric = p_train_step(state, batch)
+        train_metrics.append(train_metric)
+                
+        train_time += time.time() - train_start
+        train_metric = unreplicate(train_metric)
+
+        # ========================== Evaluating ==========================
+        # Evaluate at each eval_steps, and at the end of training at num_train_steps
+        if step % training_args.eval_steps == 0 or step == data_args.num_train_steps - 1:
+            logger.info(
+                f"Starting evaluation at step {step} of num_training_step {data_args.num_train_steps} steps. Planned evaluation every {training_args.eval_steps} steps." 
+            )
+            eval_metrics = []
+            eval_preds = []
+            eval_labels = []
+            eval_loader = data_loader(eval_dataset, eval_batch_size, drop_last=False)
+            if data_args.max_eval_samples:
+                max_eval_steps_iter = range(1 + data_args.max_eval_samples // eval_batch_size)
+            else:
+                max_eval_steps_iter = itertools.repeat(None)
+            for _ in tqdm(max_eval_steps_iter, desc="Evaluating...", position=2, leave=False):
+                # Model forward
+                try:
+                    samples = next(eval_loader)
+                except StopIteration:
+                    break
+                batch = data_collator(samples)
+                
+                labels = batch["labels"]
+
+                metrics = pad_shard_unpad(p_eval_step, static_return=True)(
+                    state.params, batch.data, min_device_batch=training_args.per_device_eval_batch_size
+                )
+                eval_metrics.append(metrics)
+
+                # Generation
+                if training_args.predict_with_generate:
+                    generated_ids = pad_shard_unpad(
+                        p_generate_step)(state.params, batch.data)
+                    eval_preds.extend(jax.device_get(
+                        generated_ids.reshape(-1, gen_kwargs["max_length"])))
+                    eval_labels.extend(labels)
+
+            # Normalize eval metrics
+            eval_metrics = get_metrics(eval_metrics)
+            eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)
+
+            # Compute metrics
+            metric_desc = ""
+            if training_args.predict_with_generate:
+                metric_values, pred_str, label_str = compute_metrics(
+                    eval_preds, eval_labels, return_preds_labels=True
+                )
+                eval_metrics.update(metric_values)
+                metric_desc = " | ".join(
+                    [f"Eval {key}: {value}" for key, value in metric_values.items()])
+
+            # Print metrics
+            desc = f"Step: {step} | Epoch: {epoch} (Eval Loss: {eval_metrics['loss']} | {metric_desc})"
+            logger.info(desc)
+
+            # Update training state
+            training_state = update_training_state(
+                training_state,
+                train_metrics,
+                eval_metrics,
+                step,
+            )
+
+            # Save metrics
+            if has_tensorboard and current_host_idx == 0:
+                log_max_predictions = data_args.log_max_eval_predictions if data_args.log_max_eval_predictions else 0
+                write_metric(
+                    summary_writer,
+                    train_metrics,
+                    eval_metrics,
+                    train_time,
+                    step,
+                    predictions=pred_str[:log_max_predictions],
+                    labels=label_str[:log_max_predictions]
+                )
+
+            # Save checkpoint at each eval_steps and push checkpoint to the hub
+            if current_host_idx  == 0:
+                params = jax.device_get(
+                    jax.tree_util.tree_map(lambda x: x[0], state.params))
+                model.save_pretrained(training_args.output_dir, params=params)
+                tokenizer.save_pretrained(training_args.output_dir)
+                # Report eval results if training is done
+                if step == data_args.num_train_steps - 1:
+                    training_summary["eval_results"] = training_state["eval_lines"][-1]
+                else:
+                    training_summary.update({"eval_lines": training_state["eval_lines"]})
+                (output_dir / "training_state.bin").write_text(json.dumps(training_state))
+                # Write model card
+                readme.write_text(TrainingSummary(**training_summary).to_model_card())
+                if training_args.push_to_hub:
+                    repo.push_to_hub(
+                        commit_message=f"Saving weights and logs of step {step} - epoch {epoch}", blocking=False)
+
+if __name__ == "__main__":
+    main()
diff --git a/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/config.yaml b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/config.yaml
new file mode 100644
index 0000000..3a0e7ad
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/config.yaml
@@ -0,0 +1,512 @@
+wandb_version: 1
+
+__cached__setup_devices:
+  desc: null
+  value: cpu
+_n_gpu:
+  desc: null
+  value: 0
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.14.0
+    code_path: code/run_flax_speech_recognition_seq2seq_streaming_debug.py
+    framework: huggingface
+    huggingface_version: 4.28.0.dev0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.10
+    start_time: 1681319517.103434
+    t:
+      1:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      2:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      3:
+      - 13
+      - 23
+      - 34
+      4: 3.8.10
+      5: 0.14.0
+      6: 4.28.0.dev0
+      8:
+      - 5
+adafactor:
+  desc: null
+  value: false
+adam_beta1:
+  desc: null
+  value: 0.9
+adam_beta2:
+  desc: null
+  value: 0.999
+adam_epsilon:
+  desc: null
+  value: 1.0e-08
+audio_column_name:
+  desc: null
+  value: audio
+auto_find_batch_size:
+  desc: null
+  value: false
+bf16:
+  desc: null
+  value: false
+bf16_full_eval:
+  desc: null
+  value: false
+cache_dir:
+  desc: null
+  value: null
+config_name:
+  desc: null
+  value: null
+data_seed:
+  desc: null
+  value: null
+dataloader_drop_last:
+  desc: null
+  value: false
+dataloader_num_workers:
+  desc: null
+  value: 0
+dataloader_pin_memory:
+  desc: null
+  value: true
+dataset_cache_dir:
+  desc: null
+  value: null
+dataset_config_name:
+  desc: null
+  value: null
+dataset_name:
+  desc: null
+  value: NbAiLab/NCC_speech_all_v5
+ddp_bucket_cap_mb:
+  desc: null
+  value: null
+ddp_find_unused_parameters:
+  desc: null
+  value: null
+ddp_timeout:
+  desc: null
+  value: 1800
+debug:
+  desc: null
+  value: []
+deepspeed:
+  desc: null
+  value: null
+disable_tqdm:
+  desc: null
+  value: false
+do_eval:
+  desc: null
+  value: true
+do_lower_case:
+  desc: null
+  value: false
+do_normalize_eval:
+  desc: null
+  value: true
+do_predict:
+  desc: null
+  value: false
+do_remove_punctuation:
+  desc: null
+  value: false
+do_train:
+  desc: null
+  value: true
+dtype:
+  desc: null
+  value: bfloat16
+eval_accumulation_steps:
+  desc: null
+  value: null
+eval_delay:
+  desc: null
+  value: 0
+eval_split_name:
+  desc: null
+  value: validation
+eval_steps:
+  desc: null
+  value: 10000
+evaluation_strategy:
+  desc: null
+  value: IntervalStrategy.NO
+feature_extractor_name:
+  desc: null
+  value: null
+fp16:
+  desc: null
+  value: false
+fp16_backend:
+  desc: null
+  value: auto
+fp16_full_eval:
+  desc: null
+  value: false
+fp16_opt_level:
+  desc: null
+  value: O1
+fsdp:
+  desc: null
+  value: []
+fsdp_config:
+  desc: null
+  value:
+    fsdp_min_num_params: 0
+    xla: false
+    xla_fsdp_grad_ckpt: false
+fsdp_min_num_params:
+  desc: null
+  value: 0
+fsdp_transformer_layer_cls_to_wrap:
+  desc: null
+  value: null
+full_determinism:
+  desc: null
+  value: false
+generation_config:
+  desc: null
+  value: null
+generation_max_length:
+  desc: null
+  value: null
+generation_num_beams:
+  desc: null
+  value: null
+gradient_accumulation_steps:
+  desc: null
+  value: 1
+gradient_checkpointing:
+  desc: null
+  value: false
+greater_is_better:
+  desc: null
+  value: null
+group_by_length:
+  desc: null
+  value: false
+half_precision_backend:
+  desc: null
+  value: auto
+hub_model_id:
+  desc: null
+  value: NbAiLab/scream_large_oct_debug_128seq
+hub_private_repo:
+  desc: null
+  value: true
+hub_strategy:
+  desc: null
+  value: HubStrategy.EVERY_SAVE
+hub_token:
+  desc: null
+  value: null
+ignore_data_skip:
+  desc: null
+  value: true
+include_inputs_for_metrics:
+  desc: null
+  value: false
+jit_mode_eval:
+  desc: null
+  value: false
+label_names:
+  desc: null
+  value: null
+label_smoothing_factor:
+  desc: null
+  value: 0.0
+language:
+  desc: null
+  value: Norwegian
+learning_rate:
+  desc: null
+  value: 5.0e-06
+length_column_name:
+  desc: null
+  value: length
+load_best_model_at_end:
+  desc: null
+  value: false
+local_rank:
+  desc: null
+  value: -1
+log_eval_predictions_fn:
+  desc: null
+  value: log_predictions.write_predictions
+log_level:
+  desc: null
+  value: passive
+log_level_replica:
+  desc: null
+  value: warning
+log_max_eval_predictions:
+  desc: null
+  value: 100
+log_on_each_node:
+  desc: null
+  value: true
+logging_dir:
+  desc: null
+  value: ../../scream_large_oct_debug_128seq/runs/Apr12_17-11-08_t1v-n-0a06f6ef-w-0
+logging_first_step:
+  desc: null
+  value: false
+logging_nan_inf_filter:
+  desc: null
+  value: true
+logging_steps:
+  desc: null
+  value: 500
+logging_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+lr_scheduler_type:
+  desc: null
+  value: SchedulerType.LINEAR
+max_duration_in_seconds:
+  desc: null
+  value: 30.0
+max_eval_samples:
+  desc: null
+  value: null
+max_grad_norm:
+  desc: null
+  value: 1.0
+max_label_length:
+  desc: null
+  value: 128.0
+max_steps:
+  desc: null
+  value: -1
+max_train_samples:
+  desc: null
+  value: null
+metric_for_best_model:
+  desc: null
+  value: null
+min_duration_in_seconds:
+  desc: null
+  value: 0.0
+model_name_or_path:
+  desc: null
+  value: openai/whisper-large-v2
+model_revision:
+  desc: null
+  value: main
+mp_parameters:
+  desc: null
+  value: ''
+no_cuda:
+  desc: null
+  value: false
+num_beams:
+  desc: null
+  value: 5
+num_train_epochs:
+  desc: null
+  value: 3.0
+num_train_steps:
+  desc: null
+  value: 50000
+optim:
+  desc: null
+  value: OptimizerNames.ADAMW_HF
+optim_args:
+  desc: null
+  value: null
+output_dir:
+  desc: null
+  value: ../../scream_large_oct_debug_128seq
+overwrite_cache:
+  desc: null
+  value: false
+overwrite_output_dir:
+  desc: null
+  value: true
+pad_input_to_multiple_of:
+  desc: null
+  value: null
+pad_target_to_multiple_of:
+  desc: null
+  value: null
+past_index:
+  desc: null
+  value: -1
+per_device_eval_batch_size:
+  desc: null
+  value: 8
+per_device_train_batch_size:
+  desc: null
+  value: 8
+per_gpu_eval_batch_size:
+  desc: null
+  value: null
+per_gpu_train_batch_size:
+  desc: null
+  value: null
+predict_with_generate:
+  desc: null
+  value: true
+prediction_loss_only:
+  desc: null
+  value: false
+preprocessing_num_workers:
+  desc: null
+  value: 32
+push_to_hub:
+  desc: null
+  value: true
+push_to_hub_model_id:
+  desc: null
+  value: null
+push_to_hub_organization:
+  desc: null
+  value: null
+push_to_hub_token:
+  desc: null
+  value: null
+ray_scope:
+  desc: null
+  value: last
+remove_unused_columns:
+  desc: null
+  value: true
+report_to:
+  desc: null
+  value:
+  - tensorboard
+  - wandb
+resume_from_checkpoint:
+  desc: null
+  value: 'True'
+run_description:
+  desc: null
+  value: A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear
+    decay on the all_v5-corpus.
+run_name:
+  desc: null
+  value: ScreamLarge - debug_beam5_long
+save_on_each_node:
+  desc: null
+  value: false
+save_steps:
+  desc: null
+  value: 500
+save_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+save_total_limit:
+  desc: null
+  value: null
+seed:
+  desc: null
+  value: 42
+sharded_ddp:
+  desc: null
+  value: []
+shuffle_buffer_size:
+  desc: null
+  value: 500
+skip_memory_metrics:
+  desc: null
+  value: true
+sortish_sampler:
+  desc: null
+  value: false
+streaming:
+  desc: null
+  value: true
+task:
+  desc: null
+  value: transcribe
+text_column:
+  desc: null
+  value: null
+text_column_name:
+  desc: null
+  value: text
+tf32:
+  desc: null
+  value: null
+tokenizer_name:
+  desc: null
+  value: null
+torch_compile:
+  desc: null
+  value: false
+torch_compile_backend:
+  desc: null
+  value: null
+torch_compile_mode:
+  desc: null
+  value: null
+torchdynamo:
+  desc: null
+  value: null
+tpu_metrics_debug:
+  desc: null
+  value: false
+tpu_num_cores:
+  desc: null
+  value: null
+train_split_name:
+  desc: null
+  value: train
+use_auth_token:
+  desc: null
+  value: true
+use_fast_tokenizer:
+  desc: null
+  value: true
+use_ipex:
+  desc: null
+  value: false
+use_legacy_prediction_loop:
+  desc: null
+  value: false
+use_mps_device:
+  desc: null
+  value: false
+wandb_entity:
+  desc: null
+  value: nbailab
+wandb_project:
+  desc: null
+  value: Scream - septimus
+warmup_ratio:
+  desc: null
+  value: 0.0
+warmup_steps:
+  desc: null
+  value: 10000
+weight_decay:
+  desc: null
+  value: 0.0
+xpu_backend:
+  desc: null
+  value: null
diff --git a/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/events.out.tfevents.1681319518.t1v-n-0a06f6ef-w-0.1593475.0.v2 b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/events.out.tfevents.1681319518.t1v-n-0a06f6ef-w-0.1593475.0.v2
new file mode 120000
index 0000000..3f8961d
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/events.out.tfevents.1681319518.t1v-n-0a06f6ef-w-0.1593475.0.v2
@@ -0,0 +1 @@
+/home/perk/models/scream_large_oct_debug_128seq/runs/Apr12_17-11-58_t1v-n-0a06f6ef-w-0/events.out.tfevents.1681319518.t1v-n-0a06f6ef-w-0.1593475.0.v2
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/requirements.txt b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/requirements.txt
new file mode 100644
index 0000000..34d1446
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/requirements.txt
@@ -0,0 +1,140 @@
+absl-py==1.4.0
+aiohttp==3.8.4
+aiosignal==1.3.1
+appdirs==1.4.4
+astunparse==1.6.3
+async-timeout==4.0.2
+attrs==22.2.0
+audioread==3.0.0
+cached-property==1.5.2
+cachetools==5.3.0
+certifi==2022.12.7
+cffi==1.15.1
+charset-normalizer==3.1.0
+chex==0.1.7
+click==8.1.3
+cmake==3.26.1
+datasets==2.11.0
+decorator==5.1.1
+dill==0.3.6
+dm-tree==0.1.8
+docker-pycreds==0.4.0
+etils==1.1.1
+evaluate==0.4.0
+filelock==3.10.7
+flatbuffers==23.3.3
+flax==0.6.8
+frozenlist==1.3.3
+fsspec==2023.3.0
+gast==0.4.0
+gitdb==4.0.10
+gitpython==3.1.31
+google-auth-oauthlib==1.0.0
+google-auth==2.17.1
+google-pasta==0.2.0
+grpcio==1.53.0
+h5py==3.8.0
+huggingface-hub==0.13.3
+idna==3.4
+importlib-metadata==6.1.0
+importlib-resources==5.12.0
+jax==0.4.8
+jaxlib==0.4.7
+jinja2==3.1.2
+jiwer==3.0.1
+joblib==1.2.0
+keras==2.12.0
+lazy-loader==0.2
+libclang==16.0.0
+librosa==0.10.0.post2
+libtpu-nightly==0.1.dev20230327
+lit==16.0.0
+llvmlite==0.39.1
+markdown-it-py==2.2.0
+markdown==3.4.3
+markupsafe==2.1.2
+mdurl==0.1.2
+ml-dtypes==0.0.4
+mpmath==1.3.0
+msgpack==1.0.5
+multidict==6.0.4
+multiprocess==0.70.14
+nest-asyncio==1.5.6
+networkx==3.0
+numba==0.56.4
+numpy==1.23.5
+nvidia-cublas-cu11==11.10.3.66
+nvidia-cuda-cupti-cu11==11.7.101
+nvidia-cuda-nvrtc-cu11==11.7.99
+nvidia-cuda-runtime-cu11==11.7.99
+nvidia-cudnn-cu11==8.5.0.96
+nvidia-cufft-cu11==10.9.0.58
+nvidia-curand-cu11==10.2.10.91
+nvidia-cusolver-cu11==11.4.0.1
+nvidia-cusparse-cu11==11.7.4.91
+nvidia-nccl-cu11==2.14.3
+nvidia-nvtx-cu11==11.7.91
+oauthlib==3.2.2
+opt-einsum==3.3.0
+optax==0.1.4
+orbax==0.1.7
+packaging==23.0
+pandas==1.5.3
+pathtools==0.1.2
+pip==23.0.1
+pkg-resources==0.0.0
+pooch==1.6.0
+protobuf==4.22.1
+psutil==5.9.4
+pyarrow==11.0.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pycparser==2.21
+pydub==0.25.1
+pygments==2.14.0
+python-dateutil==2.8.2
+pytz==2023.3
+pyyaml==6.0
+rapidfuzz==2.13.7
+regex==2023.3.23
+requests-oauthlib==1.3.1
+requests==2.28.2
+responses==0.18.0
+rich==13.3.3
+rsa==4.9
+scikit-learn==1.2.2
+scipy==1.10.1
+sentry-sdk==1.18.0
+setproctitle==1.3.2
+setuptools==44.0.0
+six==1.16.0
+smmap==5.0.0
+soundfile==0.12.1
+soxr==0.3.4
+sympy==1.11.1
+tabulate==0.9.0
+tensorboard-data-server==0.7.0
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.12.1
+tensorflow-estimator==2.12.0
+tensorflow-io-gcs-filesystem==0.32.0
+tensorflow==2.12.0
+tensorstore==0.1.35
+termcolor==2.2.0
+threadpoolctl==3.1.0
+tokenizers==0.13.2
+toolz==0.12.0
+torch==2.0.0
+torchaudio==2.0.1
+tqdm==4.65.0
+transformers==4.28.0.dev0
+triton==2.0.0
+typing-extensions==4.5.0
+urllib3==1.26.15
+wandb==0.14.0
+werkzeug==2.2.3
+wheel==0.40.0
+wrapt==1.14.1
+xxhash==3.2.0
+yarl==1.8.2
+zipp==3.15.0
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/wandb-metadata.json b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/wandb-metadata.json
new file mode 100644
index 0000000..dbdb7de
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/wandb-metadata.json
@@ -0,0 +1,1302 @@
+{
+    "os": "Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2023-04-12T17:11:58.126981",
+    "startedAt": "2023-04-12T17:11:57.094264",
+    "docker": null,
+    "cuda": null,
+    "args": [
+        "--model_name_or_path",
+        "openai/whisper-large-v2",
+        "--run_name",
+        "ScreamLarge - debug_beam5_long",
+        "--run_description",
+        "A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear decay on the all_v5-corpus.",
+        "--wandb_entity",
+        "nbailab",
+        "--wandb_project",
+        "Scream - septimus",
+        "--dataset_name",
+        "NbAiLab/NCC_speech_all_v5",
+        "--language",
+        "Norwegian",
+        "--text_column_name",
+        "text",
+        "--train_split_name",
+        "train",
+        "--eval_split_name",
+        "validation",
+        "--output_dir",
+        "../../scream_large_oct_debug_128seq",
+        "--overwrite_output_dir",
+        "--warmup_steps",
+        "10000",
+        "--do_train",
+        "--do_eval",
+        "--num_train_steps",
+        "50000",
+        "--lr_scheduler_type",
+        "linear",
+        "--eval_steps",
+        "10000",
+        "--learning_rate",
+        "5e-6",
+        "--preprocessing_num_workers",
+        "32",
+        "--per_device_train_batch_size",
+        "8",
+        "--per_device_eval_batch_size",
+        "8",
+        "--predict_with_generate",
+        "--log_max_eval_predictions",
+        "100",
+        "--log_eval_predictions_fn",
+        "log_predictions.write_predictions",
+        "--streaming",
+        "True",
+        "--use_auth_token",
+        "True",
+        "--dtype",
+        "bfloat16",
+        "--hub_private_repo",
+        "True",
+        "--hub_model_id",
+        "NbAiLab/scream_large_oct_debug_128seq",
+        "--resume_from_checkpoint",
+        "True",
+        "--num_beams",
+        "5",
+        "--ignore_data_skip",
+        "--max_label_length",
+        "128",
+        "--push_to_hub"
+    ],
+    "state": "running",
+    "program": "../run_flax_speech_recognition_seq2seq_streaming_debug.py",
+    "codePath": "run_flax_speech_recognition_seq2seq_streaming_debug.py",
+    "git": {
+        "remote": "https://github.com/NbAiLab/nb-whisper.git",
+        "commit": "c334729e58bccba553e14a54b59c0e2401ce0f86"
+    },
+    "email": "per@capia.no",
+    "root": "/home/perk/models/nb-whisper",
+    "host": "t1v-n-0a06f6ef-w-0",
+    "username": "perk",
+    "executable": "/home/perk/.whisper/bin/python",
+    "cpu_count": 120,
+    "cpu_count_logical": 240,
+    "cpu_freq": {
+        "current": 2249.9980000000096,
+        "min": 0.0,
+        "max": 0.0
+    },
+    "cpu_freq_per_core": [
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        }
+    ],
+    "disk": {
+        "total": 96.74600601196289,
+        "used": 33.38563537597656
+    },
+    "memory": {
+        "total": 400.47254943847656
+    }
+}
diff --git a/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/wandb-summary.json b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/wandb-summary.json
new file mode 100644
index 0000000..f205c24
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 32}}
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/run-x4ewuxwu.wandb b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/run-x4ewuxwu.wandb
new file mode 100644
index 0000000..7fb9c6b
Binary files /dev/null and b/experiment_scream_octavus/wandb/run-20230412_171157-x4ewuxwu/run-x4ewuxwu.wandb differ
diff --git a/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/code/run_flax_speech_recognition_seq2seq_streaming_debug.py b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/code/run_flax_speech_recognition_seq2seq_streaming_debug.py
new file mode 100644
index 0000000..782a40d
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/code/run_flax_speech_recognition_seq2seq_streaming_debug.py
@@ -0,0 +1,1318 @@
+#!/usr/bin/env python
+# coding=utf-8
+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR COND    ITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""
+Fine-tuning the Flax library models for sequence to sequence speech recognition.
+"""
+# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.
+
+import itertools
+import json
+import logging
+import os
+import shutil
+import socket
+import sys
+import tempfile
+import time
+from dataclasses import field
+from datetime import datetime
+from functools import partial
+from importlib import import_module
+from pathlib import Path
+from typing import Any, Callable, Dict, Generator, List, Optional, Union
+
+import flax
+import jax
+import jax.numpy as jnp 
+import numpy as np
+import optax
+import pandas as pd
+import torch
+# from jax.experimental.compilation_cache import compilation_cache; compilation_cache.initialize_cache(tempfile.gettempdir())
+from flax import jax_utils, traverse_util
+from flax.jax_utils import pad_shard_unpad, unreplicate
+from flax.training import train_state
+from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key
+from torch.utils.data import IterableDataset
+from tqdm import tqdm
+
+import datasets
+import evaluate
+import transformers
+from datasets import Dataset, DatasetDict, IterableDatasetDict, interleave_datasets, load_dataset
+from datasets.distributed import split_dataset_by_node
+from huggingface_hub import Repository, create_repo
+from transformers import (
+    AutoConfig,
+    AutoFeatureExtractor,
+    AutoProcessor,
+    AutoTokenizer,
+    FlaxAutoModelForSpeechSeq2Seq,
+    HfArgumentParser,
+    Seq2SeqTrainingArguments,
+    is_tensorboard_available,
+)
+from transformers.modelcard import TrainingSummary
+from transformers.models.whisper.english_normalizer import BasicTextNormalizer
+from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE
+from transformers.file_utils import get_full_repo_name
+from transformers.utils import check_min_version, send_example_telemetry
+from transformers.utils.versions import require_version
+
+from flax.training import checkpoints
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+check_min_version("4.27.0.dev0")
+
+require_version("datasets>=1.18.2",
+                "To fix: pip install -r examples/flax/speech-recogintion/requirements.txt")
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+logger = logging.getLogger(__name__)
+
+
+@flax.struct.dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={
+            "help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    feature_extractor_name: Optional[str] = field(
+        default=None, metadata={"help": "feature extractor name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Where to store the pretrained models downloaded from huggingface.co"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={
+            "help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+    dtype: Optional[str] = field(
+        default="float32",
+        metadata={
+            "help": (
+                "Floating-point format in which the model weights should be initialized and trained. Choose one of"
+                " `[float32, float16, bfloat16]`."
+            )
+        },
+    )
+    num_beams: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Number of beams to use for evaluation. This argument will be passed to `model.generate`, "
+                "which is used during evaluation."
+            )
+        },
+    )
+
+
+@flax.struct.dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    dataset_name: str = field(
+        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
+    )
+    dataset_config_name: Optional[str] = field(
+        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
+    )
+    text_column: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "The name of the column in the datasets containing the full texts (for summarization)."},
+    )
+    dataset_cache_dir: Optional[str] = field(
+        default=None, metadata={"help": "Path to cache directory for saving and loading datasets"}
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
+    )
+    preprocessing_num_workers: Optional[int] = field(
+        default=50,
+        metadata={"help": "The number of processes to use for the preprocessing."},
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    audio_column_name: str = field(
+        default="audio",
+        metadata={
+            "help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
+    )
+    text_column_name: str = field(
+        default="text",
+        metadata={
+            "help": "The name of the dataset column containing the text data. Defaults to 'text'"},
+    )
+    max_duration_in_seconds: float = field(
+        default=30.0,
+        metadata={
+            "help": "Filter audio files that are longer than `max_duration_in_seconds` seconds"},
+    )
+    min_duration_in_seconds: float = field(
+        default=0.0,
+        metadata={
+            "help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"},
+    )
+    max_label_length: Optional[int] = field(
+        default=256,
+        metadata={
+            "help": "Truncate transcriptions that are longer `max_label_length` tokens."},
+    )
+    pad_input_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the input sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the inputs to max length."
+        },
+    )
+    pad_target_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the target sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the targets to max length."
+        },
+    )
+    train_split_name: str = field(
+        default="train",
+        metadata={
+            "help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
+        },
+    )
+    eval_split_name: str = field(
+        default="validation",
+        metadata={
+            "help": "The name of the evaluation data set split to use (via the datasets library). Defaults to 'validation'"
+        },
+    )
+    do_lower_case: bool = field(
+        default=False,
+        metadata={"help": "Whether the target text should be lower cased."},
+    )
+    do_remove_punctuation: bool = field(
+        default=False,
+        metadata={
+            "help": "Whether the target text should be striped of punctuation."},
+    )
+    do_normalize_eval: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to normalise the references and predictions in the eval WER calculation."},
+    )
+    language: str = field(
+        default=None,
+        metadata={
+            "help": (
+                "Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning "
+                "only. For English speech recognition, it should be set to `None`."
+            )
+        },
+    )
+    task: str = field(
+        default="transcribe",
+        metadata={
+            "help": "Task, either `transcribe` for speech recognition or `translate` for speech translation."},
+    )
+    num_train_steps: int = field(default=50000, metadata={
+                                 "help": "The number of training steps."})
+    shuffle_buffer_size: Optional[int] = field(
+        default=500,
+        metadata={
+            "help": (
+                "The number of streamed examples to download before shuffling them. The large the buffer, "
+                "the closer it is to real offline shuffling."
+            )
+        },
+    )
+    streaming: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use streaming mode to load and pre-process the data."},
+    )
+    log_max_eval_predictions: Optional[int] = field(
+        default=0,
+        metadata={
+            "help": (
+                "Number of label and prediction pairs to write to the summary at each evaluation step."
+            )
+        },
+    )
+    log_eval_predictions_fn: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Python path to function for logging evaluation predictions. It can be an external function like fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions, labels)."
+            )
+        },
+    )
+    run_description: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "A longer description of the run/experiment."
+            )
+        },
+    )
+    wandb_entity: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases username or entity (organization name)."
+            )
+        },
+    )
+    wandb_project: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases project to log metrics to."
+            )
+        },
+    )
+
+
+def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:
+    """
+    Shift label ids one token to the right.
+    """
+    shifted_label_ids = np.zeros_like(label_ids)
+    shifted_label_ids[:, 1:] = label_ids[:, :-1]
+    shifted_label_ids[:, 0] = decoder_start_token_id
+
+    return shifted_label_ids
+
+
+@flax.struct.dataclass
+class FlaxDataCollatorSpeechSeq2SeqWithPadding:
+    """
+    Data collator that will dynamically pad the inputs received.
+    Args:
+        processor ([`Wav2Vec2Processor`])
+            The processor used for proccessing the data.
+        decoder_start_token_id (:obj: `int`)
+            The begin-of-sentence of the decoder.
+        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)
+            among:
+            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
+              sequence if provided).
+            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
+              maximum acceptable input length for the model if that argument is not provided.
+            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
+              different lengths).
+        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).
+            See above for details.
+        max_input_length (:obj:`float`, `optional`):
+            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
+        max_target_length (:obj:`int`, `optional`):
+            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).
+        pad_input_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the input sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+        pad_target_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the target sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+    """
+
+    processor: Any
+    decoder_start_token_id: int
+    input_padding: Union[bool, str] = "longest"
+    target_padding: Union[bool, str] = "max_length"
+    max_input_length: Optional[float] = None
+    max_target_length: Optional[int] = None
+    pad_input_to_multiple_of: Optional[int] = None
+    pad_target_to_multiple_of: Optional[int] = None
+
+    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:
+        model_input_name = self.processor.model_input_names[0]
+        input_features = {model_input_name: features[model_input_name]}
+        label_features = {"input_ids": features["labels"]}
+
+        # reformat list to dict and set to pytorch format
+        batch = self.processor.feature_extractor.pad(
+            input_features,
+            max_length=self.max_input_length,
+            padding=self.input_padding,
+            pad_to_multiple_of=self.pad_input_to_multiple_of,
+            return_tensors="np",
+        )
+
+        labels_batch = self.processor.tokenizer.pad(
+            label_features,
+            max_length=self.max_target_length,
+            padding=self.target_padding,
+            pad_to_multiple_of=self.pad_target_to_multiple_of,
+            return_tensors="np",
+        )
+
+        # if bos token is appended in previous tokenization step,
+        # cut bos token here as it's append later anyways
+        labels = labels_batch["input_ids"]
+        if (labels[:, 0] == self.decoder_start_token_id).all().item():
+            labels = labels[:, 1:]
+            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]
+        
+        
+            
+        decoder_input_ids = shift_tokens_right(
+            labels, self.decoder_start_token_id)
+
+        # replace padding with -100 to ignore correctly when computing the loss
+        labels = np.ma.array(labels, mask=np.not_equal(
+            labels_batch.attention_mask, 1))
+        labels = labels.filled(fill_value=-100)
+
+        batch["labels"] = labels
+        batch["decoder_input_ids"] = decoder_input_ids
+        batch["attention_mask"] = labels_batch.attention_mask  # Add attention_mask to the batch
+        
+        return batch
+
+
+def load_maybe_streaming_dataset(dataset_name, dataset_config_name, split="train", streaming=True, **kwargs):
+    """
+    Utility function to load a dataset in streaming mode. For datasets with multiple splits,
+    each split is loaded individually and then splits combined by taking alternating examples from
+    each (interleaving).
+    """
+    if "+" in split:
+        # load multiple splits separated by the `+` symbol with streaming mode
+        dataset_splits = [
+            load_dataset(dataset_name, dataset_config_name,
+                         split=split_name, streaming=streaming, **kwargs)
+            for split_name in split.split("+")
+        ]
+        # interleave multiple splits to form one dataset
+        interleaved_dataset = interleave_datasets(dataset_splits)
+        return interleaved_dataset
+    else:
+        # load a single split *with* streaming mode
+        dataset = load_dataset(
+            dataset_name, dataset_config_name, split=split, streaming=streaming, **kwargs)
+        return dataset
+
+
+def collate_batch(samples):
+    return {key: [feature[key] for feature in samples] for key in samples[0]}
+
+def data_loader(
+    dataset: Dataset,
+    batch_size: int,
+    drop_last: bool=True,
+    num_workers: int=0,
+) -> Generator:
+    """
+    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,
+    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.
+    """
+    data_loader_iterator = iter(torch.utils.data.DataLoader(
+        batch_size=batch_size,
+        dataset=dataset.with_format("torch"),
+        num_workers=num_workers,
+        collate_fn=collate_batch,
+        drop_last=drop_last,
+    ))
+    return data_loader_iterator
+
+
+class TrainState(train_state.TrainState):
+    dropout_rng: jnp.ndarray
+
+    def replicate(self):
+        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))
+
+
+def create_learning_rate_fn(
+    num_train_steps: int, num_warmup_steps: int, learning_rate: float, start_step: int=0, warmup_init_value: float=0.0, decay_end_value: float=0.0,
+) -> Callable[[int], jnp.array]:
+    """Returns a linear warmup, linear_decay learning rate function."""
+    warmup_fn = optax.linear_schedule(
+        init_value=warmup_init_value, end_value=learning_rate, transition_steps=num_warmup_steps)
+    decay_fn = optax.linear_schedule(
+        init_value=learning_rate, end_value=decay_end_value, transition_steps=num_train_steps - num_warmup_steps
+    )
+    schedule_fn = optax.join_schedules(
+        schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])
+    
+    def learning_rate_fn(step: int) -> jnp.array:
+        return schedule_fn(step + start_step)
+    
+    return learning_rate_fn
+
+
+def main():
+    # Parse input arguments
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+    parser = HfArgumentParser(
+        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
+
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args = parser.parse_json_file(
+            json_file=os.path.abspath(sys.argv[1]))
+    else:
+        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
+    # information sent is the one passed as arguments along with your JAX/Flax versions.
+    send_example_telemetry("run_speech_recognition_seq2seq",
+                           model_args, data_args, framework="flax")
+
+    # Setup logging
+    # Make one log on every process with the configuration for debugging.
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+    # Set the verbosity to info of the Transformers logger.
+    # We only want one process per machine to log things on the screen.
+
+    # logger.setLevel(logging.INFO if jax.local_devices()[0].id%jax.local_device_count() == 0 else logging.ERROR)
+
+    # logger.setLevel(logging.INFO if jax.process_index()
+    #                == 0 else logging.ERROR)
+    
+    # Number of hosts
+    num_of_hosts = jax.process_count()
+    current_host_idx = jax.process_index()
+
+    if current_host_idx == 0:
+        datasets.utils.logging.set_verbosity_warning()
+        transformers.utils.logging.set_verbosity_info()
+    else:
+        datasets.utils.logging.set_verbosity_error()
+        transformers.utils.logging.set_verbosity_error()
+    
+    logger.setLevel(logging.INFO)
+    logger.info("Training/evaluation parameters %s", training_args)
+
+    if num_of_hosts and not training_args.push_to_hub:
+        logger.warning(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --push_to_hub to be able to save checkpoints to the hub."
+        )
+    if num_of_hosts and not training_args.overwrite_output_dir and training_args.resume_from_checkpoint:
+        logger.error(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --overwrite_output_dir to be able to resume from a pushed checkpoint."
+        )
+        sys.exit(1)
+
+    # Check the output dir is valid
+    if os.path.exists(training_args.output_dir):
+        if (
+            os.listdir(training_args.output_dir)
+            and training_args.do_train
+            and not training_args.overwrite_output_dir
+        ):
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use `--overwrite_output_dir` to overcome."
+            )
+        elif training_args.overwrite_output_dir:
+            logger.warning(f"Removing path {training_args.output_dir}")
+            shutil.rmtree(training_args.output_dir)
+      
+    # Handle the repository creation
+    output_dir = Path(training_args.output_dir)
+    if training_args.push_to_hub:
+        if training_args.hub_model_id is None:
+            repo_name = get_full_repo_name(
+                output_dir.absolute().name,
+                token=training_args.hub_token,
+                organization=training_args.push_to_hub_organization,
+            )
+        else:
+            repo_name = training_args.hub_model_id
+         
+        repo_url = None  
+        while not repo_url:
+            # Workaround for an internal HuggingFace error if the repo is being created by another worker
+            try:
+                repo_url = create_repo(
+                    repo_name, exist_ok=True, token=training_args.hub_token, private=training_args.hub_private_repo
+                )
+            except:
+                time.sleep(1)
+
+        repo = Repository(training_args.output_dir,
+                          clone_from=repo_name, token=training_args.hub_token)
+
+    # Set the model_name_or_path
+    model_name_or_path = model_args.model_name_or_path
+
+    # Try to detect last checkpoint and continue if possible
+    training_state = {"step": 0, "eval_lines": []}
+    if training_args.resume_from_checkpoint:
+        if (output_dir / "flax_model.msgpack").exists() and (output_dir / "training_state.bin").exists():
+            training_state = json.loads((output_dir / "training_state.bin").read_text())
+            model_name_or_path = os.path.join(training_args.output_dir)
+            logger.info(
+                f"Checkpoint detected, resuming training from {training_args.output_dir} at step {training_state['step']}."
+            )
+        else:
+            logger.info(
+                f"No valid checkpoint found in {training_args.output_dir}. Starting from {model_name_or_path}."
+            )
+    
+    
+    # Load dataset
+    raw_datasets = IterableDatasetDict() if data_args.streaming else DatasetDict()
+
+    if training_args.do_train:
+        raw_datasets["train"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.train_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if training_args.do_eval:
+        raw_datasets["eval"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.eval_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if not training_args.do_train and not training_args.do_eval:
+        raise ValueError(
+            "Cannot not train and not do evaluation. At least one of training or evaluation has to be performed."
+        )
+
+    raw_datasets_features = list(
+        next(iter(raw_datasets.values())).features.keys())
+
+    if data_args.audio_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--audio_column_name` to the correct audio column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    if data_args.text_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--text_column_name` to the correct text column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    # Load pretrained model, tokenizer, and feature extractor
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    feature_extractor = AutoFeatureExtractor.from_pretrained(
+        model_args.feature_extractor_name if model_args.feature_extractor_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(
+        model_name_or_path,
+        config=config,
+        dtype=getattr(jnp, model_args.dtype),
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    logger.info(
+        f"Successfully loaded the model '{model_name_or_path}'."
+    )
+    
+    if model.config.decoder_start_token_id is None:
+        raise ValueError(
+            "Make sure that `config.decoder_start_token_id` is correctly defined")
+
+    # Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,
+    # so we just need to set the correct target sampling rate.
+    dataset_sampling_rate = next(
+        iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
+
+    if dataset_sampling_rate != feature_extractor.sampling_rate:
+        raw_datasets = raw_datasets.cast_column(
+            data_args.audio_column_name, datasets.features.Audio(
+                sampling_rate=feature_extractor.sampling_rate)
+        )
+
+    # Preprocessing the datasets.
+    # We need to read the audio files as arrays and tokenize the targets.
+    max_input_length = int(
+        data_args.max_duration_in_seconds * feature_extractor.sampling_rate)
+    min_input_length = int(
+        data_args.min_duration_in_seconds * feature_extractor.sampling_rate)
+    max_label_length = (
+        data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length
+    )
+    pad_input_to_multiple_of = data_args.pad_input_to_multiple_of
+    pad_target_to_multiple_of = data_args.pad_target_to_multiple_of
+    audio_column_name = data_args.audio_column_name
+    num_workers = data_args.preprocessing_num_workers
+    text_column_name = data_args.text_column_name
+    model_input_name = feature_extractor.model_input_names[0]
+    do_lower_case = data_args.do_lower_case
+    do_remove_punctuation = data_args.do_remove_punctuation
+    normalizer = BasicTextNormalizer()  # 'official' text normalizer from OpenAI
+
+    if data_args.language is not None:
+        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)
+        tokenizer.set_prefix_tokens(
+            language=data_args.language, task=data_args.task)
+    
+    
+    def prepare_dataset(batch):
+        # Process audio
+        sample = batch[audio_column_name]
+        inputs = feature_extractor(
+            sample["array"], sampling_rate=sample["sampling_rate"])
+        # Process audio length
+        batch[model_input_name] = inputs.get(model_input_name)[0]
+        batch["input_length"] = len(sample["array"])
+
+        # Process targets
+        input_str = batch[text_column_name].lower(
+        ) if do_lower_case else batch[text_column_name]
+        if do_remove_punctuation:
+            input_str = normalizer(input_str).strip()
+        batch["labels"] = tokenizer(input_str,truncation=True, max_length=max_label_length).input_ids
+        return batch
+
+    with training_args.main_process_first(desc="dataset map pre-processing"):
+        vectorized_datasets = raw_datasets.map(
+            prepare_dataset,
+            remove_columns=raw_datasets_features,
+        )
+
+    # Filter training data with inputs longer than max_input_length
+    def is_audio_in_length_range(length):
+        return min_input_length < length < max_input_length
+
+    if training_args.do_train:
+        vectorized_datasets["train"] = vectorized_datasets["train"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    if training_args.do_eval:
+        vectorized_datasets["eval"] = vectorized_datasets["eval"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    # Load metrics and write stats
+    metric_wer = evaluate.load("wer")
+    metric_cer = evaluate.load("cer")
+    do_normalize_eval = data_args.do_normalize_eval
+
+    def compute_metrics(pred_ids, label_ids, return_preds_labels=False):
+        # Replace padded labels by the padding token
+        for idx in range(len(label_ids)):
+            label_ids[idx][label_ids[idx] == -100] = tokenizer.pad_token_id
+
+        predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
+        # We do not want to group tokens when computing the metrics
+        labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
+
+        if do_normalize_eval:
+            pred_str = [normalizer(pred) for pred in predictions]
+            label_str = [normalizer(label) for label in labels]
+            # Filtering step to only evaluate the samples that correspond to non-zero references:
+            pred_str = [pred_str[i]
+                        for i in range(len(pred_str)) if len(label_str[i]) > 0]
+            label_str = [label_str[i]
+                         for i in range(len(label_str)) if len(label_str[i]) > 0]
+        else:
+            pred_str = predictions
+            label_str = labels
+
+        wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)
+        cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)
+
+        if return_preds_labels:
+            return {"wer": wer, "cer": cer}, predictions, labels
+        else:
+            return {"wer": wer, "cer": cer}
+
+    def update_training_state(training_state, train_metrics, eval_metrics, step):
+        safe_value = lambda x: float(x.tolist() if isinstance(x, jnp.ndarray) else x)
+        state = {"step": step}
+        eval_lines = training_state["eval_lines"]
+       
+        train_metrics = get_metrics(train_metrics)
+        train_metrics_dict = {}
+        for metric_name, values in train_metrics.items():
+            tag = f"train_{metric_name}"
+            for i, value in enumerate(values):
+                train_metrics_dict[step - len(values) + i + 1] = {tag: safe_value(value)}
+
+        eval_metrics_dict = {}
+        for metric_name, value in eval_metrics.items():
+            tag = f"eval_{metric_name}"
+            eval_metrics_dict.update({
+                "step": step,
+                tag: safe_value(value),
+            })
+            if step in train_metrics_dict:
+                eval_metrics_dict.update(train_metrics_dict[step])
+        eval_lines.append(eval_metrics_dict)
+        return {**state, "eval_lines": eval_lines}
+
+    def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=None, labels=None):
+        summary_writer.scalar("train_time", train_time, step)
+
+        train_metrics = get_metrics(train_metrics)
+        for key, vals in train_metrics.items():
+            tag = f"train_{key}"
+            for i, val in enumerate(vals):
+                summary_writer.scalar(tag, val, step - len(vals) + i + 1)
+
+        for metric_name, value in eval_metrics.items():
+            summary_writer.scalar(f"eval_{metric_name}", value, step)
+        
+        # Log evaluation predictions
+        if predictions and labels:
+            df = pd.DataFrame({
+                "references": labels,
+                "predictions": predictions,
+            })
+            df["wer"] = df.apply(lambda row: metric_wer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            df["cer"] = df.apply(lambda row: metric_cer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            markdown_table = df.to_markdown(index=False)
+            eval_metrics_table = pd.DataFrame.from_dict([{"step": step, **eval_metrics}]).to_markdown(index=False)
+            summary_writer.text("eval_predictions", eval_metrics_table + "\n\n" + markdown_table, step)
+            # External logging function
+            if data_args.log_eval_predictions_fn:
+                module, fname = data_args.log_eval_predictions_fn.rsplit('.', 1)
+                fn = getattr(import_module(module), fname)
+                fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=predictions, labels=labels, training_args=training_args)
+
+    # Save feature extractor, tokenizer and config
+    feature_extractor.save_pretrained(training_args.output_dir)
+    tokenizer.save_pretrained(training_args.output_dir)
+    config.save_pretrained(training_args.output_dir)
+
+    processor = AutoProcessor.from_pretrained(training_args.output_dir)
+
+    data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(
+        processor=processor,
+        decoder_start_token_id=model.config.decoder_start_token_id,
+        input_padding="longest",
+        target_padding="longest",
+        max_target_length=max_label_length,
+        pad_input_to_multiple_of=pad_input_to_multiple_of,
+        pad_target_to_multiple_of=pad_target_to_multiple_of if pad_target_to_multiple_of else max_label_length,
+    )
+
+    # Enable tensorboard only on the master node
+    has_tensorboard = is_tensorboard_available()
+    if has_tensorboard and current_host_idx == 0:
+        try:
+            # TODO: Decouple wandb from tensorboard
+            import wandb
+
+            has_wandb = True
+        except ImportError:
+            has_wandb = False
+            if data_args.wandb_entity is not None or data_args.wandb_project is not None:
+                logger.warning(
+                    f"Unable to display metrics through Weights & Biases because some packages are not installed: {ie}"
+                )
+        try:
+            if has_wandb:
+                wandb.tensorboard.patch(root_logdir=output_dir / "runs")
+                wandb.init(
+                    entity=data_args.wandb_entity,
+                    project=data_args.wandb_project,
+                    name=training_args.run_name,
+                    notes=data_args.run_description,
+                    save_code=True,
+                    sync_tensorboard=True,
+                )
+                wandb.config.update(training_args)
+                wandb.config.update(model_args)
+                wandb.config.update(data_args)
+            from flax.metrics.tensorboard import SummaryWriter
+
+            summary_writer = SummaryWriter(
+                log_dir=output_dir / "runs" / f"{datetime.now():%b%d_%H-%M-%S}_{socket.gethostname()}")
+        except ImportError as ie:
+            has_tensorboard = False
+            logger.warning(
+                f"Unable to display metrics through TensorBoard because some packages are not installed: {ie}"
+            )
+    else:
+        logger.warning(
+            "Unable to display metrics through TensorBoard because the package is not installed: "
+            "Please run pip install tensorboard to enable."
+        )
+
+    # Initialize our training
+    rng = jax.random.PRNGKey(training_args.seed)
+    rng, dropout_rng = jax.random.split(rng)
+
+    # Store some constant
+    train_batch_size = int(
+        training_args.per_device_train_batch_size) * jax.device_count()
+    eval_batch_size = int(
+        training_args.per_device_eval_batch_size) * jax.device_count()
+
+    # Create learning rate schedule
+    lr_scheduler_types = {"linear", "constant", "constant_with_warmup"}
+    if training_args.lr_scheduler_type not in lr_scheduler_types:
+        raise ValueError(
+            f"lr_scheduler_type of type {training_args.lr_scheduler_type} not supported, choose from {lr_scheduler_types}."
+        )
+    elif training_args.lr_scheduler_type == "constant":
+        warmup_init_value = training_args.learning_rate
+        decay_end_value = training_args.learning_rate
+    elif training_args.lr_scheduler_type == "constant_with_warmup":
+        warmup_init_value = 0.0
+        decay_end_value = training_args.learning_rate
+    else:
+        warmup_init_value = 0.0
+        decay_end_value = 0.0
+        
+    linear_decay_lr_schedule_fn = create_learning_rate_fn(
+        data_args.num_train_steps,
+        training_args.warmup_steps,
+        training_args.learning_rate,
+        start_step=training_state["step"],
+        warmup_init_value=warmup_init_value,
+        decay_end_value=decay_end_value
+    )
+    
+    # We use Optax's "masking" functionality to not apply weight decay
+    # to bias and LayerNorm scale parameters. decay_mask_fn returns a
+    # mask boolean with the same structure as the parameters.
+    # The mask is True for parameters that should be decayed.
+    def decay_mask_fn(params):
+        flat_params = traverse_util.flatten_dict(params)
+        # Find out all LayerNorm parameters
+        layer_norm_candidates = ["layernorm", "layer_norm", "ln"]
+        layer_norm_named_params = set(
+            [
+                layer[-2:]
+                for layer_norm_name in layer_norm_candidates
+                for layer in flat_params.keys()
+                if layer_norm_name in "".join(layer).lower()
+            ]
+        )
+        flat_mask = {path: (path[-1] != "bias" and path[-2:]
+                            not in layer_norm_named_params) for path in flat_params}
+        return traverse_util.unflatten_dict(flat_mask)
+    
+    # Create adam optimizer
+    adamw = optax.adamw(
+        learning_rate=linear_decay_lr_schedule_fn,
+        b1=training_args.adam_beta1,
+        b2=training_args.adam_beta2,
+        eps=training_args.adam_epsilon,
+        weight_decay=training_args.weight_decay,
+        mask=decay_mask_fn,
+    )
+
+    # Setup train state
+    state = TrainState.create(
+        apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)
+
+    # Label smoothed cross entropy
+    def loss_fn(logits, labels, label_smoothing_factor=0.0):
+        """
+        The label smoothing implementation is adapted from Flax's official example:
+        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104
+        """
+        vocab_size = logits.shape[-1]
+        confidence = 1.0 - label_smoothing_factor
+        low_confidence = (1.0 - confidence) / (vocab_size - 1)
+        normalizing_constant = -(
+            confidence * jnp.log(confidence) + (vocab_size - 1) *
+            low_confidence * jnp.log(low_confidence + 1e-20)
+        )
+        soft_labels = onehot(labels, vocab_size,
+                             on_value=confidence, off_value=low_confidence)
+
+        loss = optax.softmax_cross_entropy(logits, soft_labels)
+        loss = loss - normalizing_constant
+
+        # Ignore padded tokens from loss, i.e. where labels are not set to -100
+        padding_mask = labels >= 0
+        loss = loss * padding_mask
+        loss = loss.sum()
+        num_labels = padding_mask.sum()
+        return loss, num_labels
+
+    # Define gradient update step fn
+    def train_step(state, batch, label_smoothing_factor=0.0):
+        
+        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)
+
+        def compute_loss(params):
+            labels = batch.pop("labels")
+            logits = state.apply_fn(
+                **batch, params=params, dropout_rng=dropout_rng, train=True)[0]
+            loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+            return loss, num_labels
+
+        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)
+        (loss, num_labels), grad = grad_fn(state.params)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        # True grad = total grad / total samples
+        grad = jax.lax.psum(grad, "batch")
+        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)
+        new_state = state.apply_gradients(
+            grads=grad, dropout_rng=new_dropout_rng)
+
+        metrics = {"loss": loss,
+                   "learning_rate": linear_decay_lr_schedule_fn(state.step)}
+
+        return new_state, metrics
+
+    # Define eval fn
+    def eval_step(params, batch, label_smoothing_factor=0.0):
+        labels = batch.pop("labels")
+        logits = model(**batch, params=params, train=False)[0]
+
+        loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        metrics = {"loss": loss}
+        return metrics
+
+    # Define generation function
+    num_beams = model_args.num_beams if model_args.num_beams is not None else model.config.num_beams
+    gen_kwargs = {"max_length": max_label_length, "num_beams": num_beams}
+
+     
+    def generate_step(params, batch):
+        model.params = params
+        
+        attention_mask = batch.get("attention_mask")
+        
+        #if attention_mask is not None:
+        output_ids = model.generate(batch[model_input_name], attention_mask=attention_mask, **gen_kwargs)
+        #else:
+        #    output_ids = model.generate(batch[model_input_name], **gen_kwargs)
+        
+        return output_ids.sequences
+
+    # Create parallel version of the train and eval step
+    p_train_step = jax.pmap(
+        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch", donate_argnums=(0, )
+    )
+    p_eval_step = jax.pmap(partial(
+        eval_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch")
+    p_generate_step = jax.pmap(generate_step, "batch")
+
+    # Replicate the train state on each device
+    state = state.replicate()
+    
+    # Logging
+    logger.info("***** Running training *****")
+    logger.info(
+        f"  Dataset name = {data_args.dataset_name}")
+    logger.info(
+        f"  Dataset config name = {data_args.dataset_config_name}")
+    logger.info(
+        f"  Learning rate = {training_args.learning_rate}")
+    logger.info(
+        f"  Scheduler = {training_args.lr_scheduler_type}")
+    logger.info(
+        f"  Num examples = {data_args.num_train_steps * train_batch_size}")
+    if num_of_hosts > 1:
+        logger.info(
+            f"  Number of hosts = {num_of_hosts}")
+        logger.info(
+            f"  Current host idx = {current_host_idx}")
+    logger.info(
+        f"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}")
+    logger.info(
+        f"  Total train batch size per node (w. parallel & distributed) = {train_batch_size // num_of_hosts}")
+    logger.info(
+        f"  Total train batch size (w. parallel & distributed) = {train_batch_size}")
+    logger.info(f"  Total optimization steps = {data_args.num_train_steps - training_state['step']}")
+    if training_state['step'] > 0:
+        logger.info(f"  ↪ Starting at {str(training_state['step'])} and finishing at {str(data_args.num_train_steps)}")
+
+    train_time = 0
+
+    # Training summary
+    language_code = None  # Maybe 'multilingual'?
+    if data_args.language is not None:
+        language = data_args.language.lower()
+        if language in TO_LANGUAGE_CODE:
+            language_code = TO_LANGUAGE_CODE[language]
+        elif len(language) == 2:
+            language_code = language
+    training_summary = {
+        "model_name": repo_name.split("/")[-1],
+        "language": language_code,
+        "tags": ["audio", "asr", "automatic-speech-recognition", "hf-asr-leaderboard"],
+        "license": "apache-2.0",
+        "finetuned_from": model_args.model_name_or_path,
+        "tasks": ["asr"],
+        "dataset": data_args.dataset_name,
+        "dataset_args": {"name": data_args.dataset_config_name},
+        "source": "flax",
+        "eval_lines": [],
+        "eval_results": None,
+        "hyperparameters": {
+            "learning_rate": training_args.learning_rate,
+            "lr_scheduler_type": training_args.lr_scheduler_type,
+            "per_device_train_batch_size": training_args.per_device_train_batch_size,
+            "total_train_batch_size_per_node": train_batch_size // num_of_hosts,
+            "total_train_batch_size": train_batch_size,
+            "total_optimization_steps": data_args.num_train_steps - training_state['step'],
+            "starting_optimization_step": training_state['step'] if training_state['step'] > 0 else None,
+            "finishing_optimization_step": data_args.num_train_steps,
+            "num_train_dataset_workers": f"{num_workers}",
+            "total_num_training_examples": data_args.num_train_steps * train_batch_size,
+        },
+        # TODO: Adapt https://github.com/huggingface/transformers/blob/main/src/transformers/modelcard.py#L855
+        # "hyperparameters": training_args.to_sanitized_dict()
+    }
+    
+    # Create README if it does not exist
+    readme = output_dir / "README.md"
+    if not readme.exists():
+        readme.write_text(TrainingSummary(**training_summary).to_model_card())
+    
+    # ======================== Training ================================
+    train_start = time.time()
+
+    train_metrics = []
+    epoch = 0
+    train_dataset = vectorized_datasets["train"].shuffle(seed=training_args.seed, buffer_size=data_args.shuffle_buffer_size)
+    
+    # Split by node
+    train_dataset = split_dataset_by_node(train_dataset, rank=current_host_idx, world_size=num_of_hosts)   
+    
+    if train_dataset.n_shards < data_args.preprocessing_num_workers:
+        num_workers = train_dataset.n_shards
+
+    logger.info(f"  Number of train dataset workers = {num_workers} {'(Capped by the number of dataset shards)' if train_dataset.n_shards < data_args.preprocessing_num_workers else ''} {'(ADVICE: In most cases you will speed up training considerably if you increase the value of --preprocessing_num_workers!)' if num_workers < 10 else ''}")
+ 
+    eval_dataset = vectorized_datasets["eval"]
+    train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+    
+    if not training_args.ignore_data_skip and training_state["step"] > 0:
+        logger.info(
+            f"  Will skip the first {training_state['step']} steps. If this takes a lot of time,"
+            " you can add the `--ignore_data_skip` flag to your launch command, but you will resume the"
+            " training on data already seen by your model."
+        )
+        for step in tqdm(range(training_state["step"]), desc=f"Skipping data for {training_state['step']} steps...", position=1, leave=False):
+            try:
+                samples = next(train_loader)
+            except StopIteration:
+                epoch += 1
+                train_dataset.set_epoch(epoch)
+                train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+                samples = next(train_loader)
+            batch = data_collator(samples)
+            # batch = shard(batch.data)
+    
+    
+    for step in tqdm(range(data_args.num_train_steps), desc="Training...", position=1, leave=False):
+        # Skip initial steps if these are specified. 
+        if step < training_state["step"]:
+            continue
+        
+        # =========================== Training ===========================
+        try:
+            samples = next(train_loader)
+        except StopIteration:
+            epoch += 1
+            train_dataset.set_epoch(epoch)
+            train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+            samples = next(train_loader)
+            logger.info(
+                f"Completed epoch ({epoch} | Loss: {train_metric['loss']}, Learning Rate:"
+                f" {train_metric['learning_rate']})"
+            )
+        
+        batch = data_collator(samples)
+        batch = shard(batch.data)
+        state, train_metric = p_train_step(state, batch)
+        train_metrics.append(train_metric)
+                
+        train_time += time.time() - train_start
+        train_metric = unreplicate(train_metric)
+
+        # ========================== Evaluating ==========================
+        # Evaluate at each eval_steps, and at the end of training at num_train_steps
+        if step % training_args.eval_steps == 0 or step == data_args.num_train_steps - 1:
+            logger.info(
+                f"Starting evaluation at step {step} of num_training_step {data_args.num_train_steps} steps. Planned evaluation every {training_args.eval_steps} steps." 
+            )
+            eval_metrics = []
+            eval_preds = []
+            eval_labels = []
+            eval_loader = data_loader(eval_dataset, eval_batch_size, drop_last=False)
+            if data_args.max_eval_samples:
+                max_eval_steps_iter = range(1 + data_args.max_eval_samples // eval_batch_size)
+            else:
+                max_eval_steps_iter = itertools.repeat(None)
+            for _ in tqdm(max_eval_steps_iter, desc="Evaluating...", position=2, leave=False):
+                # Model forward
+                try:
+                    samples = next(eval_loader)
+                except StopIteration:
+                    break
+                batch = data_collator(samples)
+                
+                labels = batch["labels"]
+
+                metrics = pad_shard_unpad(p_eval_step, static_return=True)(
+                    state.params, batch.data, min_device_batch=training_args.per_device_eval_batch_size
+                )
+                eval_metrics.append(metrics)
+
+                # Generation
+                if training_args.predict_with_generate:
+                    generated_ids = pad_shard_unpad(
+                        p_generate_step)(state.params, batch.data)
+                    eval_preds.extend(jax.device_get(
+                        generated_ids.reshape(-1, gen_kwargs["max_length"])))
+                    eval_labels.extend(labels)
+
+            # Normalize eval metrics
+            eval_metrics = get_metrics(eval_metrics)
+            eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)
+
+            # Compute metrics
+            metric_desc = ""
+            if training_args.predict_with_generate:
+                metric_values, pred_str, label_str = compute_metrics(
+                    eval_preds, eval_labels, return_preds_labels=True
+                )
+                eval_metrics.update(metric_values)
+                metric_desc = " | ".join(
+                    [f"Eval {key}: {value}" for key, value in metric_values.items()])
+
+            # Print metrics
+            desc = f"Step: {step} | Epoch: {epoch} (Eval Loss: {eval_metrics['loss']} | {metric_desc})"
+            logger.info(desc)
+
+            # Update training state
+            training_state = update_training_state(
+                training_state,
+                train_metrics,
+                eval_metrics,
+                step,
+            )
+
+            # Save metrics
+            if has_tensorboard and current_host_idx == 0:
+                log_max_predictions = data_args.log_max_eval_predictions if data_args.log_max_eval_predictions else 0
+                write_metric(
+                    summary_writer,
+                    train_metrics,
+                    eval_metrics,
+                    train_time,
+                    step,
+                    predictions=pred_str[:log_max_predictions],
+                    labels=label_str[:log_max_predictions]
+                )
+
+            # Save checkpoint at each eval_steps and push checkpoint to the hub
+            if current_host_idx  == 0:
+                params = jax.device_get(
+                    jax.tree_util.tree_map(lambda x: x[0], state.params))
+                model.save_pretrained(training_args.output_dir, params=params)
+                tokenizer.save_pretrained(training_args.output_dir)
+                # Report eval results if training is done
+                if step == data_args.num_train_steps - 1:
+                    training_summary["eval_results"] = training_state["eval_lines"][-1]
+                else:
+                    training_summary.update({"eval_lines": training_state["eval_lines"]})
+                (output_dir / "training_state.bin").write_text(json.dumps(training_state))
+                # Write model card
+                readme.write_text(TrainingSummary(**training_summary).to_model_card())
+                if training_args.push_to_hub:
+                    repo.push_to_hub(
+                        commit_message=f"Saving weights and logs of step {step} - epoch {epoch}", blocking=False)
+
+if __name__ == "__main__":
+    main()
diff --git a/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/config.yaml b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/config.yaml
new file mode 100644
index 0000000..57ab129
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/config.yaml
@@ -0,0 +1,512 @@
+wandb_version: 1
+
+__cached__setup_devices:
+  desc: null
+  value: cpu
+_n_gpu:
+  desc: null
+  value: 0
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.14.0
+    code_path: code/run_flax_speech_recognition_seq2seq_streaming_debug.py
+    framework: huggingface
+    huggingface_version: 4.28.0.dev0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.10
+    start_time: 1681320905.568576
+    t:
+      1:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      2:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      3:
+      - 13
+      - 23
+      - 34
+      4: 3.8.10
+      5: 0.14.0
+      6: 4.28.0.dev0
+      8:
+      - 5
+adafactor:
+  desc: null
+  value: false
+adam_beta1:
+  desc: null
+  value: 0.9
+adam_beta2:
+  desc: null
+  value: 0.999
+adam_epsilon:
+  desc: null
+  value: 1.0e-08
+audio_column_name:
+  desc: null
+  value: audio
+auto_find_batch_size:
+  desc: null
+  value: false
+bf16:
+  desc: null
+  value: false
+bf16_full_eval:
+  desc: null
+  value: false
+cache_dir:
+  desc: null
+  value: null
+config_name:
+  desc: null
+  value: null
+data_seed:
+  desc: null
+  value: null
+dataloader_drop_last:
+  desc: null
+  value: false
+dataloader_num_workers:
+  desc: null
+  value: 0
+dataloader_pin_memory:
+  desc: null
+  value: true
+dataset_cache_dir:
+  desc: null
+  value: null
+dataset_config_name:
+  desc: null
+  value: null
+dataset_name:
+  desc: null
+  value: NbAiLab/NCC_speech_all_v5
+ddp_bucket_cap_mb:
+  desc: null
+  value: null
+ddp_find_unused_parameters:
+  desc: null
+  value: null
+ddp_timeout:
+  desc: null
+  value: 1800
+debug:
+  desc: null
+  value: []
+deepspeed:
+  desc: null
+  value: null
+disable_tqdm:
+  desc: null
+  value: false
+do_eval:
+  desc: null
+  value: true
+do_lower_case:
+  desc: null
+  value: false
+do_normalize_eval:
+  desc: null
+  value: true
+do_predict:
+  desc: null
+  value: false
+do_remove_punctuation:
+  desc: null
+  value: false
+do_train:
+  desc: null
+  value: true
+dtype:
+  desc: null
+  value: bfloat16
+eval_accumulation_steps:
+  desc: null
+  value: null
+eval_delay:
+  desc: null
+  value: 0
+eval_split_name:
+  desc: null
+  value: validation
+eval_steps:
+  desc: null
+  value: 10000
+evaluation_strategy:
+  desc: null
+  value: IntervalStrategy.NO
+feature_extractor_name:
+  desc: null
+  value: null
+fp16:
+  desc: null
+  value: false
+fp16_backend:
+  desc: null
+  value: auto
+fp16_full_eval:
+  desc: null
+  value: false
+fp16_opt_level:
+  desc: null
+  value: O1
+fsdp:
+  desc: null
+  value: []
+fsdp_config:
+  desc: null
+  value:
+    fsdp_min_num_params: 0
+    xla: false
+    xla_fsdp_grad_ckpt: false
+fsdp_min_num_params:
+  desc: null
+  value: 0
+fsdp_transformer_layer_cls_to_wrap:
+  desc: null
+  value: null
+full_determinism:
+  desc: null
+  value: false
+generation_config:
+  desc: null
+  value: null
+generation_max_length:
+  desc: null
+  value: null
+generation_num_beams:
+  desc: null
+  value: null
+gradient_accumulation_steps:
+  desc: null
+  value: 1
+gradient_checkpointing:
+  desc: null
+  value: false
+greater_is_better:
+  desc: null
+  value: null
+group_by_length:
+  desc: null
+  value: false
+half_precision_backend:
+  desc: null
+  value: auto
+hub_model_id:
+  desc: null
+  value: NbAiLab/scream_large_oct_debug_128seq
+hub_private_repo:
+  desc: null
+  value: true
+hub_strategy:
+  desc: null
+  value: HubStrategy.EVERY_SAVE
+hub_token:
+  desc: null
+  value: null
+ignore_data_skip:
+  desc: null
+  value: true
+include_inputs_for_metrics:
+  desc: null
+  value: false
+jit_mode_eval:
+  desc: null
+  value: false
+label_names:
+  desc: null
+  value: null
+label_smoothing_factor:
+  desc: null
+  value: 0.0
+language:
+  desc: null
+  value: Norwegian
+learning_rate:
+  desc: null
+  value: 5.0e-06
+length_column_name:
+  desc: null
+  value: length
+load_best_model_at_end:
+  desc: null
+  value: false
+local_rank:
+  desc: null
+  value: -1
+log_eval_predictions_fn:
+  desc: null
+  value: log_predictions.write_predictions
+log_level:
+  desc: null
+  value: passive
+log_level_replica:
+  desc: null
+  value: warning
+log_max_eval_predictions:
+  desc: null
+  value: 100
+log_on_each_node:
+  desc: null
+  value: true
+logging_dir:
+  desc: null
+  value: ../../scream_large_oct_debug_128seq/runs/Apr12_17-34-18_t1v-n-0a06f6ef-w-0
+logging_first_step:
+  desc: null
+  value: false
+logging_nan_inf_filter:
+  desc: null
+  value: true
+logging_steps:
+  desc: null
+  value: 500
+logging_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+lr_scheduler_type:
+  desc: null
+  value: SchedulerType.LINEAR
+max_duration_in_seconds:
+  desc: null
+  value: 30.0
+max_eval_samples:
+  desc: null
+  value: null
+max_grad_norm:
+  desc: null
+  value: 1.0
+max_label_length:
+  desc: null
+  value: 128
+max_steps:
+  desc: null
+  value: -1
+max_train_samples:
+  desc: null
+  value: null
+metric_for_best_model:
+  desc: null
+  value: null
+min_duration_in_seconds:
+  desc: null
+  value: 0.0
+model_name_or_path:
+  desc: null
+  value: openai/whisper-large-v2
+model_revision:
+  desc: null
+  value: main
+mp_parameters:
+  desc: null
+  value: ''
+no_cuda:
+  desc: null
+  value: false
+num_beams:
+  desc: null
+  value: 5
+num_train_epochs:
+  desc: null
+  value: 3.0
+num_train_steps:
+  desc: null
+  value: 50000
+optim:
+  desc: null
+  value: OptimizerNames.ADAMW_HF
+optim_args:
+  desc: null
+  value: null
+output_dir:
+  desc: null
+  value: ../../scream_large_oct_debug_128seq
+overwrite_cache:
+  desc: null
+  value: false
+overwrite_output_dir:
+  desc: null
+  value: true
+pad_input_to_multiple_of:
+  desc: null
+  value: null
+pad_target_to_multiple_of:
+  desc: null
+  value: null
+past_index:
+  desc: null
+  value: -1
+per_device_eval_batch_size:
+  desc: null
+  value: 8
+per_device_train_batch_size:
+  desc: null
+  value: 8
+per_gpu_eval_batch_size:
+  desc: null
+  value: null
+per_gpu_train_batch_size:
+  desc: null
+  value: null
+predict_with_generate:
+  desc: null
+  value: true
+prediction_loss_only:
+  desc: null
+  value: false
+preprocessing_num_workers:
+  desc: null
+  value: 32
+push_to_hub:
+  desc: null
+  value: true
+push_to_hub_model_id:
+  desc: null
+  value: null
+push_to_hub_organization:
+  desc: null
+  value: null
+push_to_hub_token:
+  desc: null
+  value: null
+ray_scope:
+  desc: null
+  value: last
+remove_unused_columns:
+  desc: null
+  value: true
+report_to:
+  desc: null
+  value:
+  - tensorboard
+  - wandb
+resume_from_checkpoint:
+  desc: null
+  value: 'True'
+run_description:
+  desc: null
+  value: A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear
+    decay on the all_v5-corpus.
+run_name:
+  desc: null
+  value: ScreamLarge - debug_beam5_long
+save_on_each_node:
+  desc: null
+  value: false
+save_steps:
+  desc: null
+  value: 500
+save_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+save_total_limit:
+  desc: null
+  value: null
+seed:
+  desc: null
+  value: 42
+sharded_ddp:
+  desc: null
+  value: []
+shuffle_buffer_size:
+  desc: null
+  value: 500
+skip_memory_metrics:
+  desc: null
+  value: true
+sortish_sampler:
+  desc: null
+  value: false
+streaming:
+  desc: null
+  value: true
+task:
+  desc: null
+  value: transcribe
+text_column:
+  desc: null
+  value: null
+text_column_name:
+  desc: null
+  value: text
+tf32:
+  desc: null
+  value: null
+tokenizer_name:
+  desc: null
+  value: null
+torch_compile:
+  desc: null
+  value: false
+torch_compile_backend:
+  desc: null
+  value: null
+torch_compile_mode:
+  desc: null
+  value: null
+torchdynamo:
+  desc: null
+  value: null
+tpu_metrics_debug:
+  desc: null
+  value: false
+tpu_num_cores:
+  desc: null
+  value: null
+train_split_name:
+  desc: null
+  value: train
+use_auth_token:
+  desc: null
+  value: true
+use_fast_tokenizer:
+  desc: null
+  value: true
+use_ipex:
+  desc: null
+  value: false
+use_legacy_prediction_loop:
+  desc: null
+  value: false
+use_mps_device:
+  desc: null
+  value: false
+wandb_entity:
+  desc: null
+  value: nbailab
+wandb_project:
+  desc: null
+  value: Scream - septimus
+warmup_ratio:
+  desc: null
+  value: 0.0
+warmup_steps:
+  desc: null
+  value: 10000
+weight_decay:
+  desc: null
+  value: 0.0
+xpu_backend:
+  desc: null
+  value: null
diff --git a/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/events.out.tfevents.1681320906.t1v-n-0a06f6ef-w-0.1601312.0.v2 b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/events.out.tfevents.1681320906.t1v-n-0a06f6ef-w-0.1601312.0.v2
new file mode 120000
index 0000000..5f61333
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/events.out.tfevents.1681320906.t1v-n-0a06f6ef-w-0.1601312.0.v2
@@ -0,0 +1 @@
+/home/perk/models/scream_large_oct_debug_128seq/runs/Apr12_17-35-06_t1v-n-0a06f6ef-w-0/events.out.tfevents.1681320906.t1v-n-0a06f6ef-w-0.1601312.0.v2
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/requirements.txt b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/requirements.txt
new file mode 100644
index 0000000..34d1446
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/requirements.txt
@@ -0,0 +1,140 @@
+absl-py==1.4.0
+aiohttp==3.8.4
+aiosignal==1.3.1
+appdirs==1.4.4
+astunparse==1.6.3
+async-timeout==4.0.2
+attrs==22.2.0
+audioread==3.0.0
+cached-property==1.5.2
+cachetools==5.3.0
+certifi==2022.12.7
+cffi==1.15.1
+charset-normalizer==3.1.0
+chex==0.1.7
+click==8.1.3
+cmake==3.26.1
+datasets==2.11.0
+decorator==5.1.1
+dill==0.3.6
+dm-tree==0.1.8
+docker-pycreds==0.4.0
+etils==1.1.1
+evaluate==0.4.0
+filelock==3.10.7
+flatbuffers==23.3.3
+flax==0.6.8
+frozenlist==1.3.3
+fsspec==2023.3.0
+gast==0.4.0
+gitdb==4.0.10
+gitpython==3.1.31
+google-auth-oauthlib==1.0.0
+google-auth==2.17.1
+google-pasta==0.2.0
+grpcio==1.53.0
+h5py==3.8.0
+huggingface-hub==0.13.3
+idna==3.4
+importlib-metadata==6.1.0
+importlib-resources==5.12.0
+jax==0.4.8
+jaxlib==0.4.7
+jinja2==3.1.2
+jiwer==3.0.1
+joblib==1.2.0
+keras==2.12.0
+lazy-loader==0.2
+libclang==16.0.0
+librosa==0.10.0.post2
+libtpu-nightly==0.1.dev20230327
+lit==16.0.0
+llvmlite==0.39.1
+markdown-it-py==2.2.0
+markdown==3.4.3
+markupsafe==2.1.2
+mdurl==0.1.2
+ml-dtypes==0.0.4
+mpmath==1.3.0
+msgpack==1.0.5
+multidict==6.0.4
+multiprocess==0.70.14
+nest-asyncio==1.5.6
+networkx==3.0
+numba==0.56.4
+numpy==1.23.5
+nvidia-cublas-cu11==11.10.3.66
+nvidia-cuda-cupti-cu11==11.7.101
+nvidia-cuda-nvrtc-cu11==11.7.99
+nvidia-cuda-runtime-cu11==11.7.99
+nvidia-cudnn-cu11==8.5.0.96
+nvidia-cufft-cu11==10.9.0.58
+nvidia-curand-cu11==10.2.10.91
+nvidia-cusolver-cu11==11.4.0.1
+nvidia-cusparse-cu11==11.7.4.91
+nvidia-nccl-cu11==2.14.3
+nvidia-nvtx-cu11==11.7.91
+oauthlib==3.2.2
+opt-einsum==3.3.0
+optax==0.1.4
+orbax==0.1.7
+packaging==23.0
+pandas==1.5.3
+pathtools==0.1.2
+pip==23.0.1
+pkg-resources==0.0.0
+pooch==1.6.0
+protobuf==4.22.1
+psutil==5.9.4
+pyarrow==11.0.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pycparser==2.21
+pydub==0.25.1
+pygments==2.14.0
+python-dateutil==2.8.2
+pytz==2023.3
+pyyaml==6.0
+rapidfuzz==2.13.7
+regex==2023.3.23
+requests-oauthlib==1.3.1
+requests==2.28.2
+responses==0.18.0
+rich==13.3.3
+rsa==4.9
+scikit-learn==1.2.2
+scipy==1.10.1
+sentry-sdk==1.18.0
+setproctitle==1.3.2
+setuptools==44.0.0
+six==1.16.0
+smmap==5.0.0
+soundfile==0.12.1
+soxr==0.3.4
+sympy==1.11.1
+tabulate==0.9.0
+tensorboard-data-server==0.7.0
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.12.1
+tensorflow-estimator==2.12.0
+tensorflow-io-gcs-filesystem==0.32.0
+tensorflow==2.12.0
+tensorstore==0.1.35
+termcolor==2.2.0
+threadpoolctl==3.1.0
+tokenizers==0.13.2
+toolz==0.12.0
+torch==2.0.0
+torchaudio==2.0.1
+tqdm==4.65.0
+transformers==4.28.0.dev0
+triton==2.0.0
+typing-extensions==4.5.0
+urllib3==1.26.15
+wandb==0.14.0
+werkzeug==2.2.3
+wheel==0.40.0
+wrapt==1.14.1
+xxhash==3.2.0
+yarl==1.8.2
+zipp==3.15.0
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/wandb-metadata.json b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/wandb-metadata.json
new file mode 100644
index 0000000..191875d
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/wandb-metadata.json
@@ -0,0 +1,1302 @@
+{
+    "os": "Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2023-04-12T17:35:06.594753",
+    "startedAt": "2023-04-12T17:35:05.559474",
+    "docker": null,
+    "cuda": null,
+    "args": [
+        "--model_name_or_path",
+        "openai/whisper-large-v2",
+        "--run_name",
+        "ScreamLarge - debug_beam5_long",
+        "--run_description",
+        "A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear decay on the all_v5-corpus.",
+        "--wandb_entity",
+        "nbailab",
+        "--wandb_project",
+        "Scream - septimus",
+        "--dataset_name",
+        "NbAiLab/NCC_speech_all_v5",
+        "--language",
+        "Norwegian",
+        "--text_column_name",
+        "text",
+        "--train_split_name",
+        "train",
+        "--eval_split_name",
+        "validation",
+        "--output_dir",
+        "../../scream_large_oct_debug_128seq",
+        "--overwrite_output_dir",
+        "--warmup_steps",
+        "10000",
+        "--do_train",
+        "--do_eval",
+        "--num_train_steps",
+        "50000",
+        "--lr_scheduler_type",
+        "linear",
+        "--eval_steps",
+        "10000",
+        "--learning_rate",
+        "5e-6",
+        "--preprocessing_num_workers",
+        "32",
+        "--per_device_train_batch_size",
+        "8",
+        "--per_device_eval_batch_size",
+        "8",
+        "--predict_with_generate",
+        "--log_max_eval_predictions",
+        "100",
+        "--log_eval_predictions_fn",
+        "log_predictions.write_predictions",
+        "--streaming",
+        "True",
+        "--use_auth_token",
+        "True",
+        "--dtype",
+        "bfloat16",
+        "--hub_private_repo",
+        "True",
+        "--hub_model_id",
+        "NbAiLab/scream_large_oct_debug_128seq",
+        "--resume_from_checkpoint",
+        "True",
+        "--num_beams",
+        "5",
+        "--ignore_data_skip",
+        "--max_label_length",
+        "128",
+        "--push_to_hub"
+    ],
+    "state": "running",
+    "program": "../run_flax_speech_recognition_seq2seq_streaming_debug.py",
+    "codePath": "run_flax_speech_recognition_seq2seq_streaming_debug.py",
+    "git": {
+        "remote": "https://github.com/NbAiLab/nb-whisper.git",
+        "commit": "0606b43fdbe518275e6b879aa5b9af23b4abe733"
+    },
+    "email": "per@capia.no",
+    "root": "/home/perk/models/nb-whisper",
+    "host": "t1v-n-0a06f6ef-w-0",
+    "username": "perk",
+    "executable": "/home/perk/.whisper/bin/python",
+    "cpu_count": 120,
+    "cpu_count_logical": 240,
+    "cpu_freq": {
+        "current": 2249.9980000000096,
+        "min": 0.0,
+        "max": 0.0
+    },
+    "cpu_freq_per_core": [
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        }
+    ],
+    "disk": {
+        "total": 96.74600601196289,
+        "used": 33.38974380493164
+    },
+    "memory": {
+        "total": 400.47254943847656
+    }
+}
diff --git a/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/wandb-summary.json b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/wandb-summary.json
new file mode 100644
index 0000000..56be06b
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 728}}
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/run-j0moecbf.wandb b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/run-j0moecbf.wandb
new file mode 100644
index 0000000..601f067
Binary files /dev/null and b/experiment_scream_octavus/wandb/run-20230412_173505-j0moecbf/run-j0moecbf.wandb differ
diff --git a/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/code/run_flax_speech_recognition_seq2seq_streaming_debug.py b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/code/run_flax_speech_recognition_seq2seq_streaming_debug.py
new file mode 100644
index 0000000..782a40d
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/code/run_flax_speech_recognition_seq2seq_streaming_debug.py
@@ -0,0 +1,1318 @@
+#!/usr/bin/env python
+# coding=utf-8
+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR COND    ITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""
+Fine-tuning the Flax library models for sequence to sequence speech recognition.
+"""
+# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.
+
+import itertools
+import json
+import logging
+import os
+import shutil
+import socket
+import sys
+import tempfile
+import time
+from dataclasses import field
+from datetime import datetime
+from functools import partial
+from importlib import import_module
+from pathlib import Path
+from typing import Any, Callable, Dict, Generator, List, Optional, Union
+
+import flax
+import jax
+import jax.numpy as jnp 
+import numpy as np
+import optax
+import pandas as pd
+import torch
+# from jax.experimental.compilation_cache import compilation_cache; compilation_cache.initialize_cache(tempfile.gettempdir())
+from flax import jax_utils, traverse_util
+from flax.jax_utils import pad_shard_unpad, unreplicate
+from flax.training import train_state
+from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key
+from torch.utils.data import IterableDataset
+from tqdm import tqdm
+
+import datasets
+import evaluate
+import transformers
+from datasets import Dataset, DatasetDict, IterableDatasetDict, interleave_datasets, load_dataset
+from datasets.distributed import split_dataset_by_node
+from huggingface_hub import Repository, create_repo
+from transformers import (
+    AutoConfig,
+    AutoFeatureExtractor,
+    AutoProcessor,
+    AutoTokenizer,
+    FlaxAutoModelForSpeechSeq2Seq,
+    HfArgumentParser,
+    Seq2SeqTrainingArguments,
+    is_tensorboard_available,
+)
+from transformers.modelcard import TrainingSummary
+from transformers.models.whisper.english_normalizer import BasicTextNormalizer
+from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE
+from transformers.file_utils import get_full_repo_name
+from transformers.utils import check_min_version, send_example_telemetry
+from transformers.utils.versions import require_version
+
+from flax.training import checkpoints
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+check_min_version("4.27.0.dev0")
+
+require_version("datasets>=1.18.2",
+                "To fix: pip install -r examples/flax/speech-recogintion/requirements.txt")
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+logger = logging.getLogger(__name__)
+
+
+@flax.struct.dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={
+            "help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    feature_extractor_name: Optional[str] = field(
+        default=None, metadata={"help": "feature extractor name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Where to store the pretrained models downloaded from huggingface.co"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={
+            "help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+    dtype: Optional[str] = field(
+        default="float32",
+        metadata={
+            "help": (
+                "Floating-point format in which the model weights should be initialized and trained. Choose one of"
+                " `[float32, float16, bfloat16]`."
+            )
+        },
+    )
+    num_beams: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Number of beams to use for evaluation. This argument will be passed to `model.generate`, "
+                "which is used during evaluation."
+            )
+        },
+    )
+
+
+@flax.struct.dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    dataset_name: str = field(
+        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
+    )
+    dataset_config_name: Optional[str] = field(
+        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
+    )
+    text_column: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "The name of the column in the datasets containing the full texts (for summarization)."},
+    )
+    dataset_cache_dir: Optional[str] = field(
+        default=None, metadata={"help": "Path to cache directory for saving and loading datasets"}
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
+    )
+    preprocessing_num_workers: Optional[int] = field(
+        default=50,
+        metadata={"help": "The number of processes to use for the preprocessing."},
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    audio_column_name: str = field(
+        default="audio",
+        metadata={
+            "help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
+    )
+    text_column_name: str = field(
+        default="text",
+        metadata={
+            "help": "The name of the dataset column containing the text data. Defaults to 'text'"},
+    )
+    max_duration_in_seconds: float = field(
+        default=30.0,
+        metadata={
+            "help": "Filter audio files that are longer than `max_duration_in_seconds` seconds"},
+    )
+    min_duration_in_seconds: float = field(
+        default=0.0,
+        metadata={
+            "help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"},
+    )
+    max_label_length: Optional[int] = field(
+        default=256,
+        metadata={
+            "help": "Truncate transcriptions that are longer `max_label_length` tokens."},
+    )
+    pad_input_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the input sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the inputs to max length."
+        },
+    )
+    pad_target_to_multiple_of: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "If set will pad the target sequence to a multiple of the provided value. "
+            "This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the targets to max length."
+        },
+    )
+    train_split_name: str = field(
+        default="train",
+        metadata={
+            "help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
+        },
+    )
+    eval_split_name: str = field(
+        default="validation",
+        metadata={
+            "help": "The name of the evaluation data set split to use (via the datasets library). Defaults to 'validation'"
+        },
+    )
+    do_lower_case: bool = field(
+        default=False,
+        metadata={"help": "Whether the target text should be lower cased."},
+    )
+    do_remove_punctuation: bool = field(
+        default=False,
+        metadata={
+            "help": "Whether the target text should be striped of punctuation."},
+    )
+    do_normalize_eval: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to normalise the references and predictions in the eval WER calculation."},
+    )
+    language: str = field(
+        default=None,
+        metadata={
+            "help": (
+                "Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning "
+                "only. For English speech recognition, it should be set to `None`."
+            )
+        },
+    )
+    task: str = field(
+        default="transcribe",
+        metadata={
+            "help": "Task, either `transcribe` for speech recognition or `translate` for speech translation."},
+    )
+    num_train_steps: int = field(default=50000, metadata={
+                                 "help": "The number of training steps."})
+    shuffle_buffer_size: Optional[int] = field(
+        default=500,
+        metadata={
+            "help": (
+                "The number of streamed examples to download before shuffling them. The large the buffer, "
+                "the closer it is to real offline shuffling."
+            )
+        },
+    )
+    streaming: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to use streaming mode to load and pre-process the data."},
+    )
+    log_max_eval_predictions: Optional[int] = field(
+        default=0,
+        metadata={
+            "help": (
+                "Number of label and prediction pairs to write to the summary at each evaluation step."
+            )
+        },
+    )
+    log_eval_predictions_fn: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Python path to function for logging evaluation predictions. It can be an external function like fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions, labels)."
+            )
+        },
+    )
+    run_description: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "A longer description of the run/experiment."
+            )
+        },
+    )
+    wandb_entity: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases username or entity (organization name)."
+            )
+        },
+    )
+    wandb_project: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": (
+                "Weights & Biases project to log metrics to."
+            )
+        },
+    )
+
+
+def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:
+    """
+    Shift label ids one token to the right.
+    """
+    shifted_label_ids = np.zeros_like(label_ids)
+    shifted_label_ids[:, 1:] = label_ids[:, :-1]
+    shifted_label_ids[:, 0] = decoder_start_token_id
+
+    return shifted_label_ids
+
+
+@flax.struct.dataclass
+class FlaxDataCollatorSpeechSeq2SeqWithPadding:
+    """
+    Data collator that will dynamically pad the inputs received.
+    Args:
+        processor ([`Wav2Vec2Processor`])
+            The processor used for proccessing the data.
+        decoder_start_token_id (:obj: `int`)
+            The begin-of-sentence of the decoder.
+        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)
+            among:
+            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
+              sequence if provided).
+            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
+              maximum acceptable input length for the model if that argument is not provided.
+            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
+              different lengths).
+        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
+            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).
+            See above for details.
+        max_input_length (:obj:`float`, `optional`):
+            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
+        max_target_length (:obj:`int`, `optional`):
+            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).
+        pad_input_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the input sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+        pad_target_to_multiple_of (:obj:`int`, `optional`):
+            If set will pad the target sequence to a multiple of the provided value.
+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
+            7.5 (Volta).
+    """
+
+    processor: Any
+    decoder_start_token_id: int
+    input_padding: Union[bool, str] = "longest"
+    target_padding: Union[bool, str] = "max_length"
+    max_input_length: Optional[float] = None
+    max_target_length: Optional[int] = None
+    pad_input_to_multiple_of: Optional[int] = None
+    pad_target_to_multiple_of: Optional[int] = None
+
+    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:
+        model_input_name = self.processor.model_input_names[0]
+        input_features = {model_input_name: features[model_input_name]}
+        label_features = {"input_ids": features["labels"]}
+
+        # reformat list to dict and set to pytorch format
+        batch = self.processor.feature_extractor.pad(
+            input_features,
+            max_length=self.max_input_length,
+            padding=self.input_padding,
+            pad_to_multiple_of=self.pad_input_to_multiple_of,
+            return_tensors="np",
+        )
+
+        labels_batch = self.processor.tokenizer.pad(
+            label_features,
+            max_length=self.max_target_length,
+            padding=self.target_padding,
+            pad_to_multiple_of=self.pad_target_to_multiple_of,
+            return_tensors="np",
+        )
+
+        # if bos token is appended in previous tokenization step,
+        # cut bos token here as it's append later anyways
+        labels = labels_batch["input_ids"]
+        if (labels[:, 0] == self.decoder_start_token_id).all().item():
+            labels = labels[:, 1:]
+            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]
+        
+        
+            
+        decoder_input_ids = shift_tokens_right(
+            labels, self.decoder_start_token_id)
+
+        # replace padding with -100 to ignore correctly when computing the loss
+        labels = np.ma.array(labels, mask=np.not_equal(
+            labels_batch.attention_mask, 1))
+        labels = labels.filled(fill_value=-100)
+
+        batch["labels"] = labels
+        batch["decoder_input_ids"] = decoder_input_ids
+        batch["attention_mask"] = labels_batch.attention_mask  # Add attention_mask to the batch
+        
+        return batch
+
+
+def load_maybe_streaming_dataset(dataset_name, dataset_config_name, split="train", streaming=True, **kwargs):
+    """
+    Utility function to load a dataset in streaming mode. For datasets with multiple splits,
+    each split is loaded individually and then splits combined by taking alternating examples from
+    each (interleaving).
+    """
+    if "+" in split:
+        # load multiple splits separated by the `+` symbol with streaming mode
+        dataset_splits = [
+            load_dataset(dataset_name, dataset_config_name,
+                         split=split_name, streaming=streaming, **kwargs)
+            for split_name in split.split("+")
+        ]
+        # interleave multiple splits to form one dataset
+        interleaved_dataset = interleave_datasets(dataset_splits)
+        return interleaved_dataset
+    else:
+        # load a single split *with* streaming mode
+        dataset = load_dataset(
+            dataset_name, dataset_config_name, split=split, streaming=streaming, **kwargs)
+        return dataset
+
+
+def collate_batch(samples):
+    return {key: [feature[key] for feature in samples] for key in samples[0]}
+
+def data_loader(
+    dataset: Dataset,
+    batch_size: int,
+    drop_last: bool=True,
+    num_workers: int=0,
+) -> Generator:
+    """
+    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,
+    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.
+    """
+    data_loader_iterator = iter(torch.utils.data.DataLoader(
+        batch_size=batch_size,
+        dataset=dataset.with_format("torch"),
+        num_workers=num_workers,
+        collate_fn=collate_batch,
+        drop_last=drop_last,
+    ))
+    return data_loader_iterator
+
+
+class TrainState(train_state.TrainState):
+    dropout_rng: jnp.ndarray
+
+    def replicate(self):
+        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))
+
+
+def create_learning_rate_fn(
+    num_train_steps: int, num_warmup_steps: int, learning_rate: float, start_step: int=0, warmup_init_value: float=0.0, decay_end_value: float=0.0,
+) -> Callable[[int], jnp.array]:
+    """Returns a linear warmup, linear_decay learning rate function."""
+    warmup_fn = optax.linear_schedule(
+        init_value=warmup_init_value, end_value=learning_rate, transition_steps=num_warmup_steps)
+    decay_fn = optax.linear_schedule(
+        init_value=learning_rate, end_value=decay_end_value, transition_steps=num_train_steps - num_warmup_steps
+    )
+    schedule_fn = optax.join_schedules(
+        schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])
+    
+    def learning_rate_fn(step: int) -> jnp.array:
+        return schedule_fn(step + start_step)
+    
+    return learning_rate_fn
+
+
+def main():
+    # Parse input arguments
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+    parser = HfArgumentParser(
+        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
+
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args = parser.parse_json_file(
+            json_file=os.path.abspath(sys.argv[1]))
+    else:
+        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
+    # information sent is the one passed as arguments along with your JAX/Flax versions.
+    send_example_telemetry("run_speech_recognition_seq2seq",
+                           model_args, data_args, framework="flax")
+
+    # Setup logging
+    # Make one log on every process with the configuration for debugging.
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+    # Set the verbosity to info of the Transformers logger.
+    # We only want one process per machine to log things on the screen.
+
+    # logger.setLevel(logging.INFO if jax.local_devices()[0].id%jax.local_device_count() == 0 else logging.ERROR)
+
+    # logger.setLevel(logging.INFO if jax.process_index()
+    #                == 0 else logging.ERROR)
+    
+    # Number of hosts
+    num_of_hosts = jax.process_count()
+    current_host_idx = jax.process_index()
+
+    if current_host_idx == 0:
+        datasets.utils.logging.set_verbosity_warning()
+        transformers.utils.logging.set_verbosity_info()
+    else:
+        datasets.utils.logging.set_verbosity_error()
+        transformers.utils.logging.set_verbosity_error()
+    
+    logger.setLevel(logging.INFO)
+    logger.info("Training/evaluation parameters %s", training_args)
+
+    if num_of_hosts and not training_args.push_to_hub:
+        logger.warning(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --push_to_hub to be able to save checkpoints to the hub."
+        )
+    if num_of_hosts and not training_args.overwrite_output_dir and training_args.resume_from_checkpoint:
+        logger.error(
+            f"If you are on a TPU Pod or a multinode setup, you need to set --overwrite_output_dir to be able to resume from a pushed checkpoint."
+        )
+        sys.exit(1)
+
+    # Check the output dir is valid
+    if os.path.exists(training_args.output_dir):
+        if (
+            os.listdir(training_args.output_dir)
+            and training_args.do_train
+            and not training_args.overwrite_output_dir
+        ):
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use `--overwrite_output_dir` to overcome."
+            )
+        elif training_args.overwrite_output_dir:
+            logger.warning(f"Removing path {training_args.output_dir}")
+            shutil.rmtree(training_args.output_dir)
+      
+    # Handle the repository creation
+    output_dir = Path(training_args.output_dir)
+    if training_args.push_to_hub:
+        if training_args.hub_model_id is None:
+            repo_name = get_full_repo_name(
+                output_dir.absolute().name,
+                token=training_args.hub_token,
+                organization=training_args.push_to_hub_organization,
+            )
+        else:
+            repo_name = training_args.hub_model_id
+         
+        repo_url = None  
+        while not repo_url:
+            # Workaround for an internal HuggingFace error if the repo is being created by another worker
+            try:
+                repo_url = create_repo(
+                    repo_name, exist_ok=True, token=training_args.hub_token, private=training_args.hub_private_repo
+                )
+            except:
+                time.sleep(1)
+
+        repo = Repository(training_args.output_dir,
+                          clone_from=repo_name, token=training_args.hub_token)
+
+    # Set the model_name_or_path
+    model_name_or_path = model_args.model_name_or_path
+
+    # Try to detect last checkpoint and continue if possible
+    training_state = {"step": 0, "eval_lines": []}
+    if training_args.resume_from_checkpoint:
+        if (output_dir / "flax_model.msgpack").exists() and (output_dir / "training_state.bin").exists():
+            training_state = json.loads((output_dir / "training_state.bin").read_text())
+            model_name_or_path = os.path.join(training_args.output_dir)
+            logger.info(
+                f"Checkpoint detected, resuming training from {training_args.output_dir} at step {training_state['step']}."
+            )
+        else:
+            logger.info(
+                f"No valid checkpoint found in {training_args.output_dir}. Starting from {model_name_or_path}."
+            )
+    
+    
+    # Load dataset
+    raw_datasets = IterableDatasetDict() if data_args.streaming else DatasetDict()
+
+    if training_args.do_train:
+        raw_datasets["train"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.train_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if training_args.do_eval:
+        raw_datasets["eval"] = load_maybe_streaming_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            split=data_args.eval_split_name,
+            cache_dir=data_args.dataset_cache_dir,
+            streaming=data_args.streaming,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if not training_args.do_train and not training_args.do_eval:
+        raise ValueError(
+            "Cannot not train and not do evaluation. At least one of training or evaluation has to be performed."
+        )
+
+    raw_datasets_features = list(
+        next(iter(raw_datasets.values())).features.keys())
+
+    if data_args.audio_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--audio_column_name` to the correct audio column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    if data_args.text_column_name not in raw_datasets_features:
+        raise ValueError(
+            f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
+            "Make sure to set `--text_column_name` to the correct text column - one of "
+            f"{', '.join(raw_datasets_features)}."
+        )
+
+    # Load pretrained model, tokenizer, and feature extractor
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    feature_extractor = AutoFeatureExtractor.from_pretrained(
+        model_args.feature_extractor_name if model_args.feature_extractor_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(
+        model_name_or_path,
+        config=config,
+        dtype=getattr(jnp, model_args.dtype),
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+    logger.info(
+        f"Successfully loaded the model '{model_name_or_path}'."
+    )
+    
+    if model.config.decoder_start_token_id is None:
+        raise ValueError(
+            "Make sure that `config.decoder_start_token_id` is correctly defined")
+
+    # Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,
+    # so we just need to set the correct target sampling rate.
+    dataset_sampling_rate = next(
+        iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
+
+    if dataset_sampling_rate != feature_extractor.sampling_rate:
+        raw_datasets = raw_datasets.cast_column(
+            data_args.audio_column_name, datasets.features.Audio(
+                sampling_rate=feature_extractor.sampling_rate)
+        )
+
+    # Preprocessing the datasets.
+    # We need to read the audio files as arrays and tokenize the targets.
+    max_input_length = int(
+        data_args.max_duration_in_seconds * feature_extractor.sampling_rate)
+    min_input_length = int(
+        data_args.min_duration_in_seconds * feature_extractor.sampling_rate)
+    max_label_length = (
+        data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length
+    )
+    pad_input_to_multiple_of = data_args.pad_input_to_multiple_of
+    pad_target_to_multiple_of = data_args.pad_target_to_multiple_of
+    audio_column_name = data_args.audio_column_name
+    num_workers = data_args.preprocessing_num_workers
+    text_column_name = data_args.text_column_name
+    model_input_name = feature_extractor.model_input_names[0]
+    do_lower_case = data_args.do_lower_case
+    do_remove_punctuation = data_args.do_remove_punctuation
+    normalizer = BasicTextNormalizer()  # 'official' text normalizer from OpenAI
+
+    if data_args.language is not None:
+        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)
+        tokenizer.set_prefix_tokens(
+            language=data_args.language, task=data_args.task)
+    
+    
+    def prepare_dataset(batch):
+        # Process audio
+        sample = batch[audio_column_name]
+        inputs = feature_extractor(
+            sample["array"], sampling_rate=sample["sampling_rate"])
+        # Process audio length
+        batch[model_input_name] = inputs.get(model_input_name)[0]
+        batch["input_length"] = len(sample["array"])
+
+        # Process targets
+        input_str = batch[text_column_name].lower(
+        ) if do_lower_case else batch[text_column_name]
+        if do_remove_punctuation:
+            input_str = normalizer(input_str).strip()
+        batch["labels"] = tokenizer(input_str,truncation=True, max_length=max_label_length).input_ids
+        return batch
+
+    with training_args.main_process_first(desc="dataset map pre-processing"):
+        vectorized_datasets = raw_datasets.map(
+            prepare_dataset,
+            remove_columns=raw_datasets_features,
+        )
+
+    # Filter training data with inputs longer than max_input_length
+    def is_audio_in_length_range(length):
+        return min_input_length < length < max_input_length
+
+    if training_args.do_train:
+        vectorized_datasets["train"] = vectorized_datasets["train"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    if training_args.do_eval:
+        vectorized_datasets["eval"] = vectorized_datasets["eval"].filter(
+            is_audio_in_length_range,
+            input_columns=["input_length"],
+        )
+
+    # Load metrics and write stats
+    metric_wer = evaluate.load("wer")
+    metric_cer = evaluate.load("cer")
+    do_normalize_eval = data_args.do_normalize_eval
+
+    def compute_metrics(pred_ids, label_ids, return_preds_labels=False):
+        # Replace padded labels by the padding token
+        for idx in range(len(label_ids)):
+            label_ids[idx][label_ids[idx] == -100] = tokenizer.pad_token_id
+
+        predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
+        # We do not want to group tokens when computing the metrics
+        labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
+
+        if do_normalize_eval:
+            pred_str = [normalizer(pred) for pred in predictions]
+            label_str = [normalizer(label) for label in labels]
+            # Filtering step to only evaluate the samples that correspond to non-zero references:
+            pred_str = [pred_str[i]
+                        for i in range(len(pred_str)) if len(label_str[i]) > 0]
+            label_str = [label_str[i]
+                         for i in range(len(label_str)) if len(label_str[i]) > 0]
+        else:
+            pred_str = predictions
+            label_str = labels
+
+        wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)
+        cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)
+
+        if return_preds_labels:
+            return {"wer": wer, "cer": cer}, predictions, labels
+        else:
+            return {"wer": wer, "cer": cer}
+
+    def update_training_state(training_state, train_metrics, eval_metrics, step):
+        safe_value = lambda x: float(x.tolist() if isinstance(x, jnp.ndarray) else x)
+        state = {"step": step}
+        eval_lines = training_state["eval_lines"]
+       
+        train_metrics = get_metrics(train_metrics)
+        train_metrics_dict = {}
+        for metric_name, values in train_metrics.items():
+            tag = f"train_{metric_name}"
+            for i, value in enumerate(values):
+                train_metrics_dict[step - len(values) + i + 1] = {tag: safe_value(value)}
+
+        eval_metrics_dict = {}
+        for metric_name, value in eval_metrics.items():
+            tag = f"eval_{metric_name}"
+            eval_metrics_dict.update({
+                "step": step,
+                tag: safe_value(value),
+            })
+            if step in train_metrics_dict:
+                eval_metrics_dict.update(train_metrics_dict[step])
+        eval_lines.append(eval_metrics_dict)
+        return {**state, "eval_lines": eval_lines}
+
+    def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=None, labels=None):
+        summary_writer.scalar("train_time", train_time, step)
+
+        train_metrics = get_metrics(train_metrics)
+        for key, vals in train_metrics.items():
+            tag = f"train_{key}"
+            for i, val in enumerate(vals):
+                summary_writer.scalar(tag, val, step - len(vals) + i + 1)
+
+        for metric_name, value in eval_metrics.items():
+            summary_writer.scalar(f"eval_{metric_name}", value, step)
+        
+        # Log evaluation predictions
+        if predictions and labels:
+            df = pd.DataFrame({
+                "references": labels,
+                "predictions": predictions,
+            })
+            df["wer"] = df.apply(lambda row: metric_wer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            df["cer"] = df.apply(lambda row: metric_cer.compute(predictions=[row["predictions"]], references=[row["references"]]), axis=1)
+            markdown_table = df.to_markdown(index=False)
+            eval_metrics_table = pd.DataFrame.from_dict([{"step": step, **eval_metrics}]).to_markdown(index=False)
+            summary_writer.text("eval_predictions", eval_metrics_table + "\n\n" + markdown_table, step)
+            # External logging function
+            if data_args.log_eval_predictions_fn:
+                module, fname = data_args.log_eval_predictions_fn.rsplit('.', 1)
+                fn = getattr(import_module(module), fname)
+                fn(summary_writer, train_metrics, eval_metrics, train_time, step, predictions=predictions, labels=labels, training_args=training_args)
+
+    # Save feature extractor, tokenizer and config
+    feature_extractor.save_pretrained(training_args.output_dir)
+    tokenizer.save_pretrained(training_args.output_dir)
+    config.save_pretrained(training_args.output_dir)
+
+    processor = AutoProcessor.from_pretrained(training_args.output_dir)
+
+    data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(
+        processor=processor,
+        decoder_start_token_id=model.config.decoder_start_token_id,
+        input_padding="longest",
+        target_padding="longest",
+        max_target_length=max_label_length,
+        pad_input_to_multiple_of=pad_input_to_multiple_of,
+        pad_target_to_multiple_of=pad_target_to_multiple_of if pad_target_to_multiple_of else max_label_length,
+    )
+
+    # Enable tensorboard only on the master node
+    has_tensorboard = is_tensorboard_available()
+    if has_tensorboard and current_host_idx == 0:
+        try:
+            # TODO: Decouple wandb from tensorboard
+            import wandb
+
+            has_wandb = True
+        except ImportError:
+            has_wandb = False
+            if data_args.wandb_entity is not None or data_args.wandb_project is not None:
+                logger.warning(
+                    f"Unable to display metrics through Weights & Biases because some packages are not installed: {ie}"
+                )
+        try:
+            if has_wandb:
+                wandb.tensorboard.patch(root_logdir=output_dir / "runs")
+                wandb.init(
+                    entity=data_args.wandb_entity,
+                    project=data_args.wandb_project,
+                    name=training_args.run_name,
+                    notes=data_args.run_description,
+                    save_code=True,
+                    sync_tensorboard=True,
+                )
+                wandb.config.update(training_args)
+                wandb.config.update(model_args)
+                wandb.config.update(data_args)
+            from flax.metrics.tensorboard import SummaryWriter
+
+            summary_writer = SummaryWriter(
+                log_dir=output_dir / "runs" / f"{datetime.now():%b%d_%H-%M-%S}_{socket.gethostname()}")
+        except ImportError as ie:
+            has_tensorboard = False
+            logger.warning(
+                f"Unable to display metrics through TensorBoard because some packages are not installed: {ie}"
+            )
+    else:
+        logger.warning(
+            "Unable to display metrics through TensorBoard because the package is not installed: "
+            "Please run pip install tensorboard to enable."
+        )
+
+    # Initialize our training
+    rng = jax.random.PRNGKey(training_args.seed)
+    rng, dropout_rng = jax.random.split(rng)
+
+    # Store some constant
+    train_batch_size = int(
+        training_args.per_device_train_batch_size) * jax.device_count()
+    eval_batch_size = int(
+        training_args.per_device_eval_batch_size) * jax.device_count()
+
+    # Create learning rate schedule
+    lr_scheduler_types = {"linear", "constant", "constant_with_warmup"}
+    if training_args.lr_scheduler_type not in lr_scheduler_types:
+        raise ValueError(
+            f"lr_scheduler_type of type {training_args.lr_scheduler_type} not supported, choose from {lr_scheduler_types}."
+        )
+    elif training_args.lr_scheduler_type == "constant":
+        warmup_init_value = training_args.learning_rate
+        decay_end_value = training_args.learning_rate
+    elif training_args.lr_scheduler_type == "constant_with_warmup":
+        warmup_init_value = 0.0
+        decay_end_value = training_args.learning_rate
+    else:
+        warmup_init_value = 0.0
+        decay_end_value = 0.0
+        
+    linear_decay_lr_schedule_fn = create_learning_rate_fn(
+        data_args.num_train_steps,
+        training_args.warmup_steps,
+        training_args.learning_rate,
+        start_step=training_state["step"],
+        warmup_init_value=warmup_init_value,
+        decay_end_value=decay_end_value
+    )
+    
+    # We use Optax's "masking" functionality to not apply weight decay
+    # to bias and LayerNorm scale parameters. decay_mask_fn returns a
+    # mask boolean with the same structure as the parameters.
+    # The mask is True for parameters that should be decayed.
+    def decay_mask_fn(params):
+        flat_params = traverse_util.flatten_dict(params)
+        # Find out all LayerNorm parameters
+        layer_norm_candidates = ["layernorm", "layer_norm", "ln"]
+        layer_norm_named_params = set(
+            [
+                layer[-2:]
+                for layer_norm_name in layer_norm_candidates
+                for layer in flat_params.keys()
+                if layer_norm_name in "".join(layer).lower()
+            ]
+        )
+        flat_mask = {path: (path[-1] != "bias" and path[-2:]
+                            not in layer_norm_named_params) for path in flat_params}
+        return traverse_util.unflatten_dict(flat_mask)
+    
+    # Create adam optimizer
+    adamw = optax.adamw(
+        learning_rate=linear_decay_lr_schedule_fn,
+        b1=training_args.adam_beta1,
+        b2=training_args.adam_beta2,
+        eps=training_args.adam_epsilon,
+        weight_decay=training_args.weight_decay,
+        mask=decay_mask_fn,
+    )
+
+    # Setup train state
+    state = TrainState.create(
+        apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)
+
+    # Label smoothed cross entropy
+    def loss_fn(logits, labels, label_smoothing_factor=0.0):
+        """
+        The label smoothing implementation is adapted from Flax's official example:
+        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104
+        """
+        vocab_size = logits.shape[-1]
+        confidence = 1.0 - label_smoothing_factor
+        low_confidence = (1.0 - confidence) / (vocab_size - 1)
+        normalizing_constant = -(
+            confidence * jnp.log(confidence) + (vocab_size - 1) *
+            low_confidence * jnp.log(low_confidence + 1e-20)
+        )
+        soft_labels = onehot(labels, vocab_size,
+                             on_value=confidence, off_value=low_confidence)
+
+        loss = optax.softmax_cross_entropy(logits, soft_labels)
+        loss = loss - normalizing_constant
+
+        # Ignore padded tokens from loss, i.e. where labels are not set to -100
+        padding_mask = labels >= 0
+        loss = loss * padding_mask
+        loss = loss.sum()
+        num_labels = padding_mask.sum()
+        return loss, num_labels
+
+    # Define gradient update step fn
+    def train_step(state, batch, label_smoothing_factor=0.0):
+        
+        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)
+
+        def compute_loss(params):
+            labels = batch.pop("labels")
+            logits = state.apply_fn(
+                **batch, params=params, dropout_rng=dropout_rng, train=True)[0]
+            loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+            return loss, num_labels
+
+        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)
+        (loss, num_labels), grad = grad_fn(state.params)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        # True grad = total grad / total samples
+        grad = jax.lax.psum(grad, "batch")
+        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)
+        new_state = state.apply_gradients(
+            grads=grad, dropout_rng=new_dropout_rng)
+
+        metrics = {"loss": loss,
+                   "learning_rate": linear_decay_lr_schedule_fn(state.step)}
+
+        return new_state, metrics
+
+    # Define eval fn
+    def eval_step(params, batch, label_smoothing_factor=0.0):
+        labels = batch.pop("labels")
+        logits = model(**batch, params=params, train=False)[0]
+
+        loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)
+        num_labels = jax.lax.psum(num_labels, "batch")
+
+        # True loss = total loss / total samples
+        loss = jax.lax.psum(loss, "batch")
+        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)
+
+        metrics = {"loss": loss}
+        return metrics
+
+    # Define generation function
+    num_beams = model_args.num_beams if model_args.num_beams is not None else model.config.num_beams
+    gen_kwargs = {"max_length": max_label_length, "num_beams": num_beams}
+
+     
+    def generate_step(params, batch):
+        model.params = params
+        
+        attention_mask = batch.get("attention_mask")
+        
+        #if attention_mask is not None:
+        output_ids = model.generate(batch[model_input_name], attention_mask=attention_mask, **gen_kwargs)
+        #else:
+        #    output_ids = model.generate(batch[model_input_name], **gen_kwargs)
+        
+        return output_ids.sequences
+
+    # Create parallel version of the train and eval step
+    p_train_step = jax.pmap(
+        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch", donate_argnums=(0, )
+    )
+    p_eval_step = jax.pmap(partial(
+        eval_step, label_smoothing_factor=training_args.label_smoothing_factor), "batch")
+    p_generate_step = jax.pmap(generate_step, "batch")
+
+    # Replicate the train state on each device
+    state = state.replicate()
+    
+    # Logging
+    logger.info("***** Running training *****")
+    logger.info(
+        f"  Dataset name = {data_args.dataset_name}")
+    logger.info(
+        f"  Dataset config name = {data_args.dataset_config_name}")
+    logger.info(
+        f"  Learning rate = {training_args.learning_rate}")
+    logger.info(
+        f"  Scheduler = {training_args.lr_scheduler_type}")
+    logger.info(
+        f"  Num examples = {data_args.num_train_steps * train_batch_size}")
+    if num_of_hosts > 1:
+        logger.info(
+            f"  Number of hosts = {num_of_hosts}")
+        logger.info(
+            f"  Current host idx = {current_host_idx}")
+    logger.info(
+        f"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}")
+    logger.info(
+        f"  Total train batch size per node (w. parallel & distributed) = {train_batch_size // num_of_hosts}")
+    logger.info(
+        f"  Total train batch size (w. parallel & distributed) = {train_batch_size}")
+    logger.info(f"  Total optimization steps = {data_args.num_train_steps - training_state['step']}")
+    if training_state['step'] > 0:
+        logger.info(f"  ↪ Starting at {str(training_state['step'])} and finishing at {str(data_args.num_train_steps)}")
+
+    train_time = 0
+
+    # Training summary
+    language_code = None  # Maybe 'multilingual'?
+    if data_args.language is not None:
+        language = data_args.language.lower()
+        if language in TO_LANGUAGE_CODE:
+            language_code = TO_LANGUAGE_CODE[language]
+        elif len(language) == 2:
+            language_code = language
+    training_summary = {
+        "model_name": repo_name.split("/")[-1],
+        "language": language_code,
+        "tags": ["audio", "asr", "automatic-speech-recognition", "hf-asr-leaderboard"],
+        "license": "apache-2.0",
+        "finetuned_from": model_args.model_name_or_path,
+        "tasks": ["asr"],
+        "dataset": data_args.dataset_name,
+        "dataset_args": {"name": data_args.dataset_config_name},
+        "source": "flax",
+        "eval_lines": [],
+        "eval_results": None,
+        "hyperparameters": {
+            "learning_rate": training_args.learning_rate,
+            "lr_scheduler_type": training_args.lr_scheduler_type,
+            "per_device_train_batch_size": training_args.per_device_train_batch_size,
+            "total_train_batch_size_per_node": train_batch_size // num_of_hosts,
+            "total_train_batch_size": train_batch_size,
+            "total_optimization_steps": data_args.num_train_steps - training_state['step'],
+            "starting_optimization_step": training_state['step'] if training_state['step'] > 0 else None,
+            "finishing_optimization_step": data_args.num_train_steps,
+            "num_train_dataset_workers": f"{num_workers}",
+            "total_num_training_examples": data_args.num_train_steps * train_batch_size,
+        },
+        # TODO: Adapt https://github.com/huggingface/transformers/blob/main/src/transformers/modelcard.py#L855
+        # "hyperparameters": training_args.to_sanitized_dict()
+    }
+    
+    # Create README if it does not exist
+    readme = output_dir / "README.md"
+    if not readme.exists():
+        readme.write_text(TrainingSummary(**training_summary).to_model_card())
+    
+    # ======================== Training ================================
+    train_start = time.time()
+
+    train_metrics = []
+    epoch = 0
+    train_dataset = vectorized_datasets["train"].shuffle(seed=training_args.seed, buffer_size=data_args.shuffle_buffer_size)
+    
+    # Split by node
+    train_dataset = split_dataset_by_node(train_dataset, rank=current_host_idx, world_size=num_of_hosts)   
+    
+    if train_dataset.n_shards < data_args.preprocessing_num_workers:
+        num_workers = train_dataset.n_shards
+
+    logger.info(f"  Number of train dataset workers = {num_workers} {'(Capped by the number of dataset shards)' if train_dataset.n_shards < data_args.preprocessing_num_workers else ''} {'(ADVICE: In most cases you will speed up training considerably if you increase the value of --preprocessing_num_workers!)' if num_workers < 10 else ''}")
+ 
+    eval_dataset = vectorized_datasets["eval"]
+    train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+    
+    if not training_args.ignore_data_skip and training_state["step"] > 0:
+        logger.info(
+            f"  Will skip the first {training_state['step']} steps. If this takes a lot of time,"
+            " you can add the `--ignore_data_skip` flag to your launch command, but you will resume the"
+            " training on data already seen by your model."
+        )
+        for step in tqdm(range(training_state["step"]), desc=f"Skipping data for {training_state['step']} steps...", position=1, leave=False):
+            try:
+                samples = next(train_loader)
+            except StopIteration:
+                epoch += 1
+                train_dataset.set_epoch(epoch)
+                train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+                samples = next(train_loader)
+            batch = data_collator(samples)
+            # batch = shard(batch.data)
+    
+    
+    for step in tqdm(range(data_args.num_train_steps), desc="Training...", position=1, leave=False):
+        # Skip initial steps if these are specified. 
+        if step < training_state["step"]:
+            continue
+        
+        # =========================== Training ===========================
+        try:
+            samples = next(train_loader)
+        except StopIteration:
+            epoch += 1
+            train_dataset.set_epoch(epoch)
+            train_loader = data_loader(train_dataset, train_batch_size // num_of_hosts, num_workers=num_workers)
+            samples = next(train_loader)
+            logger.info(
+                f"Completed epoch ({epoch} | Loss: {train_metric['loss']}, Learning Rate:"
+                f" {train_metric['learning_rate']})"
+            )
+        
+        batch = data_collator(samples)
+        batch = shard(batch.data)
+        state, train_metric = p_train_step(state, batch)
+        train_metrics.append(train_metric)
+                
+        train_time += time.time() - train_start
+        train_metric = unreplicate(train_metric)
+
+        # ========================== Evaluating ==========================
+        # Evaluate at each eval_steps, and at the end of training at num_train_steps
+        if step % training_args.eval_steps == 0 or step == data_args.num_train_steps - 1:
+            logger.info(
+                f"Starting evaluation at step {step} of num_training_step {data_args.num_train_steps} steps. Planned evaluation every {training_args.eval_steps} steps." 
+            )
+            eval_metrics = []
+            eval_preds = []
+            eval_labels = []
+            eval_loader = data_loader(eval_dataset, eval_batch_size, drop_last=False)
+            if data_args.max_eval_samples:
+                max_eval_steps_iter = range(1 + data_args.max_eval_samples // eval_batch_size)
+            else:
+                max_eval_steps_iter = itertools.repeat(None)
+            for _ in tqdm(max_eval_steps_iter, desc="Evaluating...", position=2, leave=False):
+                # Model forward
+                try:
+                    samples = next(eval_loader)
+                except StopIteration:
+                    break
+                batch = data_collator(samples)
+                
+                labels = batch["labels"]
+
+                metrics = pad_shard_unpad(p_eval_step, static_return=True)(
+                    state.params, batch.data, min_device_batch=training_args.per_device_eval_batch_size
+                )
+                eval_metrics.append(metrics)
+
+                # Generation
+                if training_args.predict_with_generate:
+                    generated_ids = pad_shard_unpad(
+                        p_generate_step)(state.params, batch.data)
+                    eval_preds.extend(jax.device_get(
+                        generated_ids.reshape(-1, gen_kwargs["max_length"])))
+                    eval_labels.extend(labels)
+
+            # Normalize eval metrics
+            eval_metrics = get_metrics(eval_metrics)
+            eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)
+
+            # Compute metrics
+            metric_desc = ""
+            if training_args.predict_with_generate:
+                metric_values, pred_str, label_str = compute_metrics(
+                    eval_preds, eval_labels, return_preds_labels=True
+                )
+                eval_metrics.update(metric_values)
+                metric_desc = " | ".join(
+                    [f"Eval {key}: {value}" for key, value in metric_values.items()])
+
+            # Print metrics
+            desc = f"Step: {step} | Epoch: {epoch} (Eval Loss: {eval_metrics['loss']} | {metric_desc})"
+            logger.info(desc)
+
+            # Update training state
+            training_state = update_training_state(
+                training_state,
+                train_metrics,
+                eval_metrics,
+                step,
+            )
+
+            # Save metrics
+            if has_tensorboard and current_host_idx == 0:
+                log_max_predictions = data_args.log_max_eval_predictions if data_args.log_max_eval_predictions else 0
+                write_metric(
+                    summary_writer,
+                    train_metrics,
+                    eval_metrics,
+                    train_time,
+                    step,
+                    predictions=pred_str[:log_max_predictions],
+                    labels=label_str[:log_max_predictions]
+                )
+
+            # Save checkpoint at each eval_steps and push checkpoint to the hub
+            if current_host_idx  == 0:
+                params = jax.device_get(
+                    jax.tree_util.tree_map(lambda x: x[0], state.params))
+                model.save_pretrained(training_args.output_dir, params=params)
+                tokenizer.save_pretrained(training_args.output_dir)
+                # Report eval results if training is done
+                if step == data_args.num_train_steps - 1:
+                    training_summary["eval_results"] = training_state["eval_lines"][-1]
+                else:
+                    training_summary.update({"eval_lines": training_state["eval_lines"]})
+                (output_dir / "training_state.bin").write_text(json.dumps(training_state))
+                # Write model card
+                readme.write_text(TrainingSummary(**training_summary).to_model_card())
+                if training_args.push_to_hub:
+                    repo.push_to_hub(
+                        commit_message=f"Saving weights and logs of step {step} - epoch {epoch}", blocking=False)
+
+if __name__ == "__main__":
+    main()
diff --git a/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/config.yaml b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/config.yaml
new file mode 100644
index 0000000..db4d612
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/config.yaml
@@ -0,0 +1,512 @@
+wandb_version: 1
+
+__cached__setup_devices:
+  desc: null
+  value: cpu
+_n_gpu:
+  desc: null
+  value: 0
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.14.0
+    code_path: code/run_flax_speech_recognition_seq2seq_streaming_debug.py
+    framework: huggingface
+    huggingface_version: 4.28.0.dev0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.10
+    start_time: 1681322733.820935
+    t:
+      1:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      2:
+      - 1
+      - 2
+      - 3
+      - 5
+      - 11
+      - 12
+      - 45
+      - 49
+      - 51
+      - 53
+      - 55
+      3:
+      - 13
+      - 23
+      - 34
+      4: 3.8.10
+      5: 0.14.0
+      6: 4.28.0.dev0
+      8:
+      - 5
+adafactor:
+  desc: null
+  value: false
+adam_beta1:
+  desc: null
+  value: 0.9
+adam_beta2:
+  desc: null
+  value: 0.999
+adam_epsilon:
+  desc: null
+  value: 1.0e-08
+audio_column_name:
+  desc: null
+  value: audio
+auto_find_batch_size:
+  desc: null
+  value: false
+bf16:
+  desc: null
+  value: false
+bf16_full_eval:
+  desc: null
+  value: false
+cache_dir:
+  desc: null
+  value: null
+config_name:
+  desc: null
+  value: null
+data_seed:
+  desc: null
+  value: null
+dataloader_drop_last:
+  desc: null
+  value: false
+dataloader_num_workers:
+  desc: null
+  value: 0
+dataloader_pin_memory:
+  desc: null
+  value: true
+dataset_cache_dir:
+  desc: null
+  value: null
+dataset_config_name:
+  desc: null
+  value: null
+dataset_name:
+  desc: null
+  value: NbAiLab/NCC_speech_all_v5
+ddp_bucket_cap_mb:
+  desc: null
+  value: null
+ddp_find_unused_parameters:
+  desc: null
+  value: null
+ddp_timeout:
+  desc: null
+  value: 1800
+debug:
+  desc: null
+  value: []
+deepspeed:
+  desc: null
+  value: null
+disable_tqdm:
+  desc: null
+  value: false
+do_eval:
+  desc: null
+  value: true
+do_lower_case:
+  desc: null
+  value: false
+do_normalize_eval:
+  desc: null
+  value: true
+do_predict:
+  desc: null
+  value: false
+do_remove_punctuation:
+  desc: null
+  value: false
+do_train:
+  desc: null
+  value: true
+dtype:
+  desc: null
+  value: bfloat16
+eval_accumulation_steps:
+  desc: null
+  value: null
+eval_delay:
+  desc: null
+  value: 0
+eval_split_name:
+  desc: null
+  value: validation
+eval_steps:
+  desc: null
+  value: 10000
+evaluation_strategy:
+  desc: null
+  value: IntervalStrategy.NO
+feature_extractor_name:
+  desc: null
+  value: null
+fp16:
+  desc: null
+  value: false
+fp16_backend:
+  desc: null
+  value: auto
+fp16_full_eval:
+  desc: null
+  value: false
+fp16_opt_level:
+  desc: null
+  value: O1
+fsdp:
+  desc: null
+  value: []
+fsdp_config:
+  desc: null
+  value:
+    fsdp_min_num_params: 0
+    xla: false
+    xla_fsdp_grad_ckpt: false
+fsdp_min_num_params:
+  desc: null
+  value: 0
+fsdp_transformer_layer_cls_to_wrap:
+  desc: null
+  value: null
+full_determinism:
+  desc: null
+  value: false
+generation_config:
+  desc: null
+  value: null
+generation_max_length:
+  desc: null
+  value: null
+generation_num_beams:
+  desc: null
+  value: null
+gradient_accumulation_steps:
+  desc: null
+  value: 1
+gradient_checkpointing:
+  desc: null
+  value: false
+greater_is_better:
+  desc: null
+  value: null
+group_by_length:
+  desc: null
+  value: false
+half_precision_backend:
+  desc: null
+  value: auto
+hub_model_id:
+  desc: null
+  value: NbAiLab/scream_large_oct_debug_128seq
+hub_private_repo:
+  desc: null
+  value: true
+hub_strategy:
+  desc: null
+  value: HubStrategy.EVERY_SAVE
+hub_token:
+  desc: null
+  value: null
+ignore_data_skip:
+  desc: null
+  value: true
+include_inputs_for_metrics:
+  desc: null
+  value: false
+jit_mode_eval:
+  desc: null
+  value: false
+label_names:
+  desc: null
+  value: null
+label_smoothing_factor:
+  desc: null
+  value: 0.0
+language:
+  desc: null
+  value: Norwegian
+learning_rate:
+  desc: null
+  value: 5.0e-06
+length_column_name:
+  desc: null
+  value: length
+load_best_model_at_end:
+  desc: null
+  value: false
+local_rank:
+  desc: null
+  value: -1
+log_eval_predictions_fn:
+  desc: null
+  value: log_predictions.write_predictions
+log_level:
+  desc: null
+  value: passive
+log_level_replica:
+  desc: null
+  value: warning
+log_max_eval_predictions:
+  desc: null
+  value: 100
+log_on_each_node:
+  desc: null
+  value: true
+logging_dir:
+  desc: null
+  value: ../../scream_large_oct_debug_128seq/runs/Apr12_18-04-46_t1v-n-0a06f6ef-w-0
+logging_first_step:
+  desc: null
+  value: false
+logging_nan_inf_filter:
+  desc: null
+  value: true
+logging_steps:
+  desc: null
+  value: 500
+logging_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+lr_scheduler_type:
+  desc: null
+  value: SchedulerType.LINEAR
+max_duration_in_seconds:
+  desc: null
+  value: 30.0
+max_eval_samples:
+  desc: null
+  value: null
+max_grad_norm:
+  desc: null
+  value: 1.0
+max_label_length:
+  desc: null
+  value: 128
+max_steps:
+  desc: null
+  value: -1
+max_train_samples:
+  desc: null
+  value: null
+metric_for_best_model:
+  desc: null
+  value: null
+min_duration_in_seconds:
+  desc: null
+  value: 0.0
+model_name_or_path:
+  desc: null
+  value: openai/whisper-large-v2
+model_revision:
+  desc: null
+  value: main
+mp_parameters:
+  desc: null
+  value: ''
+no_cuda:
+  desc: null
+  value: false
+num_beams:
+  desc: null
+  value: 5
+num_train_epochs:
+  desc: null
+  value: 3.0
+num_train_steps:
+  desc: null
+  value: 50000
+optim:
+  desc: null
+  value: OptimizerNames.ADAMW_HF
+optim_args:
+  desc: null
+  value: null
+output_dir:
+  desc: null
+  value: ../../scream_large_oct_debug_128seq
+overwrite_cache:
+  desc: null
+  value: false
+overwrite_output_dir:
+  desc: null
+  value: true
+pad_input_to_multiple_of:
+  desc: null
+  value: null
+pad_target_to_multiple_of:
+  desc: null
+  value: null
+past_index:
+  desc: null
+  value: -1
+per_device_eval_batch_size:
+  desc: null
+  value: 5
+per_device_train_batch_size:
+  desc: null
+  value: 5
+per_gpu_eval_batch_size:
+  desc: null
+  value: null
+per_gpu_train_batch_size:
+  desc: null
+  value: null
+predict_with_generate:
+  desc: null
+  value: true
+prediction_loss_only:
+  desc: null
+  value: false
+preprocessing_num_workers:
+  desc: null
+  value: 32
+push_to_hub:
+  desc: null
+  value: true
+push_to_hub_model_id:
+  desc: null
+  value: null
+push_to_hub_organization:
+  desc: null
+  value: null
+push_to_hub_token:
+  desc: null
+  value: null
+ray_scope:
+  desc: null
+  value: last
+remove_unused_columns:
+  desc: null
+  value: true
+report_to:
+  desc: null
+  value:
+  - tensorboard
+  - wandb
+resume_from_checkpoint:
+  desc: null
+  value: 'True'
+run_description:
+  desc: null
+  value: A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear
+    decay on the all_v5-corpus.
+run_name:
+  desc: null
+  value: ScreamLarge - debug_beam5_long
+save_on_each_node:
+  desc: null
+  value: false
+save_steps:
+  desc: null
+  value: 500
+save_strategy:
+  desc: null
+  value: IntervalStrategy.STEPS
+save_total_limit:
+  desc: null
+  value: null
+seed:
+  desc: null
+  value: 42
+sharded_ddp:
+  desc: null
+  value: []
+shuffle_buffer_size:
+  desc: null
+  value: 500
+skip_memory_metrics:
+  desc: null
+  value: true
+sortish_sampler:
+  desc: null
+  value: false
+streaming:
+  desc: null
+  value: true
+task:
+  desc: null
+  value: transcribe
+text_column:
+  desc: null
+  value: null
+text_column_name:
+  desc: null
+  value: text
+tf32:
+  desc: null
+  value: null
+tokenizer_name:
+  desc: null
+  value: null
+torch_compile:
+  desc: null
+  value: false
+torch_compile_backend:
+  desc: null
+  value: null
+torch_compile_mode:
+  desc: null
+  value: null
+torchdynamo:
+  desc: null
+  value: null
+tpu_metrics_debug:
+  desc: null
+  value: false
+tpu_num_cores:
+  desc: null
+  value: null
+train_split_name:
+  desc: null
+  value: train
+use_auth_token:
+  desc: null
+  value: true
+use_fast_tokenizer:
+  desc: null
+  value: true
+use_ipex:
+  desc: null
+  value: false
+use_legacy_prediction_loop:
+  desc: null
+  value: false
+use_mps_device:
+  desc: null
+  value: false
+wandb_entity:
+  desc: null
+  value: nbailab
+wandb_project:
+  desc: null
+  value: Scream - septimus
+warmup_ratio:
+  desc: null
+  value: 0.0
+warmup_steps:
+  desc: null
+  value: 10000
+weight_decay:
+  desc: null
+  value: 0.0
+xpu_backend:
+  desc: null
+  value: null
diff --git a/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/events.out.tfevents.1681322734.t1v-n-0a06f6ef-w-0.1609993.0.v2 b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/events.out.tfevents.1681322734.t1v-n-0a06f6ef-w-0.1609993.0.v2
new file mode 120000
index 0000000..6af4fa1
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/events.out.tfevents.1681322734.t1v-n-0a06f6ef-w-0.1609993.0.v2
@@ -0,0 +1 @@
+/home/perk/models/scream_large_oct_debug_128seq/runs/Apr12_18-05-34_t1v-n-0a06f6ef-w-0/events.out.tfevents.1681322734.t1v-n-0a06f6ef-w-0.1609993.0.v2
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/requirements.txt b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/requirements.txt
new file mode 100644
index 0000000..34d1446
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/requirements.txt
@@ -0,0 +1,140 @@
+absl-py==1.4.0
+aiohttp==3.8.4
+aiosignal==1.3.1
+appdirs==1.4.4
+astunparse==1.6.3
+async-timeout==4.0.2
+attrs==22.2.0
+audioread==3.0.0
+cached-property==1.5.2
+cachetools==5.3.0
+certifi==2022.12.7
+cffi==1.15.1
+charset-normalizer==3.1.0
+chex==0.1.7
+click==8.1.3
+cmake==3.26.1
+datasets==2.11.0
+decorator==5.1.1
+dill==0.3.6
+dm-tree==0.1.8
+docker-pycreds==0.4.0
+etils==1.1.1
+evaluate==0.4.0
+filelock==3.10.7
+flatbuffers==23.3.3
+flax==0.6.8
+frozenlist==1.3.3
+fsspec==2023.3.0
+gast==0.4.0
+gitdb==4.0.10
+gitpython==3.1.31
+google-auth-oauthlib==1.0.0
+google-auth==2.17.1
+google-pasta==0.2.0
+grpcio==1.53.0
+h5py==3.8.0
+huggingface-hub==0.13.3
+idna==3.4
+importlib-metadata==6.1.0
+importlib-resources==5.12.0
+jax==0.4.8
+jaxlib==0.4.7
+jinja2==3.1.2
+jiwer==3.0.1
+joblib==1.2.0
+keras==2.12.0
+lazy-loader==0.2
+libclang==16.0.0
+librosa==0.10.0.post2
+libtpu-nightly==0.1.dev20230327
+lit==16.0.0
+llvmlite==0.39.1
+markdown-it-py==2.2.0
+markdown==3.4.3
+markupsafe==2.1.2
+mdurl==0.1.2
+ml-dtypes==0.0.4
+mpmath==1.3.0
+msgpack==1.0.5
+multidict==6.0.4
+multiprocess==0.70.14
+nest-asyncio==1.5.6
+networkx==3.0
+numba==0.56.4
+numpy==1.23.5
+nvidia-cublas-cu11==11.10.3.66
+nvidia-cuda-cupti-cu11==11.7.101
+nvidia-cuda-nvrtc-cu11==11.7.99
+nvidia-cuda-runtime-cu11==11.7.99
+nvidia-cudnn-cu11==8.5.0.96
+nvidia-cufft-cu11==10.9.0.58
+nvidia-curand-cu11==10.2.10.91
+nvidia-cusolver-cu11==11.4.0.1
+nvidia-cusparse-cu11==11.7.4.91
+nvidia-nccl-cu11==2.14.3
+nvidia-nvtx-cu11==11.7.91
+oauthlib==3.2.2
+opt-einsum==3.3.0
+optax==0.1.4
+orbax==0.1.7
+packaging==23.0
+pandas==1.5.3
+pathtools==0.1.2
+pip==23.0.1
+pkg-resources==0.0.0
+pooch==1.6.0
+protobuf==4.22.1
+psutil==5.9.4
+pyarrow==11.0.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pycparser==2.21
+pydub==0.25.1
+pygments==2.14.0
+python-dateutil==2.8.2
+pytz==2023.3
+pyyaml==6.0
+rapidfuzz==2.13.7
+regex==2023.3.23
+requests-oauthlib==1.3.1
+requests==2.28.2
+responses==0.18.0
+rich==13.3.3
+rsa==4.9
+scikit-learn==1.2.2
+scipy==1.10.1
+sentry-sdk==1.18.0
+setproctitle==1.3.2
+setuptools==44.0.0
+six==1.16.0
+smmap==5.0.0
+soundfile==0.12.1
+soxr==0.3.4
+sympy==1.11.1
+tabulate==0.9.0
+tensorboard-data-server==0.7.0
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.12.1
+tensorflow-estimator==2.12.0
+tensorflow-io-gcs-filesystem==0.32.0
+tensorflow==2.12.0
+tensorstore==0.1.35
+termcolor==2.2.0
+threadpoolctl==3.1.0
+tokenizers==0.13.2
+toolz==0.12.0
+torch==2.0.0
+torchaudio==2.0.1
+tqdm==4.65.0
+transformers==4.28.0.dev0
+triton==2.0.0
+typing-extensions==4.5.0
+urllib3==1.26.15
+wandb==0.14.0
+werkzeug==2.2.3
+wheel==0.40.0
+wrapt==1.14.1
+xxhash==3.2.0
+yarl==1.8.2
+zipp==3.15.0
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/wandb-metadata.json b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/wandb-metadata.json
new file mode 100644
index 0000000..7a881f6
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/wandb-metadata.json
@@ -0,0 +1,1302 @@
+{
+    "os": "Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2023-04-12T18:05:34.809856",
+    "startedAt": "2023-04-12T18:05:33.811661",
+    "docker": null,
+    "cuda": null,
+    "args": [
+        "--model_name_or_path",
+        "openai/whisper-large-v2",
+        "--run_name",
+        "ScreamLarge - debug_beam5_long",
+        "--run_description",
+        "A Large Whisper Scream model with 5 batch size. Trained with 5e-6 and linear decay on the all_v5-corpus.",
+        "--wandb_entity",
+        "nbailab",
+        "--wandb_project",
+        "Scream - septimus",
+        "--dataset_name",
+        "NbAiLab/NCC_speech_all_v5",
+        "--language",
+        "Norwegian",
+        "--text_column_name",
+        "text",
+        "--train_split_name",
+        "train",
+        "--eval_split_name",
+        "validation",
+        "--output_dir",
+        "../../scream_large_oct_debug_128seq",
+        "--overwrite_output_dir",
+        "--warmup_steps",
+        "10000",
+        "--do_train",
+        "--do_eval",
+        "--num_train_steps",
+        "50000",
+        "--lr_scheduler_type",
+        "linear",
+        "--eval_steps",
+        "10000",
+        "--learning_rate",
+        "5e-6",
+        "--preprocessing_num_workers",
+        "32",
+        "--per_device_train_batch_size",
+        "5",
+        "--per_device_eval_batch_size",
+        "5",
+        "--predict_with_generate",
+        "--log_max_eval_predictions",
+        "100",
+        "--log_eval_predictions_fn",
+        "log_predictions.write_predictions",
+        "--streaming",
+        "True",
+        "--use_auth_token",
+        "True",
+        "--dtype",
+        "bfloat16",
+        "--hub_private_repo",
+        "True",
+        "--hub_model_id",
+        "NbAiLab/scream_large_oct_debug_128seq",
+        "--resume_from_checkpoint",
+        "True",
+        "--num_beams",
+        "5",
+        "--ignore_data_skip",
+        "--max_label_length",
+        "128",
+        "--push_to_hub"
+    ],
+    "state": "running",
+    "program": "../run_flax_speech_recognition_seq2seq_streaming_debug.py",
+    "codePath": "run_flax_speech_recognition_seq2seq_streaming_debug.py",
+    "git": {
+        "remote": "https://github.com/NbAiLab/nb-whisper.git",
+        "commit": "05683a618e552161edcc42e78932880ec342c56f"
+    },
+    "email": "per@capia.no",
+    "root": "/home/perk/models/nb-whisper",
+    "host": "t1v-n-0a06f6ef-w-0",
+    "username": "perk",
+    "executable": "/home/perk/.whisper/bin/python",
+    "cpu_count": 120,
+    "cpu_count_logical": 240,
+    "cpu_freq": {
+        "current": 2249.9980000000096,
+        "min": 0.0,
+        "max": 0.0
+    },
+    "cpu_freq_per_core": [
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        },
+        {
+            "current": 2249.998,
+            "min": 0.0,
+            "max": 0.0
+        }
+    ],
+    "disk": {
+        "total": 96.74600601196289,
+        "used": 33.40357208251953
+    },
+    "memory": {
+        "total": 400.47254943847656
+    }
+}
diff --git a/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/wandb-summary.json b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/wandb-summary.json
new file mode 100644
index 0000000..affb235
--- /dev/null
+++ b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 895}}
\ No newline at end of file
diff --git a/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/run-qc6ey75b.wandb b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/run-qc6ey75b.wandb
new file mode 100644
index 0000000..bdad8ba
Binary files /dev/null and b/experiment_scream_octavus/wandb/run-20230412_180533-qc6ey75b/run-qc6ey75b.wandb differ
